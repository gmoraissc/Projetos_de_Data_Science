{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmoraissc/Projetos_de_Data_Science/blob/main/Sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R-W39LWMbod"
      },
      "source": [
        "INSTALLATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sebAH0Oeu13t",
        "outputId": "1f1a3983-57ff-44f0-999c-aab2828a804f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pt_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.2.5/pt_core_news_sm-2.2.5.tar.gz (21.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.2 MB 56.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from pt_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('pt_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download pt_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DV_e0ztqQxk",
        "outputId": "9c7c174d-3a01-48b0-fd0e-178a68a3b3a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.6.3.tar.gz (174 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 61 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 71 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 92 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 102 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 112 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 122 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 133 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 143 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 153 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 163 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174 kB 7.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.3-py3-none-any.whl size=170298 sha256=dd01cecc4986b63d1c3b10d12df5a63d7e72af8414ae15325965c9705d8c2915\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/8b/d7/ad579fbef83c287215c0caab60fb0ae0f30c4d7ce5f580eade\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.3\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlhQQe_PnbDC",
        "outputId": "cbf41bcc-7fc3-4c0d-d7be-87207d8775e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.7/dist-packages (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdlamNraMahj"
      },
      "source": [
        "IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYpz5_54gGjk",
        "outputId": "6e19c555-b859-4965-c485-28cbadfa350e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: googletrans==4.0.0-rc1 in /usr/local/lib/python3.7/dist-packages (4.0.0rc1)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.7/dist-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2021.10.8)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2021.12.1)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n"
          ]
        }
      ],
      "source": [
        "pip install googletrans==4.0.0-rc1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2De13A6eBh2",
        "outputId": "2fd4d852-c5ea-4098-9301-166e8b084ae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ],
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tweepy, nltk, json, os, csv, xlsxwriter, pytz, re, heapq, string, spacy, glob, pathlib, time, functools\n",
        "from pandas import ExcelWriter as ExcelWriter\n",
        "from pathlib import Path\n",
        "from datetime import datetime, date, timedelta\n",
        "from csv import writer\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "from textblob.classifiers import PositiveNaiveBayesClassifier\n",
        "from textblob import TextBlob\n",
        "from googletrans import Translator\n",
        "from itertools import combinations\n",
        "%matplotlib inline\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "yyOu2lQiu8Dd"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('/usr/local/lib/python3.7/dist-packages/pt_core_news_sm/pt_core_news_sm-2.2.5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "sgMlLgjTzLRi"
      },
      "outputs": [],
      "source": [
        "# fazer auth_Twitter uma subclasse de pesquisar_tweets para herdar o api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "gmuNrfoS7pC-"
      },
      "outputs": [],
      "source": [
        "class auth_twitter(object):\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def parametros_de_acesso(self, arquivo):\n",
        "    \n",
        "    self.__arquivo = arquivo\n",
        "\n",
        "    with open (arquivo, 'r') as arquivo:\n",
        "      texto = arquivo.readlines()[0].split(',')\n",
        "    \n",
        "    access_token = texto[0].split('=')[1]\n",
        "    access_token_secret = texto[1].split('=')[1]\n",
        "    bearer_token = texto[2].split('=')[1]\n",
        "    consumer_key = texto[3].split('=')[1]\n",
        "    consumer_secret = texto[4].split('=')[1]\n",
        "\n",
        "    arquivo.close()\n",
        "\n",
        "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_token, access_token_secret)\n",
        "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "    \n",
        "    return api"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzWKDB6gMX6J"
      },
      "source": [
        "DATA EXCTRACTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "RL_XrvFE1GBp"
      },
      "outputs": [],
      "source": [
        "class pesquisar_tweets(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.resultados_da_pesquisa = list()\n",
        "    self.historico_info_participantes = list()\n",
        "    self.informacoes_hist_participante = list()\n",
        "    self.perfis_a_excluir = set(termos_de_pesquisa().participantes_totais()) | set(termos_de_pesquisa().perfis_oficiais_bbb)\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def publico(self, query: tuple, result_type='popular', count=int(10)):\n",
        "\n",
        "    \"Esta consulta por tweets exclui perfis oficiais ou de participantes para torná-la puramente pública.\"\n",
        "\n",
        "    global api\n",
        "\n",
        "    self.__query = query\n",
        "    self.__result_type = result_type\n",
        "    self.__count = count\n",
        "\n",
        "    for status in tweepy.Cursor(api.search,\n",
        "                                    q=self.__query,\n",
        "                                    result_type=self.__result_type,\n",
        "                                    count=self.__count,\n",
        "                                    exclude_replies=True).items(1000):\n",
        "\n",
        "            _ = json.dumps(status._json)\n",
        "            x = json.loads(_)\n",
        "            if x['user']['screen_name'] not in self.perfis_a_excluir:\n",
        "              self.resultados_da_pesquisa.append(status)\n",
        "\n",
        "    return self.resultados_da_pesquisa, self.__result_type, 'Publico'\n",
        " \n",
        "  @functools.lru_cache(maxsize=128)\n",
        "  def por_termo(self, query: tuple, result_type='recent', count=int(100)):\n",
        "    \n",
        "    \"\"\"Este método faz a consulta de tweets a partir de termos. \n",
        "    Query: a query deve ser uma tupla (str) utilizando-se de operadores lógicos\n",
        "    tais como OR, AND. Ex. BBB AND #REDEBBB. \n",
        "    Result_type: pode ser ou popular ou recent. O primeiro buscará os tweets\n",
        "    que corresponderem a query publicados recentemente. O segundo será aqueles\n",
        "    tweets mais populares. O padrão é recent.\n",
        "    Count: recebe um valor inteiro para limitar o número de tweets coletados por consulta. O padrão é 100.\n",
        "    Retorna os resultados da pesquisa (lista) e o tipo de resultado (popular ou recent).\n",
        "    \"\"\"\n",
        "\n",
        "    global api\n",
        "\n",
        "    self.__query = query\n",
        "    self.__result_type = result_type\n",
        "    self.__count = count\n",
        "\n",
        "    for status in tweepy.Cursor(api.search,\n",
        "                                q=self.__query,\n",
        "                                result_type=self.__result_type,\n",
        "                                count=self.__count).items():\n",
        "\n",
        "                                self.resultados_da_pesquisa.append(status)\n",
        "\n",
        "    return self.resultados_da_pesquisa, self.__result_type\n",
        "  \n",
        "  @functools.lru_cache(maxsize=128)\n",
        "  def por_usuario(self, participante: tuple, result_type: str ='recent', count: int =int(100)):\n",
        "    \n",
        "    \"\"\"Este método faz a consulta de tweets a partir do nome de um usuário. \n",
        "    Participante: recebe o screen_name do usuário do twitter. \n",
        "    Result_type: pode ser ou popular ou recent. O primeiro buscará os tweets\n",
        "    que corresponderem a query publicados recentemente. O segundo será aqueles\n",
        "    tweets mais populares. O padrão é recent.\n",
        "    Count: recebe um valor inteiro para limitar o número de tweets coletados por consulta. O padrão é 100.\n",
        "    Retorna os resultados da pesquisa em uma lista, o tipo de resultado (popular ou recent) e o tipo de\n",
        "    participante (camarote, pipoca, público ou perfil oficial).\n",
        "    \"\"\"\n",
        "\n",
        "    global api\n",
        "\n",
        "    self.__participante = participante\n",
        "    self.__result_type = result_type\n",
        "    self.__count = count\n",
        "    \n",
        "    tipo_participante = informacoes_de_participantes().checar_tipo_participante(self.__participante)\n",
        "\n",
        "    for status in tweepy.Cursor(api.user_timeline,\n",
        "                                screen_name=self.__participante,\n",
        "                                result_type=self.__result_type,\n",
        "                                exclude_replies=False).items(self.__count):\n",
        "                                \n",
        "                                self.resultados_da_pesquisa.append(status)\n",
        "\n",
        "    return self.resultados_da_pesquisa, self.__result_type, tipo_participante\n",
        "\n",
        "  \n",
        "  def por_usuarios(self, usuarios: tuple, result_type: str='recent', count: int =int(100)):\n",
        "    \n",
        "    \"\"\"Este método faz a consulta de tweets a partir do nome de pelo menos um usuário. \n",
        "    Participante: recebe o screen_name dos usuários do twitter que deseja ser consultado. \n",
        "    Result_type: pode ser ou popular ou recent. O primeiro buscará os tweets\n",
        "    que corresponderem a query publicados recentemente. O segundo será aqueles\n",
        "    tweets mais populares. O padrão é recent.\n",
        "    Count: recebe um valor inteiro para limitar o número de tweets coletados por consulta. O padrão é 100.\n",
        "    Retorna uma lista com os resultados da pesquisa, o tipo de resultado (popular ou recent) e o tipo de pesquisa (única ou múltipla).\n",
        "    \"\"\"\n",
        "\n",
        "    global api\n",
        "\n",
        "    self.__result_type = result_type\n",
        "    self.__count = count\n",
        "    self.__usuarios = usuarios\n",
        "    \n",
        "    for self._participante in self.__usuarios:\n",
        "\n",
        "      for status in tweepy.Cursor(api.user_timeline,\n",
        "                                  screen_name=self._participante,\n",
        "                                  result_type=self.__result_type,\n",
        "                                  exclude_replies=False).items(self.__count):\n",
        "                                  \n",
        "                                  self.resultados_da_pesquisa.append(status)\n",
        "\n",
        "    return self.resultados_da_pesquisa, self.__result_type, 'Todos'\n",
        "\n",
        "  @functools.lru_cache(maxsize=128)\n",
        "  def infos_do_usuario(self, usuario: str) -> list:\n",
        "  \n",
        "    \"\"\"Este método faz a consulta de informações do usuário (número de seguidores (int), número de perfis que segue (int),\n",
        "    número de listados (int), número de tweets favoritados (int), número de tweets totais (int), verificado (booleano) e a data\n",
        "    é uma informação gerada pelo método \"corrigir_timezone\" cujo padrão é a hora, a fim de organizar os arquivos nas devidas pastas). \n",
        "    Usuario: recebe o screen_name do usuário do twitter que deseja ser consultado.\n",
        "    Retorna uma lista.\n",
        "    \"\"\"\n",
        "\n",
        "    global api\n",
        "\n",
        "    self.usuario = usuario\n",
        "\n",
        "    self.__user_results = api.get_user(self.usuario)\n",
        "\n",
        "    extracao = organizar_coletas_de_tweets.corrigir_timezone(tipo='hora')\n",
        "    \n",
        "    self.informacoes_hist_participante.append({'participante': str(self.__user_results.name),\n",
        "                                          'twitter': str(self.__user_results.screen_name),\n",
        "                                          'n_seguidores': int(self.__user_results.followers_count),\n",
        "                                          'n_seguindo': int(self.__user_results.friends_count),\n",
        "                                          'n_de_listados': int(self.__user_results.listed_count),\n",
        "                                          'n_tweets_favoritados': int(self.__user_results.favourites_count),\n",
        "                                          'num_de_tweets_totais': int(self.__user_results.statuses_count),\n",
        "                                          'verificado': bool(self.__user_results.verified),\n",
        "                                          'data': extracao})\n",
        "    \n",
        "    return self.informacoes_hist_participante\n",
        "\n",
        "  \n",
        "  def historico_participante(self, participantes: list):\n",
        "\n",
        "    \"\"\"Este métood visa, a partir da busa por usuario, iterar em uma lista de usuários e fazer uma varredura por todos os participantes\n",
        "    buscando as informações históricas (citadas em infos_do_usuario).\n",
        "    Participantes: recebe uma lista de screen_names para iteração.\n",
        "    Retorna uma lista com todas as informações coletadas.\n",
        "    \"\"\"\n",
        "\n",
        "    for self._participante in participantes:\n",
        "      _ = self.infos_do_usuario(self._participante)\n",
        "      self.historico_info_participantes.extend(_)\n",
        "    \n",
        "    return self.historico_info_participantes, 'historico', 'todos'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi2o1bokMVPW"
      },
      "source": [
        "DATA ORGANIZATION/FILE HANDLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "VUzJV9SfHLw5"
      },
      "outputs": [],
      "source": [
        "class organizar_coletas_de_tweets(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.dicionario = {}\n",
        "    self.lista = []\n",
        "    self.arquivo_consolidado = []\n",
        "    self.arquivos_a_consolidar = []\n",
        "    self.consolidados = f'/content/drive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/DataFrames/Consolidados/'\n",
        "    self.diretorio = f\"/content/drive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/DataFrames/\"\n",
        "\n",
        "  def criar_dicionario(self, tweets_status, tipo):\n",
        "\n",
        "    d1 = organizar_coletas_de_tweets.corrigir_timezone(tipo='data')\n",
        "\n",
        "    self.__tweets_status = tweets_status\n",
        "    self.__tipo = tipo\n",
        "\n",
        "    for each_json_tweet in tweets_status:\n",
        "      _ = json.dumps(each_json_tweet._json)\n",
        "      \n",
        "      if d1 not in self.dicionario.keys():\n",
        "        tweet = {d1: [json.loads(_)]}\n",
        "        self.dicionario.update(tweet)\n",
        "        \n",
        "      else:\n",
        "        self.dicionario[d1].append(json.loads(_))\n",
        "      \n",
        "    return self.dicionario, self.__tipo\n",
        "\n",
        "  def dataframe_tweets(self, dicionario, tipo_de_pesquisa, multipla=False):\n",
        "    \n",
        "    d1 = organizar_coletas_de_tweets.corrigir_timezone(tipo='data')\n",
        "    \n",
        "    self.__dicionario = dicionario\n",
        "    self.__tipo_de_pesquisa = tipo_de_pesquisa\n",
        "\n",
        "    for i in range(0, len(dicionario[d1])):\n",
        "\n",
        "          tweet_id = self.__dicionario[d1][i]['id']\n",
        "          text = self.__dicionario[d1][i]['text']\n",
        "          favorite_count = self.__dicionario[d1][i]['favorite_count']\n",
        "          retweet_count = self.__dicionario[d1][i]['retweet_count']\n",
        "          created_at = self.__dicionario[d1][i]['created_at']\n",
        "          hashtags = self.__dicionario[d1][i]['entities']['hashtags']\n",
        "          user_mentions = self.__dicionario[d1][i]['entities']['user_mentions']\n",
        "          name = self.__dicionario[d1][i]['user']['name']\n",
        "          screen_name = self.__dicionario[d1][i]['user']['screen_name']\n",
        "\n",
        "          self.lista.append({'tweet_id': str(tweet_id),\n",
        "                        'text': str(text),\n",
        "                        'favorite_count': int(favorite_count),\n",
        "                        'retweet_count': int(retweet_count),\n",
        "                        'created_at': created_at,\n",
        "                        'user_mentions': user_mentions,\n",
        "                        'name': name,\n",
        "                        'screen_name': screen_name})\n",
        "          \n",
        "    self.__tweet_json_ = pd.DataFrame(self.lista, columns = \n",
        "                              ['tweet_id', 'text', \n",
        "                                'favorite_count', 'retweet_count', \n",
        "                                'created_at', 'hashtags', \n",
        "                                'user_mentions', 'name',\n",
        "                                'screen_name'])\n",
        "    \n",
        "    if multipla == False:\n",
        "      \n",
        "      self.__tipo_participante = informacoes_de_participantes().checar_tipo_participante(screen_name)\n",
        "    \n",
        "    else:\n",
        "      \n",
        "      self.__tipo_participante = 'Todos'\n",
        "\n",
        "    return self.__tweet_json_, self.__tipo_participante, self.__tipo_de_pesquisa\n",
        "\n",
        "  def dataframe_dos_participantes(self, informacoes_hist_participante):\n",
        "\n",
        "      self.__informacoes_hist_participante = informacoes_hist_participante\n",
        "\n",
        "      self.__dados_participantes = pd.DataFrame(self.__informacoes_hist_participante, columns = \n",
        "                                      ['participante', 'twitter', 'n_seguidores', \n",
        "                                      'n_seguindo','n_de_listados', 'n_tweets_favoritados',\n",
        "                                      'num_de_tweets_totais', 'verificado', 'data'])\n",
        "      \n",
        "      return self.__dados_participantes, 'historico', None\n",
        "  \n",
        "  @staticmethod\n",
        "  def salvar_arquivo(dataframe, tipo_pesquisa, tipo_participante):\n",
        "    \n",
        "    data_extracao = organizar_coletas_de_tweets.corrigir_timezone(tipo='data')\n",
        "    dir_csvs = f'/content/drive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/DataFrames/'\n",
        "\n",
        "    if tipo_pesquisa == 'historico':\n",
        "           \n",
        "      file_variable_ = dir_csvs + 'Participantes'\n",
        "      file_name = 'historico.csv'\n",
        "      file_path_ = file_variable_ + \"/\" + file_name\n",
        "      organizar_coletas_de_tweets.diretorios(file_variable=file_variable_, file_path=file_path_, dataframe=dataframe)\n",
        "    \n",
        "    else:\n",
        "\n",
        "      file_variable_ = dir_csvs + \"/\" + tipo_participante  + \"/\" + tipo_pesquisa + \"/\" + data_extracao\n",
        "      file_name = tipo_participante[0:3] + \"_\" + tipo_pesquisa[0:3] + \"_\" + data_extracao + '.csv'\n",
        "      file_path_ = file_variable_ + \"/\" + file_name\n",
        "      organizar_coletas_de_tweets.diretorios(file_variable=file_variable_, file_path=file_path_, dataframe=dataframe)\n",
        "\n",
        "  @staticmethod     \n",
        "  def diretorios(file_variable, file_path, dataframe):\n",
        "\n",
        "    if not os.path.exists(file_variable):\n",
        "      os.makedirs(file_variable)\n",
        "      dataframe.to_csv(file_path, index=False)\n",
        "\n",
        "    else:\n",
        "      with open(file_path, 'a') as f_object:\n",
        "        writer_object = writer(f_object, delimiter=',')\n",
        "        \n",
        "        linhas = []\n",
        "        \n",
        "        for row in range(0, len(dataframe)):\n",
        "          linhas.append([row for row in dataframe.iloc[row]])\n",
        "          writer_object.writerow(linhas[row])\n",
        "\n",
        "        f_object.close()\n",
        "    \n",
        "  @staticmethod\n",
        "  def corrigir_timezone(tipo):\n",
        "    \n",
        "    utcmoment_naive = datetime.utcnow()\n",
        "    utcmoment = utcmoment_naive.replace(tzinfo=pytz.utc)\n",
        "    tz = 'America/Sao_Paulo'\n",
        "    extracao = utcmoment.astimezone(pytz.timezone(tz)) - timedelta(hours=0, minutes=60)\n",
        "    data_de_extracao = extracao.date().strftime(\"%d%m%Y\")\n",
        "\n",
        "    if tipo == 'data':\n",
        "      return data_de_extracao\n",
        "    \n",
        "    elif tipo == 'hora':\n",
        "      return extracao\n",
        "\n",
        "  \n",
        "#arquivos_consolidados = {'Camarote':[],\n",
        "#                         'Pipoca': [],\n",
        "#                         'Publico': [],\n",
        "#                         'PerfilOficial': [],\n",
        "#                         'Todos': []}\n",
        "  \n",
        "  def consolidar(self, name, path, tipo):\n",
        "\n",
        "    self.arquivo_consolidado.append(name)\n",
        "    self.arquivos_a_consolidar.append(pathlib.PurePath(path, name))\n",
        "    self.combined_csv = pd.concat([pd.read_csv(f, usecols=['tweet_id', 'text', 'screen_name']) for f in self.arquivos_a_consolidar])\n",
        "    self.diretorio_a_consolidar = pathlib.PurePath(self.consolidados + tipo,'consolidado_' + tipo + '.csv')\n",
        "    self.combined_csv.to_csv(self.diretorio_a_consolidar, index=False )\n",
        "\n",
        "  \n",
        "  def consolidar_total(self):\n",
        "\n",
        "    for path, dirs, files in os.walk(self.diretorio, topdown=False):\n",
        "      \n",
        "      for name in files:\n",
        "        if 'Camarote' in path:\n",
        "          _ = 'Camarote'\n",
        "          self.consolidar(name=name, path=path, tipo=_)\n",
        "                \n",
        "        elif 'Pipoca' in path:\n",
        "          _ = 'Pipoca'\n",
        "          self.consolidar(name=name, path=path, tipo=_)\n",
        "\n",
        "        elif 'Publico' in path:\n",
        "          _ = 'Publico'\n",
        "          self.consolidar(name=name, path=path, tipo=_)\n",
        "        \n",
        "        elif 'PerfilOficial' in path:\n",
        "          _ = 'PerfilOficial'\n",
        "          self.consolidar(name=name, path=path, tipo=_)\n",
        "        \n",
        "        elif 'Todos' in path:\n",
        "          _ = 'Todos'\n",
        "          self.consolidar(name=name, path=path, tipo=_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tWq-LFkMPmL"
      },
      "source": [
        "PARTICIPANTS INFORMATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "Lu2x5fyPALbL"
      },
      "outputs": [],
      "source": [
        "class informacoes_de_participantes(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  @staticmethod\n",
        "  def checar_tipo_participante(usuario):\n",
        "      if usuario in termos_de_pesquisa().participantes_camarote:\n",
        "        return 'Camarote'\n",
        "      elif usuario in termos_de_pesquisa().participantes_pipoca:\n",
        "        return 'Pipoca'\n",
        "      elif usuario in termos_de_pesquisa().perfis_oficiais_bbb:\n",
        "        return 'PerfilOficial'\n",
        "      else:\n",
        "        return 'Publico'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdXXGgr9MKZG"
      },
      "source": [
        "TEXT OPERATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "SbQrOBweyleK"
      },
      "outputs": [],
      "source": [
        "class termos_de_pesquisa(object):\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.__participantes_totais = []\n",
        "    self.participantes_camarote = ['Aguiarthur', 'Naiarazevedo', 'PedroScooby', 'brunnagoncalves', \n",
        "                          'iampauloandre', 'eumaria', 'jadepicon', 'Silva_DG', 'linndaquebrada', 'TiagoAbravanel']\n",
        "\n",
        "    self.participantes_pipoca = ['Dra_laiscaldass', 'LucianoEstevan', 'a_jessilane', 'Eli', 'eslomarques', \n",
        "                      'bbaheck', 'oficialmussi', 'oficial_deodato', 'vyniof', 'LucasBissoli_']\n",
        "\n",
        "    self.participantes_camarote_twitter = ['@Aguiarthur', '@Naiarazevedo', '@PedroScooby', '@brunnagoncalves', \n",
        "                        '@iampauloandre', '@eumaria', '@jadepicon', '@Silva_DG', '@linndaquebrada', '@TiagoAbravanel']\n",
        "\n",
        "    self.participantes_pipoca_twitter = ['@Dra_laiscaldass', '@LucianoEstevan', '@a_jessilane', '@eusouoeli', '@eslomarques', \n",
        "                      '@bbaheck', '@oficialmussi', '@oficial_deodato', '@vyniof', '@LucasBissoli_']\n",
        "\n",
        "    self.perfis_oficiais_bbb = ['bbb', 'globoplay', 'tadeuschmidt']\n",
        "\n",
        "    self.perfis_oficiais_bbb_twitter = ['@bbb', '@globoplay', '@tadeuschmidt']\n",
        "\n",
        "  def termos(self):\n",
        "    \n",
        "    self.dicionario_de_termos_de_pesquisa = {'BigBrotherBrasil': {'BBB', 'BBB22', '#REDEBBB', '#BBB', '#BBB22'},\n",
        "         'Participantes': {'Arthur': ['Arthur', 'Artur', 'Aguiar', 'Arthur Aguiar', '\\u2747\\ufe0f', '#TeamAguiar', '#TeamArthurAguiar'],\n",
        "                           'Naiara Azevedo': ['Naiara', 'Azevedo', '1f4b8', '#TeamNaiara', 'Nai', 'cantora'],\n",
        "                           'Pedro Scooby': ['Pedro', 'Scooby', '1f30a', '#TeamScooby', '#TimeScooby', '1f499'],\n",
        "                           'Brunna Gonçalves': ['Brunna', 'Gonçalves', '1f984', '#BBBRUNNA', 'BBBrunna'],\n",
        "                           'Paulo André': ['Paulo', 'André', '1f3c1', '#TeamPauloAndré', 'TeamPauloAndre'],\n",
        "                           'Maria': ['Maria', '1f40d', '#TEAMMARIA', 'TIME MARIA', 'MARICONAS'],\n",
        "                           'Jade Picon': ['Jade', 'Picon', '1f32a\\ufe0f', 'furacão', '#TeamJade', 'Picão', 'KdJade', 'Picões', 'Furacao'],\n",
        "                           'Douglas Silva': ['Douglas', 'Silva', '1f3b2', 'dado', 'dadinho', '#TeamDouglasSilva', '1f44a\\U0001f3ff'],\n",
        "                           'Linn da Quebrada': ['Linn', 'Quebrada', '1f9dc\\u200d\\u2640\\ufe0f', '1f9dc\\U0001f3ff\\u200d\\u2640\\ufe0f',\n",
        "                                                '1f9dc\\U0001f3fb\\u200d\\u2640\\ufe0f', '1f9dc\\U0001f3fe\\u200d\\u2640\\ufe0f', '1f9dc\\U0001f3fc\\u200d\\u2640\\ufe0f',\n",
        "                                                '1f9dc\\U0001f3fd\\u200d\\u2640\\ufe0f', '1f9dc', '1f9dc\\U0001f3ff', '1f9dc\\U0001f3fb', '1f9dc\\U0001f3fe',\n",
        "                                                '1f9dc\\U0001f3fc', '1f9dc\\U0001f3fd', '#TeamLinn', '#linndonas', '#LinnDonas'],\n",
        "                           'Tiago Abravanel': ['Tiago', 'Abravanel', '1f43b', '#TeamAbrava', '#TeamAbravanel', '1f9f8'],\n",
        "                           'Laís Caldas': ['Laís', 'Caldas', '1f462', '#TeamLais', '#TeamLaís', 'Time Lais', '1f49a'],\n",
        "                           'Luciano Estevan': ['Luciano', 'Estevan', '1f981', '#TeamLucianoEstevan', 'Lu', '1f346'],\n",
        "                           'Jessilane': ['Jessilane', '1f9ec', 'Jessi', '#TeamJessi', '#TimeJessi', '1f49c', 'Charmanders'],\n",
        "                           'Eliezer': ['Eliezer', '1F437', 'Eli', '#TeamEli', '1F953', '1F416', '1F43D'],\n",
        "                           'Eslovênia Marques': ['Eslovênia', 'Eslováquia', 'Marques', 'Eslô', '1f1f8\\U0001f1ee',\n",
        "                                                 'Time Eslo', 'Team Eslo', '#TimeEslô', '#TeamEslô', 'Eslovenia'],\n",
        "                           'Bábara Heck': ['Bárbara', 'Heck', '1f980', '#TeamBá', 'TeamBá', 'TimeBá',\n",
        "                                           'TimeBa', 'TeamBa', 'TeamBah', 'BBBah', 'Bá', 'Ba', '1f9a6'],\n",
        "                           'Rodrigo Mussi': ['Rodrigo Mussi', '#TeamMussi', \"BBB\", \"BIG BROTHER BRASIL\",\n",
        "                                             '#TimeMussi', '1f977\\U0001f3ff', '1f977', '1f977\\U0001f3fb',\n",
        "                                             '1f977\\U0001f3fe', '1f977\\U0001f3fc', '1f977\\U0001f3fd', 'ninja', 'ninjas', '#TEAMNINJA'],\n",
        "                           'Natália Deodato': ['Natália', 'Deodato', '#TeamNaty', '#TimeNaty', 'Naty', 'vitilindos',\n",
        "                                               '1f483', '1f483\\U0001f3ff', '1f483\\U0001f3fb',\n",
        "                                               '1f483\\U0001f3fe', '1f483\\U0001f3fc', '1f483\\U0001f3fd'],\n",
        "                           'Vinicius': ['Vinicius', 'TeamVyni', '#TimeVyni', '#TeamVyni', '1f4a1'],\n",
        "                           'Lucas Bissoli': ['Lucas', 'Bissoli', '#TimeBissoli', '#TeamBissoli', \n",
        "                                             '1f3c4\\u200d\\u2642\\ufe0f', '1f3c4\\U0001f3ff\\u200d\\u2642\\ufe0f', \n",
        "                                             '1f3c4\\U0001f3fb\\u200d\\u2642\\ufe0f', '1f3c4\\U0001f3fe\\u200d\\u2642\\ufe0f',\n",
        "                                             '1f3c4\\U0001f3fc\\u200d\\u2642\\ufe0f', '1f3c4\\U0001f3fd\\u200d\\u2642\\ufe0f',\n",
        "                                             '1f3c4', '1f3c4\\U0001f3ff', '1f3c4\\U0001f3fb', '1f3c4\\U0001f3fe',\n",
        "                                             '1f3c4\\U0001f3fc', '1f3c4\\U0001f3fd', '1f3c4\\u200d\\u2640\\ufe0f',\n",
        "                                             '1f3c4\\U0001f3ff\\u200d\\u2640\\ufe0f', '1f3c4\\U0001f3fb\\u200d\\u2640\\ufe0f',\n",
        "                                             '1f3c4\\U0001f3fe\\u200d\\u2640\\ufe0f', '1f3c4\\U0001f3fc\\u200d\\u2640\\ufe0f',\n",
        "                                             '1f3c4\\U0001f3fd\\u200d\\u2640\\ufe0f']}}\n",
        "    return self.dicionario_de_termos_de_pesquisa\n",
        "    #termos_a_analisar = []\n",
        "\n",
        "    #avaliador_de_termos()\n",
        "\n",
        "    #return termos_de_pesquisa_BBB\n",
        "\n",
        "    #analisador_de_termos(termos_de_pesquisa_participantes)\n",
        "\n",
        "  def analisador_de_termos(self, termos_atuais, termos_novos):\n",
        "\n",
        "    self.__termos_atuais = termos_atuais\n",
        "    self.__termos_novos = termos_novos\n",
        "\n",
        "    #for termo in termos_atuais:\n",
        "      # if novo termo ajudar a explicar:\n",
        "      # append\n",
        "      # if len(termos) > limite:\n",
        "      # termo_a_remover = ()\n",
        "      # for termo in termos atuais atualizado:\n",
        "      # termo_a_remover['explicacao'] < min_a_explicar:\n",
        "      # termos_atuais.pop(termo_a_remover)\n",
        "\n",
        "  def tendencia(self, termo_novo):\n",
        "\n",
        "    self.termo_novo = termo_novo\n",
        "    pass\n",
        "\n",
        "  def correlacao_participantes(self):\n",
        "    pass    \n",
        "\n",
        "  def dicionario_participantes(self):\n",
        "\n",
        "    self.dicionario_de_participantes = {'@Aguiarthur': 'Arthur Aguiar', \n",
        "                            '@Naiarazevedo': 'Naiara Azevedo',\n",
        "                            '@PedroScooby': 'Pedro Scooby',\n",
        "                            '@brunnagoncalves': 'Bruna Gonçalves',\n",
        "                            '@iampauloandre': 'Paulo André',\n",
        "                            '@eumaria': 'Maria',\n",
        "                            '@jadepicon': 'Jade Picon',\n",
        "                            '@Silva_DG': 'Douglas Silva',\n",
        "                            '@linndaquebrada': 'Linn da Quebrada',\n",
        "                            '@TiagoAbravanel': 'Tiago Abravanel',\n",
        "                            '@Dra_laiscaldass': 'Laís Caldas',\n",
        "                            '@LucianoEstevan': 'Luciano Estevan',\n",
        "                            '@a_jessilane': 'Jessilane',\n",
        "                            '@eusouoeli': 'Eliezer',\n",
        "                            '@eslomarques': 'Eslovênia Maques',\n",
        "                            '@bbaheck': 'Bábara Heck',\n",
        "                            '@oficialmussi': 'Rodrigo Mussi',\n",
        "                            '@oficial_deodato': 'Natália Deodato',\n",
        "                            '@vyniof': 'Vinicius',\n",
        "                            '@LucasBissoli_': 'Lucas Bissoli'}\n",
        "\n",
        "    return self.dicionario_de_participantes\n",
        "  \n",
        "  def lista_participantes(self, tipo_participante, perfil=True):     \n",
        "\n",
        "    if tipo_participante == 'camarote':\n",
        "      if perfil:\n",
        "        return self.participantes_camarote_twitter\n",
        "      else:\n",
        "        return self.participantes_camarote\n",
        "    elif tipo_participante == 'pipoca':\n",
        "      if perfil:\n",
        "        return self.participantes_pipoca_twitter\n",
        "      else:\n",
        "        return self.participantes_pipoca\n",
        "    elif tipo_participante == 'oficial':\n",
        "      if perfil:\n",
        "        return self.perfis_oficiais_bbb_twitter\n",
        "      else:\n",
        "        return self.perfis_oficiais_bbb\n",
        "\n",
        "  def participantes_totais(self):\n",
        "\n",
        "    self.__participantes_totais.extend(self.participantes_pipoca)\n",
        "    self.__participantes_totais.extend(self.participantes_camarote)\n",
        "\n",
        "    return self.__participantes_totais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUaqg0LXjs8l"
      },
      "source": [
        "TESTES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "af5Ep4TuZbTp"
      },
      "outputs": [],
      "source": [
        "api = auth_twitter().parametros_de_acesso('/content/drive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/auth.txt')\n",
        "pesquisar = pesquisar_tweets()\n",
        "organizar = organizar_coletas_de_tweets()\n",
        "pesquisas = termos_de_pesquisa()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m2OSKZl5MeN"
      },
      "source": [
        "PESQUISAS POR NOME DE USUARIO(S)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "HyDlkJwE5O2I"
      },
      "outputs": [],
      "source": [
        "tweets, tipo_de_pesquisa, tipo_de_participante = pesquisar.por_usuarios(usuarios=pesquisas.participantes_totais())\n",
        "dicionario, tipo_de_pesquisa = organizar.criar_dicionario(tweets, tipo_de_pesquisa)\n",
        "dataframe, tipo_de_participante, tipo_de_pesquisa = organizar.dataframe_tweets(dicionario, tipo_de_pesquisa, multipla=True)\n",
        "organizar.salvar_arquivo(tipo_pesquisa=tipo_de_pesquisa, dataframe=dataframe, tipo_participante=tipo_de_participante)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnL3PgPJMV_J"
      },
      "source": [
        "HISTÓRICO DOS PARTICIPANTES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "ilwiR0-WvgMZ"
      },
      "outputs": [],
      "source": [
        "tweets, tipo_de_pesquisa, tipo_de_participante = pesquisar.historico_participante(pesquisas.participantes_totais())\n",
        "dataframe, tipo_de_pesquisa, tipo_de_participante = organizar.dataframe_dos_participantes(tweets)\n",
        "organizar.salvar_arquivo(dataframe=dataframe, tipo_pesquisa=tipo_de_pesquisa, tipo_participante=tipo_de_participante)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-s4JoBObuUH"
      },
      "source": [
        "PESQUISA PÚBLICO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "v-3mUz1bbxWJ"
      },
      "outputs": [],
      "source": [
        "tweets, tipo_de_pesquisa, tipo_de_participante = pesquisar.publico('BBB -filter:retweets', result_type='popular')\n",
        "dicionario, tipo_de_pesquisa = organizar.criar_dicionario(tweets, tipo_de_pesquisa)\n",
        "dataframe, tipo_de_participante, tipo_de_pesquisa = organizar.dataframe_tweets(dicionario, tipo_de_pesquisa, multipla=False)\n",
        "organizar.salvar_arquivo(tipo_pesquisa=tipo_de_pesquisa, dataframe=dataframe, tipo_participante=tipo_de_participante)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7ANX5byMa0q"
      },
      "source": [
        "PESQUISA POR TERMO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "Zy4tqzy0cVrj"
      },
      "outputs": [],
      "source": [
        "tweets, tipo_de_pesquisa = pesquisar.por_termo(query=\"(BBB22 -tag -mutirão) -from:bbb -from:globoplay -from:tracklist -from:bchartsnet -from:multishow -is:retweet\", result_type='popular', count=100)\n",
        "dicionario, tipo_de_pesquisa = organizar.criar_dicionario(tweets, tipo_de_pesquisa)\n",
        "dataframe, tipo_participante, tipo_de_pesquisa = organizar.dataframe_tweets(dicionario, tipo_de_pesquisa)\n",
        "organizar.salvar_arquivo(tipo_participante=tipo_participante, tipo_pesquisa=tipo_de_pesquisa, dataframe=dataframe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn0nzU5z4RvS"
      },
      "source": [
        "PESQUISA PERFIL OFICIAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "n7oXnKqfIzYR"
      },
      "outputs": [],
      "source": [
        "tweets, tipo_de_pesquisa, tipo_de_participante = pesquisar.por_usuario('@bbb')\n",
        "dicionario, tipo_de_pesquisa = organizar.criar_dicionario(tweets, tipo_de_pesquisa)\n",
        "dataframe, tipo_de_participante, tipo_de_pesquisa = organizar.dataframe_tweets(dicionario, tipo_de_pesquisa, multipla=False)\n",
        "organizar.salvar_arquivo(tipo_pesquisa=tipo_de_pesquisa, dataframe=dataframe, tipo_participante=tipo_de_participante)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "anLqMdhiPJwq"
      },
      "outputs": [],
      "source": [
        "organizar.consolidar_total()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emUJ6NgeWJbO"
      },
      "source": [
        "NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "x2TvzwPJ3NJW"
      },
      "outputs": [],
      "source": [
        "class processamento_de_texto(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.texto_processado = []\n",
        "    self.texto_sendo_analisado = []\n",
        "    self.texto_sem_pontuacao = []\n",
        "    self.texto_sem_stopwords = []\n",
        "    self.texto_lematizado = []\n",
        "    self.texto_tokenizado = []\n",
        " \n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def remover_https_e_espacamento(self, x): \n",
        "\n",
        "      self.__sem_https = re.sub(r'https.+', '', str(x))\n",
        "      self.__sem_arroba = re.sub(r'@+', '', self.__sem_https)\n",
        "      self.__sem_caractere_de_espacamento = re.sub(r'\\n+', ' ', self.__sem_arroba)\n",
        "\n",
        "      return self.__sem_caractere_de_espacamento\n",
        "  \n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def remover_pontuacao(self, x):\n",
        "    \n",
        "      self.__s = re.sub(r'\\W', ' ', x)\n",
        "      self.__x = re.sub(' +', ' ', self.__s)\n",
        "      \n",
        "      return self.__x\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def remover_stopwords(self, x):  \n",
        "\n",
        "      return \" \".join([palavra for palavra in x.split() if palavra.lower() not in nltk.corpus.stopwords.words('portuguese')])\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def lemmatizar(self, x):\n",
        "\n",
        "      return ' '.join([token.lemma_ for token in nlp(x)])\n",
        "  \n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def tokenizar_texto(self, x):\n",
        "        \n",
        "      return word_tokenize(x)\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def traduzir(self, x):\n",
        "\n",
        "    if x is not None and not len(x)==0:\n",
        "      time.sleep(1)\n",
        "      return Translator().translate(x, dest='en').text\n",
        "      \n",
        "    else:\n",
        "      time.sleep(1)\n",
        "      return x\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def preprocessar(self, x):\n",
        "\n",
        "    self.__sem_link_nem_espaco = self.remover_https_e_espacamento(x)\n",
        "    self.__sem_pontuacao = self.remover_pontuacao(self.__sem_link_nem_espaco)  \n",
        "    self.__sem_stopwords = self.remover_stopwords(self.__sem_pontuacao)\n",
        "    self.__lemmatizado = self.lemmatizar(self.__sem_stopwords)\n",
        "    self.__traduzido = self.traduzir(self.__lemmatizado)\n",
        "        \n",
        "    return self.__traduzido\n",
        "  \n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def list_reduction(self, texto_processado):\n",
        "\n",
        "    return [item for sublist in texto_processado[0] for item in sublist]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7EIhyB3FjM2"
      },
      "source": [
        "MELHORIAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "aTkkDYfUpFWI"
      },
      "outputs": [],
      "source": [
        "# Funcionalidades\n",
        "# property para não precisar \"herdar\" as informações de tipo de pesquisa e tipo de consulta (GETTER)\n",
        "# try except clauses\n",
        "# metodo estático para criar a query a partir de um dicionario dado\n",
        "# metodo para adicionar termos mais relevantes para o modelo *publico e por participante, exemplo, memes\n",
        "#termos mais frequentes para determinado participante\n",
        "# verificar se o arquivo já foi consolidado para não \"retrabalho\"\n",
        "# coletas automáticas/schedule!\n",
        "\n",
        "\n",
        "# Analises\n",
        "# prob de ser eliminado no paredão de acordo com o atual nível de popularidade\n",
        "# modelo de classificação baseado na popularidade do participante\n",
        "# em média, quantos seguidores ganha após um tweet positivo? e quantos perde após um tweet negativo?\n",
        "\n",
        "\n",
        "#wordcloud\n",
        "#plt.figure(figsize = (20,20))\n",
        "#wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
        "               #collocations=False).generate(\" \".join(data_neg))\n",
        "#plt.imshow(wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObZVWpGlOW7p"
      },
      "source": [
        "______________________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN_o49xnNmKB"
      },
      "outputs": [],
      "source": [
        "# def status_do_participante -> eliminado? anjo? lider? acompanhar ao longo do tempo e comparar com o \"desempenho\" no twitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "SSKsT9_DNo9Y"
      },
      "outputs": [],
      "source": [
        "class processar_bases_de_treino(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.train_total = pd.read_csv(f'/content/drive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/DataFrames/BaseDeTreino/base_de_treino.csv')\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def train_fala_do_jogo(self):\n",
        "    self.__subset = self.train_total[['texto_traduzido', 'jogo']]\n",
        "    self._train_is_jogo = [tuple(x) for x in self.__subset.to_numpy()]\n",
        "\n",
        "    return self._train_is_jogo\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def train_intencao_de_voto(self):\n",
        "    self.__subset = self.train_total[['texto_traduzido', 'voto']]\n",
        "    self._train_intencao = [tuple(x) for x in self.__subset.to_numpy()]\n",
        "    \n",
        "    return self._train_intencao\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def train_spam(self):\n",
        "    self.__subset = self.train_total[['texto_traduzido', 'spam']]\n",
        "    self._train_is_spam = [tuple(x) for x in self.__subset.to_numpy()]\n",
        "    \n",
        "    return self._train_is_spam\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def train_sentimento(self):\n",
        "    self.__subset = self.train_total[['texto_traduzido', 'sentimento']]\n",
        "    self._train_sentimento = [tuple(x) for x in self.__subset.to_numpy()]\n",
        "    \n",
        "    return self._train_sentimento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "WOi3YCWxlHR1"
      },
      "outputs": [],
      "source": [
        "class analises_de_tweets(processar_bases_de_treino):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__(self)\n",
        "    self.cl_sent = NaiveBayesClassifier(super().train_sentimento)\n",
        "    self.classifier_jogo = PositiveNaiveBayesClassifier(positive_set=[x for x,_ in list(filter(lambda x: x[1] == 1, super().train_fala_do_jogo())) if type(x) == str], unlabeled_set=[x for x,_ in list(filter(lambda x: x[1] == 0, super().train_fala_do_jogo())) if type(x) == str])\n",
        "    self.classifier_spam = PositiveNaiveBayesClassifier(positive_set=[x for x,_ in list(filter(lambda x: x[1] == 1, super().train_spam())) if type(x) == str], unlabeled_set=[x for x,_ in list(filter(lambda x: x[1] == 0, super().train_spam())) if type(x) == str])\n",
        "    self.classifier_intencao = PositiveNaiveBayesClassifier(positive_set=[x for x,_ in list(filter(lambda x: x[1] == 1, super().train_intencao_de_voto())) if type(x) == str], unlabeled_set=[x for x,_ in list(filter(lambda x: x[1] == 0, super().train_intencao_de_voto())) if type(x) == str])\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def classificar_sentimento_do_tweet(self, x):\n",
        "    return self.cl_sent.classify(x)\n",
        "  \n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def classificar_tweet_jogo(self, x):\n",
        "    return self.classifier_jogo.classify(x)\n",
        "  \n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def classificar_intencao_de_voto(self, x):\n",
        "    return self.classifier_itencao.classify(x)\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def classificar_spam(self, x):\n",
        "    return self.classifier_spam.classify(x)\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def analisar_sentimento_do_tweet(self, x):\n",
        "    self.sent = float(TextBlob(x).sentiment[0])\n",
        "    return self.sent\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def analisar_polaridade_do_tweet(self, x):\n",
        "    self.pol = float(TextBlob(x).sentiment[1])\n",
        "    return self.pol"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# olhar para o text e identificar sobre qual jogador (es) está falando?\n",
        "publico_06022022 = pd.read_csv(f'/content/drive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/DataFrames/Publico/popular/06022022/Pub_pop_06022022.csv')\n",
        "publico_06022022.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "zsLDgWSGmIet",
        "outputId": "069c482c-cc97-42d9-b0ec-99fc2f3d4788"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-674c1af1-e01a-41d8-8f00-58ffa0cf3e90\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>created_at</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>user_mentions</th>\n",
              "      <th>name</th>\n",
              "      <th>screen_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1490375498865721353</td>\n",
              "      <td>RT @acrimewavee: lembrando aqui de quando o po...</td>\n",
              "      <td>0</td>\n",
              "      <td>111</td>\n",
              "      <td>Sun Feb 06 17:23:03 +0000 2022</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'screen_name': 'acrimewavee', 'name': 'sâmi'...</td>\n",
              "      <td>Pedrinho #BBB22</td>\n",
              "      <td>psjds_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1490375495288033290</td>\n",
              "      <td>RT @_talithafreitas: A única pessoa que entend...</td>\n",
              "      <td>0</td>\n",
              "      <td>288</td>\n",
              "      <td>Sun Feb 06 17:23:02 +0000 2022</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'screen_name': '_talithafreitas', 'name': 'T...</td>\n",
              "      <td>𝒋𝒐𝒚</td>\n",
              "      <td>wtfxjoy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1490375494449176578</td>\n",
              "      <td>RT @romanoandre: Até o @chicobarney eles coloc...</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>Sun Feb 06 17:23:02 +0000 2022</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'screen_name': 'romanoandre', 'name': 'André...</td>\n",
              "      <td>エリカDamasceno🎧🅾+</td>\n",
              "      <td>DamascenoErica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1490375493190828032</td>\n",
              "      <td>RT @Silva_DG: Tamo viciados nessa amizade! Iss...</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>Sun Feb 06 17:23:02 +0000 2022</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'screen_name': 'Silva_DG', 'name': 'Douglas ...</td>\n",
              "      <td>Quezia</td>\n",
              "      <td>quezzymarvila</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1490375493178298368</td>\n",
              "      <td>@readingpace puta\\n\\nlaís luciano jessilane el...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Sun Feb 06 17:23:02 +0000 2022</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'screen_name': 'readingpace', 'name': 'nai☁️...</td>\n",
              "      <td>ana</td>\n",
              "      <td>barrowells</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-674c1af1-e01a-41d8-8f00-58ffa0cf3e90')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-674c1af1-e01a-41d8-8f00-58ffa0cf3e90 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-674c1af1-e01a-41d8-8f00-58ffa0cf3e90');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "              tweet_id  ...     screen_name\n",
              "0  1490375498865721353  ...          psjds_\n",
              "1  1490375495288033290  ...         wtfxjoy\n",
              "2  1490375494449176578  ...  DamascenoErica\n",
              "3  1490375493190828032  ...   quezzymarvila\n",
              "4  1490375493178298368  ...      barrowells\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9zPkj13M4CX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sheet_id = '15_1pGfHuKuLsqbhXLMQnCP5l7LCTyxYktG2gi0ezt44'\n",
        "sheet_name = 'consolidado_Publico'\n",
        "url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
        "DfConsolidadoPublico = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "4MV9g9WQhWJc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Sentiment_analysis.ipynb",
      "provenance": [],
      "mount_file_id": "1SR-3i9Wrab9mlb7RdoAnO_79DAVvlJaf",
      "authorship_tag": "ABX9TyMvLZi5qQdnh2yKYLBRShmU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}