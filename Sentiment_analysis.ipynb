{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmoraissc/Projetos_de_Data_Science/blob/main/Sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R-W39LWMbod"
      },
      "source": [
        "INSTALLATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sebAH0Oeu13t",
        "outputId": "63e03a49-703f-4af1-a6ec-75db37835d94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pt_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.2.5/pt_core_news_sm-2.2.5.tar.gz (21.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.2 MB 20.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from pt_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.10)\n",
            "Building wheels for collected packages: pt-core-news-sm\n",
            "  Building wheel for pt-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pt-core-news-sm: filename=pt_core_news_sm-2.2.5-py3-none-any.whl size=21186281 sha256=6d2852f5e6975143b8959c881c99f922960654807086902d68d9c52a053b7f18\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2xjnphjw/wheels/c3/f9/0c/5c014a36941a00f5df5fc0756cb961d7c457a978e697a6ce3b\n",
            "Successfully built pt-core-news-sm\n",
            "Installing collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('pt_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download pt_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DV_e0ztqQxk",
        "outputId": "95a72d97-958f-4c79-fd0c-f6243d84509f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.6.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlhQQe_PnbDC",
        "outputId": "02e8c8eb-a86f-4abd-8cd2-2c7b40ef5bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.0.2-py3-none-any.whl (149 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▏                             | 10 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 20 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 30 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 40 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 51 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 61 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 71 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 81 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 92 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 102 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 112 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 122 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 133 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 143 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 149 kB 7.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdlamNraMahj"
      },
      "source": [
        "IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYpz5_54gGjk",
        "outputId": "b8991b73-8a3d-495c-ed88-20625701361d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting hstspreload\n",
            "  Downloading hstspreload-2021.12.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 17.0 MB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2021.10.8)\n",
            "Collecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Collecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 814 kB/s \n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17416 sha256=317c5c783734c90b2387cf81c59cf5ab6865c7b4f7166c2f770c85eb4a567953\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/34/00/4fe71786ea6d12314b29037620c36d857e5d104ac2748bf82a\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hyperframe, hpack, sniffio, h2, h11, rfc3986, httpcore, hstspreload, httpx, googletrans\n",
            "Successfully installed googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2021.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install googletrans==4.0.0-rc1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2De13A6eBh2",
        "outputId": "c81941ea-c4a6-417d-e64f-c5763653614a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tweepy, nltk, json, os, csv, xlsxwriter, pytz, re, heapq, string, spacy, glob, pathlib, time, functools\n",
        "from pandas import ExcelWriter as ExcelWriter\n",
        "from pathlib import Path\n",
        "from datetime import datetime, date, timedelta\n",
        "from csv import writer\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "from textblob.classifiers import PositiveNaiveBayesClassifier\n",
        "from textblob import TextBlob\n",
        "from googletrans import Translator\n",
        "from itertools import combinations\n",
        "%matplotlib inline\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yyOu2lQiu8Dd"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('/usr/local/lib/python3.7/dist-packages/pt_core_news_sm/pt_core_news_sm-2.2.5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgMlLgjTzLRi"
      },
      "outputs": [],
      "source": [
        "# fazer auth_Twitter uma subclasse de pesquisar_tweets para herdar o api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmuNrfoS7pC-"
      },
      "outputs": [],
      "source": [
        "class auth_twitter(object):\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def parametros_de_acesso(self, arquivo):\n",
        "    \n",
        "    self.__arquivo = arquivo\n",
        "\n",
        "    with open (arquivo, 'r') as arquivo:\n",
        "      texto = arquivo.readlines()[0].split(',')\n",
        "    \n",
        "    access_token = texto[0].split('=')[1]\n",
        "    access_token_secret = texto[1].split('=')[1]\n",
        "    bearer_token = texto[2].split('=')[1]\n",
        "    consumer_key = texto[3].split('=')[1]\n",
        "    consumer_secret = texto[4].split('=')[1]\n",
        "\n",
        "    arquivo.close()\n",
        "\n",
        "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_token, access_token_secret)\n",
        "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "    \n",
        "    return api"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzWKDB6gMX6J"
      },
      "source": [
        "DATA EXCTRACTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL_XrvFE1GBp"
      },
      "outputs": [],
      "source": [
        "class pesquisar_tweets(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.resultados_da_pesquisa = list()\n",
        "    self.historico_info_participantes = list()\n",
        "    self.informacoes_hist_participante = list()\n",
        "    self.perfis_a_excluir = set(termos_de_pesquisa().participantes_totais()) | set(termos_de_pesquisa().perfis_oficiais_bbb)\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def publico(self, query: tuple, result_type='popular', count=int(10)):\n",
        "\n",
        "    \"Esta consulta por tweets exclui perfis oficiais ou de participantes para torná-la puramente pública.\"\n",
        "\n",
        "    global api\n",
        "\n",
        "    self.__query = query\n",
        "    self.__result_type = result_type\n",
        "    self.__count = count\n",
        "\n",
        "    for status in tweepy.Cursor(api.search,\n",
        "                                    q=self.__query,\n",
        "                                    result_type=self.__result_type,\n",
        "                                    count=self.__count,\n",
        "                                    exclude_replies=True).items(1000):\n",
        "\n",
        "            _ = json.dumps(status._json)\n",
        "            x = json.loads(_)\n",
        "            if x['user']['screen_name'] not in self.perfis_a_excluir:\n",
        "              self.resultados_da_pesquisa.append(status)\n",
        "\n",
        "    return self.resultados_da_pesquisa, self.__result_type, 'Publico'\n",
        " \n",
        "  @functools.lru_cache(maxsize=128)\n",
        "  def por_termo(self, query: tuple, result_type='recent', count=int(100)):\n",
        "    \n",
        "    \"\"\"Este método faz a consulta de tweets a partir de termos. \n",
        "    Query: a query deve ser uma tupla (str) utilizando-se de operadores lógicos\n",
        "    tais como OR, AND. Ex. BBB AND #REDEBBB. \n",
        "    Result_type: pode ser ou popular ou recent. O primeiro buscará os tweets\n",
        "    que corresponderem a query publicados recentemente. O segundo será aqueles\n",
        "    tweets mais populares. O padrão é recent.\n",
        "    Count: recebe um valor inteiro para limitar o número de tweets coletados por consulta. O padrão é 100.\n",
        "    Retorna os resultados da pesquisa (lista) e o tipo de resultado (popular ou recent).\n",
        "    \"\"\"\n",
        "\n",
        "    global api\n",
        "\n",
        "    self.__query = query\n",
        "    self.__result_type = result_type\n",
        "    self.__count = count\n",
        "\n",
        "    for status in tweepy.Cursor(api.search,\n",
        "                                q=self.__query,\n",
        "                                result_type=self.__result_type,\n",
        "                                count=self.__count).items():\n",
        "\n",
        "                                self.resultados_da_pesquisa.append(status)\n",
        "\n",
        "    return self.resultados_da_pesquisa, self.__result_type\n",
        "  \n",
        "  @functools.lru_cache(maxsize=128)\n",
        "  def por_usuario(self, participante: tuple, result_type: str ='recent', count: int =int(100)):\n",
        "    \n",
        "    \"\"\"Este método faz a consulta de tweets a partir do nome de um usuário. \n",
        "    Participante: recebe o screen_name do usuário do twitter. \n",
        "    Result_type: pode ser ou popular ou recent. O primeiro buscará os tweets\n",
        "    que corresponderem a query publicados recentemente. O segundo será aqueles\n",
        "    tweets mais populares. O padrão é recent.\n",
        "    Count: recebe um valor inteiro para limitar o número de tweets coletados por consulta. O padrão é 100.\n",
        "    Retorna os resultados da pesquisa em uma lista, o tipo de resultado (popular ou recent) e o tipo de\n",
        "    participante (camarote, pipoca, público ou perfil oficial).\n",
        "    \"\"\"\n",
        "\n",
        "    global api\n",
        "\n",
        "    self.__participante = participante\n",
        "    self.__result_type = result_type\n",
        "    self.__count = count\n",
        "    \n",
        "    tipo_participante = informacoes_de_participantes().checar_tipo_participante(self.__participante)\n",
        "\n",
        "    for status in tweepy.Cursor(api.user_timeline,\n",
        "                                screen_name=self.__participante,\n",
        "                                result_type=self.__result_type,\n",
        "                                exclude_replies=False).items(self.__count):\n",
        "                                \n",
        "                                self.resultados_da_pesquisa.append(status)\n",
        "\n",
        "    return self.resultados_da_pesquisa, self.__result_type, tipo_participante\n",
        "\n",
        "  \n",
        "  def por_usuarios(self, usuarios: tuple, result_type: str='recent', count: int =int(100)):\n",
        "    \n",
        "    \"\"\"Este método faz a consulta de tweets a partir do nome de pelo menos um usuário. \n",
        "    Participante: recebe o screen_name dos usuários do twitter que deseja ser consultado. \n",
        "    Result_type: pode ser ou popular ou recent. O primeiro buscará os tweets\n",
        "    que corresponderem a query publicados recentemente. O segundo será aqueles\n",
        "    tweets mais populares. O padrão é recent.\n",
        "    Count: recebe um valor inteiro para limitar o número de tweets coletados por consulta. O padrão é 100.\n",
        "    Retorna uma lista com os resultados da pesquisa, o tipo de resultado (popular ou recent) e o tipo de pesquisa (única ou múltipla).\n",
        "    \"\"\"\n",
        "\n",
        "    global api\n",
        "\n",
        "    self.__result_type = result_type\n",
        "    self.__count = count\n",
        "    self.__usuarios = usuarios\n",
        "    \n",
        "    for self._participante in self.__usuarios:\n",
        "\n",
        "      for status in tweepy.Cursor(api.user_timeline,\n",
        "                                  screen_name=self._participante,\n",
        "                                  result_type=self.__result_type,\n",
        "                                  exclude_replies=False).items(self.__count):\n",
        "                                  \n",
        "                                  self.resultados_da_pesquisa.append(status)\n",
        "\n",
        "    return self.resultados_da_pesquisa, self.__result_type, 'Todos'\n",
        "\n",
        "  @functools.lru_cache(maxsize=128)\n",
        "  def infos_do_usuario(self, usuario: str) -> list:\n",
        "  \n",
        "    \"\"\"Este método faz a consulta de informações do usuário (número de seguidores (int), número de perfis que segue (int),\n",
        "    número de listados (int), número de tweets favoritados (int), número de tweets totais (int), verificado (booleano) e a data\n",
        "    é uma informação gerada pelo método \"corrigir_timezone\" cujo padrão é a hora, a fim de organizar os arquivos nas devidas pastas). \n",
        "    Usuario: recebe o screen_name do usuário do twitter que deseja ser consultado.\n",
        "    Retorna uma lista.\n",
        "    \"\"\"\n",
        "\n",
        "    global api\n",
        "\n",
        "    self.usuario = usuario\n",
        "\n",
        "    self.__user_results = api.get_user(self.usuario)\n",
        "\n",
        "    extracao = organizar_coletas_de_tweets.corrigir_timezone(tipo='hora')\n",
        "    \n",
        "    self.informacoes_hist_participante.append({'participante': str(self.__user_results.name),\n",
        "                                          'twitter': str(self.__user_results.screen_name),\n",
        "                                          'n_seguidores': int(self.__user_results.followers_count),\n",
        "                                          'n_seguindo': int(self.__user_results.friends_count),\n",
        "                                          'n_de_listados': int(self.__user_results.listed_count),\n",
        "                                          'n_tweets_favoritados': int(self.__user_results.favourites_count),\n",
        "                                          'num_de_tweets_totais': int(self.__user_results.statuses_count),\n",
        "                                          'verificado': bool(self.__user_results.verified),\n",
        "                                          'data': extracao})\n",
        "    \n",
        "    return self.informacoes_hist_participante\n",
        "\n",
        "  \n",
        "  def historico_participante(self, participantes: list):\n",
        "\n",
        "    \"\"\"Este métood visa, a partir da busa por usuario, iterar em uma lista de usuários e fazer uma varredura por todos os participantes\n",
        "    buscando as informações históricas (citadas em infos_do_usuario).\n",
        "    Participantes: recebe uma lista de screen_names para iteração.\n",
        "    Retorna uma lista com todas as informações coletadas.\n",
        "    \"\"\"\n",
        "\n",
        "    for self._participante in participantes:\n",
        "      _ = self.infos_do_usuario(self._participante)\n",
        "      self.historico_info_participantes.extend(_)\n",
        "    \n",
        "    return self.historico_info_participantes, 'historico', 'todos'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi2o1bokMVPW"
      },
      "source": [
        "DATA ORGANIZATION/FILE HANDLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUzJV9SfHLw5"
      },
      "outputs": [],
      "source": [
        "class organizar_coletas_de_tweets(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.dicionario = {}\n",
        "    self.lista = []\n",
        "    self.arquivo_consolidado = []\n",
        "    self.arquivos_a_consolidar = []\n",
        "    self.consolidados = f'/content/drive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/DataFrames/Consolidados/'\n",
        "    self.diretorio = f\"/content/drive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/DataFrames/\"\n",
        "\n",
        "  def criar_dicionario(self, tweets_status, tipo):\n",
        "\n",
        "    d1 = organizar_coletas_de_tweets.corrigir_timezone(tipo='data')\n",
        "\n",
        "    self.__tweets_status = tweets_status\n",
        "    self.__tipo = tipo\n",
        "\n",
        "    for each_json_tweet in tweets_status:\n",
        "      _ = json.dumps(each_json_tweet._json)\n",
        "      \n",
        "      if d1 not in self.dicionario.keys():\n",
        "        tweet = {d1: [json.loads(_)]}\n",
        "        self.dicionario.update(tweet)\n",
        "        \n",
        "      else:\n",
        "        self.dicionario[d1].append(json.loads(_))\n",
        "      \n",
        "    return self.dicionario, self.__tipo\n",
        "\n",
        "  def dataframe_tweets(self, dicionario, tipo_de_pesquisa, multipla=False):\n",
        "    \n",
        "    d1 = organizar_coletas_de_tweets.corrigir_timezone(tipo='data')\n",
        "    \n",
        "    self.__dicionario = dicionario\n",
        "    self.__tipo_de_pesquisa = tipo_de_pesquisa\n",
        "\n",
        "    for i in range(0, len(dicionario[d1])):\n",
        "\n",
        "          tweet_id = self.__dicionario[d1][i]['id']\n",
        "          text = self.__dicionario[d1][i]['text']\n",
        "          favorite_count = self.__dicionario[d1][i]['favorite_count']\n",
        "          retweet_count = self.__dicionario[d1][i]['retweet_count']\n",
        "          created_at = self.__dicionario[d1][i]['created_at']\n",
        "          hashtags = self.__dicionario[d1][i]['entities']['hashtags']\n",
        "          user_mentions = self.__dicionario[d1][i]['entities']['user_mentions']\n",
        "          name = self.__dicionario[d1][i]['user']['name']\n",
        "          screen_name = self.__dicionario[d1][i]['user']['screen_name']\n",
        "\n",
        "          self.lista.append({'tweet_id': str(tweet_id),\n",
        "                        'text': str(text),\n",
        "                        'favorite_count': int(favorite_count),\n",
        "                        'retweet_count': int(retweet_count),\n",
        "                        'created_at': created_at,\n",
        "                        'user_mentions': user_mentions,\n",
        "                        'name': name,\n",
        "                        'screen_name': screen_name})\n",
        "          \n",
        "    self.__tweet_json_ = pd.DataFrame(self.lista, columns = \n",
        "                              ['tweet_id', 'text', \n",
        "                                'favorite_count', 'retweet_count', \n",
        "                                'created_at', 'hashtags', \n",
        "                                'user_mentions', 'name',\n",
        "                                'screen_name'])\n",
        "    \n",
        "    if multipla == False:\n",
        "      \n",
        "      self.__tipo_participante = informacoes_de_participantes().checar_tipo_participante(screen_name)\n",
        "    \n",
        "    else:\n",
        "      \n",
        "      self.__tipo_participante = 'Todos'\n",
        "\n",
        "    return self.__tweet_json_, self.__tipo_participante, self.__tipo_de_pesquisa\n",
        "\n",
        "  def dataframe_dos_participantes(self, informacoes_hist_participante):\n",
        "\n",
        "      self.__informacoes_hist_participante = informacoes_hist_participante\n",
        "\n",
        "      self.__dados_participantes = pd.DataFrame(self.__informacoes_hist_participante, columns = \n",
        "                                      ['participante', 'twitter', 'n_seguidores', \n",
        "                                      'n_seguindo','n_de_listados', 'n_tweets_favoritados',\n",
        "                                      'num_de_tweets_totais', 'verificado', 'data'])\n",
        "      \n",
        "      return self.__dados_participantes, 'historico', None\n",
        "  \n",
        "  @staticmethod\n",
        "  def salvar_arquivo(dataframe, tipo_pesquisa, tipo_participante):\n",
        "    \n",
        "    data_extracao = organizar_coletas_de_tweets.corrigir_timezone(tipo='data')\n",
        "    dir_csvs = f'/content/drive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/DataFrames/'\n",
        "\n",
        "    if tipo_pesquisa == 'historico':\n",
        "           \n",
        "      file_variable_ = dir_csvs + 'Participantes'\n",
        "      file_name = 'historico.csv'\n",
        "      file_path_ = file_variable_ + \"/\" + file_name\n",
        "      organizar_coletas_de_tweets.diretorios(file_variable=file_variable_, file_path=file_path_, dataframe=dataframe)\n",
        "    \n",
        "    else:\n",
        "\n",
        "      file_variable_ = dir_csvs + \"/\" + tipo_participante  + \"/\" + tipo_pesquisa + \"/\" + data_extracao\n",
        "      file_name = tipo_participante[0:3] + \"_\" + tipo_pesquisa[0:3] + \"_\" + data_extracao + '.csv'\n",
        "      file_path_ = file_variable_ + \"/\" + file_name\n",
        "      organizar_coletas_de_tweets.diretorios(file_variable=file_variable_, file_path=file_path_, dataframe=dataframe)\n",
        "\n",
        "  @staticmethod     \n",
        "  def diretorios(file_variable, file_path, dataframe):\n",
        "\n",
        "    if not os.path.exists(file_variable):\n",
        "      os.makedirs(file_variable)\n",
        "      dataframe.to_csv(file_path, index=False)\n",
        "\n",
        "    else:\n",
        "      with open(file_path, 'a') as f_object:\n",
        "        writer_object = writer(f_object, delimiter=',')\n",
        "        \n",
        "        linhas = []\n",
        "        \n",
        "        for row in range(0, len(dataframe)):\n",
        "          linhas.append([row for row in dataframe.iloc[row]])\n",
        "          writer_object.writerow(linhas[row])\n",
        "\n",
        "        f_object.close()\n",
        "    \n",
        "  @staticmethod\n",
        "  def corrigir_timezone(tipo):\n",
        "    \n",
        "    utcmoment_naive = datetime.utcnow()\n",
        "    utcmoment = utcmoment_naive.replace(tzinfo=pytz.utc)\n",
        "    tz = 'America/Sao_Paulo'\n",
        "    extracao = utcmoment.astimezone(pytz.timezone(tz)) - timedelta(hours=0, minutes=60)\n",
        "    data_de_extracao = extracao.date().strftime(\"%d%m%Y\")\n",
        "\n",
        "    if tipo == 'data':\n",
        "      return data_de_extracao\n",
        "    \n",
        "    elif tipo == 'hora':\n",
        "      return extracao\n",
        "\n",
        "  \n",
        "#arquivos_consolidados = {'Camarote':[],\n",
        "#                         'Pipoca': [],\n",
        "#                         'Publico': [],\n",
        "#                         'PerfilOficial': [],\n",
        "#                         'Todos': []}\n",
        "  \n",
        "  def consolidar(self, name, path, tipo):\n",
        "\n",
        "    self.arquivo_consolidado.append(name)\n",
        "    self.arquivos_a_consolidar.append(pathlib.PurePath(path, name))\n",
        "    self.combined_csv = pd.concat([pd.read_csv(f, usecols=['tweet_id', 'text', 'screen_name']) for f in self.arquivos_a_consolidar])\n",
        "    self.diretorio_a_consolidar = pathlib.PurePath(self.consolidados + tipo,'consolidado_' + tipo + '.csv')\n",
        "    self.combined_csv.to_csv(self.diretorio_a_consolidar, index=False )\n",
        "\n",
        "  \n",
        "  def consolidar_total(self):\n",
        "\n",
        "    for path, dirs, files in os.walk(self.diretorio, topdown=False):\n",
        "      \n",
        "      for name in files:\n",
        "        if 'Camarote' in path:\n",
        "          _ = 'Camarote'\n",
        "          self.consolidar(name=name, path=path, tipo=_)\n",
        "                \n",
        "        elif 'Pipoca' in path:\n",
        "          _ = 'Pipoca'\n",
        "          self.consolidar(name=name, path=path, tipo=_)\n",
        "\n",
        "        elif 'Publico' in path:\n",
        "          _ = 'Publico'\n",
        "          self.consolidar(name=name, path=path, tipo=_)\n",
        "        \n",
        "        elif 'PerfilOficial' in path:\n",
        "          _ = 'PerfilOficial'\n",
        "          self.consolidar(name=name, path=path, tipo=_)\n",
        "        \n",
        "        elif 'Todos' in path:\n",
        "          _ = 'Todos'\n",
        "          self.consolidar(name=name, path=path, tipo=_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tWq-LFkMPmL"
      },
      "source": [
        "PARTICIPANTS INFORMATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu2x5fyPALbL"
      },
      "outputs": [],
      "source": [
        "class informacoes_de_participantes(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  @staticmethod\n",
        "  def checar_tipo_participante(usuario):\n",
        "      if usuario in termos_de_pesquisa().participantes_camarote:\n",
        "        return 'Camarote'\n",
        "      elif usuario in termos_de_pesquisa().participantes_pipoca:\n",
        "        return 'Pipoca'\n",
        "      elif usuario in termos_de_pesquisa().perfis_oficiais_bbb:\n",
        "        return 'PerfilOficial'\n",
        "      else:\n",
        "        return 'Publico'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdXXGgr9MKZG"
      },
      "source": [
        "TEXT OPERATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbQrOBweyleK"
      },
      "outputs": [],
      "source": [
        "class termos_de_pesquisa(object):\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.__participantes_totais = []\n",
        "    self.participantes_camarote = ['Aguiarthur', 'Naiarazevedo', 'PedroScooby', 'brunnagoncalves', \n",
        "                          'iampauloandre', 'eumaria', 'jadepicon', 'Silva_DG', 'linndaquebrada', 'TiagoAbravanel']\n",
        "\n",
        "    self.participantes_pipoca = ['Dra_laiscaldass', 'LucianoEstevan', 'a_jessilane', 'Eli', 'eslomarques', \n",
        "                      'bbaheck', 'oficialmussi', 'oficial_deodato', 'vyniof', 'LucasBissoli_']\n",
        "\n",
        "    self.participantes_camarote_twitter = ['@Aguiarthur', '@Naiarazevedo', '@PedroScooby', '@brunnagoncalves', \n",
        "                        '@iampauloandre', '@eumaria', '@jadepicon', '@Silva_DG', '@linndaquebrada', '@TiagoAbravanel']\n",
        "\n",
        "    self.participantes_pipoca_twitter = ['@Dra_laiscaldass', '@LucianoEstevan', '@a_jessilane', '@eusouoeli', '@eslomarques', \n",
        "                      '@bbaheck', '@oficialmussi', '@oficial_deodato', '@vyniof', '@LucasBissoli_']\n",
        "\n",
        "    self.perfis_oficiais_bbb = ['bbb', 'globoplay', 'tadeuschmidt']\n",
        "\n",
        "    self.perfis_oficiais_bbb_twitter = ['@bbb', '@globoplay', '@tadeuschmidt']\n",
        "\n",
        "  def termos(self):\n",
        "    \n",
        "    self.dicionario_de_termos_de_pesquisa = {'BigBrotherBrasil': {'BBB', 'BBB22', '#REDEBBB', '#BBB', '#BBB22'},\n",
        "         'Participantes': {'Arthur': ['Arthur', 'Artur', 'Aguiar', 'Arthur Aguiar', '\\u2747\\ufe0f', '#TeamAguiar', '#TeamArthurAguiar'],\n",
        "                           'Naiara Azevedo': ['Naiara', 'Azevedo', '1f4b8', '#TeamNaiara', 'Nai', 'cantora'],\n",
        "                           'Pedro Scooby': ['Pedro', 'Scooby', '1f30a', '#TeamScooby', '#TimeScooby', '1f499'],\n",
        "                           'Brunna Gonçalves': ['Brunna', 'Gonçalves', '1f984', '#BBBRUNNA', 'BBBrunna'],\n",
        "                           'Paulo André': ['Paulo', 'André', '1f3c1', '#TeamPauloAndré', 'TeamPauloAndre'],\n",
        "                           'Maria': ['Maria', '1f40d', '#TEAMMARIA', 'TIME MARIA', 'MARICONAS'],\n",
        "                           'Jade Picon': ['Jade', 'Picon', '1f32a\\ufe0f', 'furacão', '#TeamJade', 'Picão', 'KdJade', 'Picões', 'Furacao'],\n",
        "                           'Douglas Silva': ['Douglas', 'Silva', '1f3b2', 'dado', 'dadinho', '#TeamDouglasSilva', '1f44a\\U0001f3ff'],\n",
        "                           'Linn da Quebrada': ['Linn', 'Quebrada', '1f9dc\\u200d\\u2640\\ufe0f', '1f9dc\\U0001f3ff\\u200d\\u2640\\ufe0f',\n",
        "                                                '1f9dc\\U0001f3fb\\u200d\\u2640\\ufe0f', '1f9dc\\U0001f3fe\\u200d\\u2640\\ufe0f', '1f9dc\\U0001f3fc\\u200d\\u2640\\ufe0f',\n",
        "                                                '1f9dc\\U0001f3fd\\u200d\\u2640\\ufe0f', '1f9dc', '1f9dc\\U0001f3ff', '1f9dc\\U0001f3fb', '1f9dc\\U0001f3fe',\n",
        "                                                '1f9dc\\U0001f3fc', '1f9dc\\U0001f3fd', '#TeamLinn', '#linndonas', '#LinnDonas'],\n",
        "                           'Tiago Abravanel': ['Tiago', 'Abravanel', '1f43b', '#TeamAbrava', '#TeamAbravanel', '1f9f8'],\n",
        "                           'Laís Caldas': ['Laís', 'Caldas', '1f462', '#TeamLais', '#TeamLaís', 'Time Lais', '1f49a'],\n",
        "                           'Luciano Estevan': ['Luciano', 'Estevan', '1f981', '#TeamLucianoEstevan', 'Lu', '1f346'],\n",
        "                           'Jessilane': ['Jessilane', '1f9ec', 'Jessi', '#TeamJessi', '#TimeJessi', '1f49c', 'Charmanders'],\n",
        "                           'Eliezer': ['Eliezer', '1F437', 'Eli', '#TeamEli', '1F953', '1F416', '1F43D'],\n",
        "                           'Eslovênia Marques': ['Eslovênia', 'Eslováquia', 'Marques', 'Eslô', '1f1f8\\U0001f1ee',\n",
        "                                                 'Time Eslo', 'Team Eslo', '#TimeEslô', '#TeamEslô', 'Eslovenia'],\n",
        "                           'Bábara Heck': ['Bárbara', 'Heck', '1f980', '#TeamBá', 'TeamBá', 'TimeBá',\n",
        "                                           'TimeBa', 'TeamBa', 'TeamBah', 'BBBah', 'Bá', 'Ba', '1f9a6'],\n",
        "                           'Rodrigo Mussi': ['Rodrigo Mussi', '#TeamMussi', \"BBB\", \"BIG BROTHER BRASIL\",\n",
        "                                             '#TimeMussi', '1f977\\U0001f3ff', '1f977', '1f977\\U0001f3fb',\n",
        "                                             '1f977\\U0001f3fe', '1f977\\U0001f3fc', '1f977\\U0001f3fd', 'ninja', 'ninjas', '#TEAMNINJA'],\n",
        "                           'Natália Deodato': ['Natália', 'Deodato', '#TeamNaty', '#TimeNaty', 'Naty', 'vitilindos',\n",
        "                                               '1f483', '1f483\\U0001f3ff', '1f483\\U0001f3fb',\n",
        "                                               '1f483\\U0001f3fe', '1f483\\U0001f3fc', '1f483\\U0001f3fd'],\n",
        "                           'Vinicius': ['Vinicius', 'TeamVyni', '#TimeVyni', '#TeamVyni', '1f4a1'],\n",
        "                           'Lucas Bissoli': ['Lucas', 'Bissoli', '#TimeBissoli', '#TeamBissoli', \n",
        "                                             '1f3c4\\u200d\\u2642\\ufe0f', '1f3c4\\U0001f3ff\\u200d\\u2642\\ufe0f', \n",
        "                                             '1f3c4\\U0001f3fb\\u200d\\u2642\\ufe0f', '1f3c4\\U0001f3fe\\u200d\\u2642\\ufe0f',\n",
        "                                             '1f3c4\\U0001f3fc\\u200d\\u2642\\ufe0f', '1f3c4\\U0001f3fd\\u200d\\u2642\\ufe0f',\n",
        "                                             '1f3c4', '1f3c4\\U0001f3ff', '1f3c4\\U0001f3fb', '1f3c4\\U0001f3fe',\n",
        "                                             '1f3c4\\U0001f3fc', '1f3c4\\U0001f3fd', '1f3c4\\u200d\\u2640\\ufe0f',\n",
        "                                             '1f3c4\\U0001f3ff\\u200d\\u2640\\ufe0f', '1f3c4\\U0001f3fb\\u200d\\u2640\\ufe0f',\n",
        "                                             '1f3c4\\U0001f3fe\\u200d\\u2640\\ufe0f', '1f3c4\\U0001f3fc\\u200d\\u2640\\ufe0f',\n",
        "                                             '1f3c4\\U0001f3fd\\u200d\\u2640\\ufe0f']}}\n",
        "    return self.dicionario_de_termos_de_pesquisa\n",
        "    #termos_a_analisar = []\n",
        "\n",
        "    #avaliador_de_termos()\n",
        "\n",
        "    #return termos_de_pesquisa_BBB\n",
        "\n",
        "    #analisador_de_termos(termos_de_pesquisa_participantes)\n",
        "\n",
        "  def analisador_de_termos(self, termos_atuais, termos_novos):\n",
        "\n",
        "    self.__termos_atuais = termos_atuais\n",
        "    self.__termos_novos = termos_novos\n",
        "\n",
        "    #for termo in termos_atuais:\n",
        "      # if novo termo ajudar a explicar:\n",
        "      # append\n",
        "      # if len(termos) > limite:\n",
        "      # termo_a_remover = ()\n",
        "      # for termo in termos atuais atualizado:\n",
        "      # termo_a_remover['explicacao'] < min_a_explicar:\n",
        "      # termos_atuais.pop(termo_a_remover)\n",
        "\n",
        "  def tendencia(self, termo_novo):\n",
        "\n",
        "    self.termo_novo = termo_novo\n",
        "    pass\n",
        "\n",
        "  def correlacao_participantes(self):\n",
        "    pass    \n",
        "\n",
        "  def dicionario_participantes(self):\n",
        "\n",
        "    self.dicionario_de_participantes = {'@Aguiarthur': 'Arthur Aguiar', \n",
        "                            '@Naiarazevedo': 'Naiara Azevedo',\n",
        "                            '@PedroScooby': 'Pedro Scooby',\n",
        "                            '@brunnagoncalves': 'Bruna Gonçalves',\n",
        "                            '@iampauloandre': 'Paulo André',\n",
        "                            '@eumaria': 'Maria',\n",
        "                            '@jadepicon': 'Jade Picon',\n",
        "                            '@Silva_DG': 'Douglas Silva',\n",
        "                            '@linndaquebrada': 'Linn da Quebrada',\n",
        "                            '@TiagoAbravanel': 'Tiago Abravanel',\n",
        "                            '@Dra_laiscaldass': 'Laís Caldas',\n",
        "                            '@LucianoEstevan': 'Luciano Estevan',\n",
        "                            '@a_jessilane': 'Jessilane',\n",
        "                            '@eusouoeli': 'Eliezer',\n",
        "                            '@eslomarques': 'Eslovênia Maques',\n",
        "                            '@bbaheck': 'Bábara Heck',\n",
        "                            '@oficialmussi': 'Rodrigo Mussi',\n",
        "                            '@oficial_deodato': 'Natália Deodato',\n",
        "                            '@vyniof': 'Vinicius',\n",
        "                            '@LucasBissoli_': 'Lucas Bissoli'}\n",
        "\n",
        "    return self.dicionario_de_participantes\n",
        "  \n",
        "  def lista_participantes(self, tipo_participante, perfil=True):     \n",
        "\n",
        "    if tipo_participante == 'camarote':\n",
        "      if perfil:\n",
        "        return self.participantes_camarote_twitter\n",
        "      else:\n",
        "        return self.participantes_camarote\n",
        "    elif tipo_participante == 'pipoca':\n",
        "      if perfil:\n",
        "        return self.participantes_pipoca_twitter\n",
        "      else:\n",
        "        return self.participantes_pipoca\n",
        "    elif tipo_participante == 'oficial':\n",
        "      if perfil:\n",
        "        return self.perfis_oficiais_bbb_twitter\n",
        "      else:\n",
        "        return self.perfis_oficiais_bbb\n",
        "\n",
        "  def participantes_totais(self):\n",
        "\n",
        "    self.__participantes_totais.extend(self.participantes_pipoca)\n",
        "    self.__participantes_totais.extend(self.participantes_camarote)\n",
        "\n",
        "    return self.__participantes_totais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUaqg0LXjs8l"
      },
      "source": [
        "TESTES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af5Ep4TuZbTp"
      },
      "outputs": [],
      "source": [
        "api = auth_twitter().parametros_de_acesso('/content/drive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/auth.txt')\n",
        "pesquisar = pesquisar_tweets()\n",
        "organizar = organizar_coletas_de_tweets()\n",
        "pesquisas = termos_de_pesquisa()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m2OSKZl5MeN"
      },
      "source": [
        "PESQUISAS POR NOME DE USUARIO(S)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyDlkJwE5O2I"
      },
      "outputs": [],
      "source": [
        "tweets, tipo_de_pesquisa, tipo_de_participante = pesquisar.por_usuarios(usuarios=pesquisas.participantes_totais())\n",
        "dicionario, tipo_de_pesquisa = organizar.criar_dicionario(tweets, tipo_de_pesquisa)\n",
        "dataframe, tipo_de_participante, tipo_de_pesquisa = organizar.dataframe_tweets(dicionario, tipo_de_pesquisa, multipla=True)\n",
        "organizar.salvar_arquivo(tipo_pesquisa=tipo_de_pesquisa, dataframe=dataframe, tipo_participante=tipo_de_participante)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnL3PgPJMV_J"
      },
      "source": [
        "HISTÓRICO DOS PARTICIPANTES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilwiR0-WvgMZ"
      },
      "outputs": [],
      "source": [
        "tweets, tipo_de_pesquisa, tipo_de_participante = pesquisar.historico_participante(pesquisas.participantes_totais())\n",
        "dataframe, tipo_de_pesquisa, tipo_de_participante = organizar.dataframe_dos_participantes(tweets)\n",
        "organizar.salvar_arquivo(dataframe=dataframe, tipo_pesquisa=tipo_de_pesquisa, tipo_participante=tipo_de_participante)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-s4JoBObuUH"
      },
      "source": [
        "PESQUISA PÚBLICO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-3mUz1bbxWJ"
      },
      "outputs": [],
      "source": [
        "tweets, tipo_de_pesquisa, tipo_de_participante = pesquisar.publico('BBB -filter:retweets', result_type='popular')\n",
        "dicionario, tipo_de_pesquisa = organizar.criar_dicionario(tweets, tipo_de_pesquisa)\n",
        "dataframe, tipo_de_participante, tipo_de_pesquisa = organizar.dataframe_tweets(dicionario, tipo_de_pesquisa, multipla=False)\n",
        "organizar.salvar_arquivo(tipo_pesquisa=tipo_de_pesquisa, dataframe=dataframe, tipo_participante=tipo_de_participante)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7ANX5byMa0q"
      },
      "source": [
        "PESQUISA POR TERMO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "arfCUelVHepm",
        "outputId": "f68278ac-acc7-40a4-84ee-4538f512ce2b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"('Rodrigo Mussi'  '#TeamMussi')\""
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "termos = termos_de_pesquisa().termos()\n",
        "options = termos['Participantes']['Rodrigo Mussi']\n",
        "combinacao_de_termos_a_pesquisar_do_usuario = list(combinations(options,2))\n",
        "query = [str(item).replace(',', ' ') for item in combinacao_de_termos_a_pesquisar_do_usuario]\n",
        "query[0]\n",
        "#pesquisar.por_termo(query=\"(Rodrigo BBB22 -tag -mutirão) -from:bbb -from:tracklist -from:bchartsnet -from:multishow - from:globoplay -is:retweet\", result_type='popular', count=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zy4tqzy0cVrj"
      },
      "outputs": [],
      "source": [
        "tweets, tipo_de_pesquisa = pesquisar.por_termo(query=\"(BBB22 -tag -mutirão) -from:bbb -from:globoplay -from:tracklist -from:bchartsnet -from:multishow -is:retweet\", result_type='popular', count=100)\n",
        "dicionario, tipo_de_pesquisa = organizar.criar_dicionario(tweets, tipo_de_pesquisa)\n",
        "dataframe, tipo_participante, tipo_de_pesquisa = organizar.dataframe_tweets(dicionario, tipo_de_pesquisa)\n",
        "organizar.salvar_arquivo(tipo_participante=tipo_participante, tipo_pesquisa=tipo_de_pesquisa, dataframe=dataframe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn0nzU5z4RvS"
      },
      "source": [
        "PESQUISA PERFIL OFICIAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7oXnKqfIzYR"
      },
      "outputs": [],
      "source": [
        "tweets, tipo_de_pesquisa, tipo_de_participante = pesquisar.por_usuario('@bbb')\n",
        "dicionario, tipo_de_pesquisa = organizar.criar_dicionario(tweets, tipo_de_pesquisa)\n",
        "dataframe, tipo_de_participante, tipo_de_pesquisa = organizar.dataframe_tweets(dicionario, tipo_de_pesquisa, multipla=False)\n",
        "organizar.salvar_arquivo(tipo_pesquisa=tipo_de_pesquisa, dataframe=dataframe, tipo_participante=tipo_de_participante)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anLqMdhiPJwq"
      },
      "outputs": [],
      "source": [
        "organizar.consolidar_total()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emUJ6NgeWJbO"
      },
      "source": [
        "NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "x2TvzwPJ3NJW"
      },
      "outputs": [],
      "source": [
        "class processamento_de_texto(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.texto_processado = []\n",
        "    self.texto_sendo_analisado = []\n",
        "    self.texto_sem_pontuacao = []\n",
        "    self.texto_sem_stopwords = []\n",
        "    self.texto_lematizado = []\n",
        "    self.texto_tokenizado = []\n",
        " \n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def remover_https_e_espacamento(self, x): \n",
        "\n",
        "      self.__sem_https = re.sub(r'https.+', '', str(x))\n",
        "      self.__sem_arroba = re.sub(r'@+', '', self.__sem_https)\n",
        "      self.__sem_caractere_de_espacamento = re.sub(r'\\n+', ' ', self.__sem_arroba)\n",
        "\n",
        "      return self.__sem_caractere_de_espacamento\n",
        "  \n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def remover_pontuacao(self, x):\n",
        "    \n",
        "      self.__s = re.sub(r'\\W', ' ', x)\n",
        "      self.__x = re.sub(' +', ' ', self.__s)\n",
        "      \n",
        "      return self.__x\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def remover_stopwords(self, x):  \n",
        "\n",
        "      return \" \".join([palavra for palavra in x.split() if palavra.lower() not in nltk.corpus.stopwords.words('portuguese')])\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def lemmatizar(self, x):\n",
        "\n",
        "      return ' '.join([token.lemma_ for token in nlp(x)])\n",
        "  \n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def tokenizar_texto(self, x):\n",
        "        \n",
        "      return word_tokenize(x)\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def traduzir(self, x):\n",
        "\n",
        "    if x is not None and not len(x)==0:\n",
        "      time.sleep(0.5)\n",
        "      return Translator().translate(x, dest='en').text\n",
        "      \n",
        "    else:\n",
        "      time.sleep(0.5)\n",
        "      return x\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def preprocessar(self, x):\n",
        "\n",
        "    self.__sem_link_nem_espaco = self.remover_https_e_espacamento(x)\n",
        "    self.__sem_pontuacao = self.remover_pontuacao(self.__sem_link_nem_espaco)  \n",
        "    self.__sem_stopwords = self.remover_stopwords(self.__sem_pontuacao)\n",
        "    self.__lemmatizado = self.lemmatizar(self.__sem_stopwords)\n",
        "    self.__traduzido = self.traduzir(self.__lemmatizado)\n",
        "        \n",
        "    return self.__traduzido\n",
        "  \n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def list_reduction(self, texto_processado):\n",
        "\n",
        "    return [item for sublist in texto_processado[0] for item in sublist]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7EIhyB3FjM2"
      },
      "source": [
        "MELHORIAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTkkDYfUpFWI"
      },
      "outputs": [],
      "source": [
        "# Funcionalidades\n",
        "# property para não precisar \"herdar\" as informações de tipo de pesquisa e tipo de consulta (GETTER)\n",
        "# try except clauses\n",
        "# metodo estático para criar a query a partir de um dicionario dado\n",
        "# metodo para adicionar termos mais relevantes para o modelo *publico e por participante, exemplo, memes\n",
        "#termos mais frequentes para determinado participante\n",
        "# verificar se o arquivo já foi consolidado para não \"retrabalho\"\n",
        "# coletas automáticas/schedule!\n",
        "\n",
        "\n",
        "# Analises\n",
        "# prob de ser eliminado no paredão de acordo com o atual nível de popularidade\n",
        "# modelo de classificação baseado na popularidade do participante\n",
        "# em média, quantos seguidores ganha após um tweet positivo? e quantos perde após um tweet negativo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObZVWpGlOW7p"
      },
      "source": [
        "______________________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN_o49xnNmKB"
      },
      "outputs": [],
      "source": [
        "# def status_do_participante -> eliminado? anjo? lider? acompanhar ao longo do tempo e comparar com o \"desempenho\" no twitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "SSKsT9_DNo9Y"
      },
      "outputs": [],
      "source": [
        "class processar_bases_de_treino(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.train_total = pd.read_csv(f'/content/drive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/DataFrames/BaseDeTreino/base_de_treino.csv')\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def train_fala_do_jogo(self):\n",
        "    self.__subset = self.train_total[['texto_traduzido', 'jogo']]\n",
        "    self._train_is_jogo = [tuple(x) for x in self.__subset.to_numpy()]\n",
        "\n",
        "    return self._train_is_jogo\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def train_intencao_de_voto(self):\n",
        "    self.__subset = self.train_total[['texto_traduzido', 'voto']]\n",
        "    self._train_intencao = [tuple(x) for x in self.__subset.to_numpy()]\n",
        "    \n",
        "    return self._train_intencao\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def train_spam(self):\n",
        "    self.__subset = self.train_total[['texto_traduzido', 'spam']]\n",
        "    self._train_is_spam = [tuple(x) for x in self.__subset.to_numpy()]\n",
        "    \n",
        "    return self._train_is_spam\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def train_sentimento(self):\n",
        "    self.__subset = self.train_total[['texto_traduzido', 'sentimento']]\n",
        "    self._train_sentimento = [tuple(x) for x in self.__subset.to_numpy()]\n",
        "    \n",
        "    return self._train_sentimento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "WOi3YCWxlHR1"
      },
      "outputs": [],
      "source": [
        "class analises_de_tweets(processar_bases_de_treino):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__(self)\n",
        "    self.cl_sent = NaiveBayesClassifier(super().train_sentimento)\n",
        "    self.classifier_jogo = PositiveNaiveBayesClassifier(positive_set=[x for x,_ in list(filter(lambda x: x[1] ==1, super().train_fala_do_jogo()))], unlabeled_set=[x for x,_ in list(filter(lambda x: x[1] ==0, super().train_fala_do_jogo()))])\n",
        "    self.classifier_spam = PositiveNaiveBayesClassifier(positive_set=[x for x,_ in list(filter(lambda x: x[1] ==1, super().train_spam()))], unlabeled_set=[x for x,_ in list(filter(lambda x: x[1] ==0, super().train_spam()))])\n",
        "    self.classifier_spam = PositiveNaiveBayesClassifier(positive_set=[x for x,_ in list(filter(lambda x: x[1] ==1, super().train_intencao_de_voto()))], unlabeled_set=[x for x,_ in list(filter(lambda x: x[1] ==0, super().train_intencao_de_voto()))])\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def classificar_sentimento_do_tweet(self, x):\n",
        "    return self.cl_sent.classify(x)\n",
        "  \n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def classificar_tweet_jogo(self, x):\n",
        "    return self.classifier_jogo.classify(x)\n",
        "  \n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def classificar_intencao_de_voto(self, x):\n",
        "    return self.classifier_itencao.classify(x)\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def classificar_spam(self, x):\n",
        "    return self.classifier.classify(x)\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def analisar_sentimento_do_tweet(self, x):\n",
        "    self.sent = float(TextBlob(x).sentiment[0])\n",
        "    return self.sent\n",
        "\n",
        "  @functools.lru_cache(maxsize=32)\n",
        "  def analisar_polaridade_do_tweet(self, x):\n",
        "    self.pol = float(TextBlob(x).sentiment[1])\n",
        "    return self.pol"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# olhar para o text e identificar sobre qual jogador (es) está falando?\n",
        "publico_06022022 = pd.read_csv(f'/content/drive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/DataFrames/Publico/popular/06022022/Pub_pop_06022022.csv')\n",
        "publico_06022022.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "zsLDgWSGmIet",
        "outputId": "069c482c-cc97-42d9-b0ec-99fc2f3d4788"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-674c1af1-e01a-41d8-8f00-58ffa0cf3e90\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>created_at</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>user_mentions</th>\n",
              "      <th>name</th>\n",
              "      <th>screen_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1490375498865721353</td>\n",
              "      <td>RT @acrimewavee: lembrando aqui de quando o po...</td>\n",
              "      <td>0</td>\n",
              "      <td>111</td>\n",
              "      <td>Sun Feb 06 17:23:03 +0000 2022</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'screen_name': 'acrimewavee', 'name': 'sâmi'...</td>\n",
              "      <td>Pedrinho #BBB22</td>\n",
              "      <td>psjds_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1490375495288033290</td>\n",
              "      <td>RT @_talithafreitas: A única pessoa que entend...</td>\n",
              "      <td>0</td>\n",
              "      <td>288</td>\n",
              "      <td>Sun Feb 06 17:23:02 +0000 2022</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'screen_name': '_talithafreitas', 'name': 'T...</td>\n",
              "      <td>𝒋𝒐𝒚</td>\n",
              "      <td>wtfxjoy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1490375494449176578</td>\n",
              "      <td>RT @romanoandre: Até o @chicobarney eles coloc...</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>Sun Feb 06 17:23:02 +0000 2022</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'screen_name': 'romanoandre', 'name': 'André...</td>\n",
              "      <td>エリカDamasceno🎧🅾+</td>\n",
              "      <td>DamascenoErica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1490375493190828032</td>\n",
              "      <td>RT @Silva_DG: Tamo viciados nessa amizade! Iss...</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>Sun Feb 06 17:23:02 +0000 2022</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'screen_name': 'Silva_DG', 'name': 'Douglas ...</td>\n",
              "      <td>Quezia</td>\n",
              "      <td>quezzymarvila</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1490375493178298368</td>\n",
              "      <td>@readingpace puta\\n\\nlaís luciano jessilane el...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Sun Feb 06 17:23:02 +0000 2022</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'screen_name': 'readingpace', 'name': 'nai☁️...</td>\n",
              "      <td>ana</td>\n",
              "      <td>barrowells</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-674c1af1-e01a-41d8-8f00-58ffa0cf3e90')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-674c1af1-e01a-41d8-8f00-58ffa0cf3e90 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-674c1af1-e01a-41d8-8f00-58ffa0cf3e90');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "              tweet_id  ...     screen_name\n",
              "0  1490375498865721353  ...          psjds_\n",
              "1  1490375495288033290  ...         wtfxjoy\n",
              "2  1490375494449176578  ...  DamascenoErica\n",
              "3  1490375493190828032  ...   quezzymarvila\n",
              "4  1490375493178298368  ...      barrowells\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2aOlWbmG0CS",
        "outputId": "4f3a88ae-072b-4795-a3fc-7545de50bb79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Jajá proving immunity anxious to see Teamscooby prove',\n",
              " 'Do something to think Pedro Scooby advise to do',\n",
              " 'Finished dynamic presentation do like to accompany BBB22',\n",
              " 'Presentation Pedro Personal marrying to know how to know Quantum likes to exchange to know',\n",
              " 'Discussing Fucking Brother Silva_DG BBB22',\n",
              " 'Pedro now throw barbie fucking kkkkkkk with people BBB22',\n",
              " \"comentasteph I'm so happy to receive\",\n",
              " 'Taking advantage of new participant Brothers sisters now make wheel presentations',\n",
              " 'X-ray Scooby today 20 01 BBB22',\n",
              " 'twisted scooby go be so party pedro start drinking',\n",
              " 'blend to dry talk open look schedule lkkkkkkkkkkkkkk',\n",
              " 'Pedro do delinear linn say getting thick another likes to result',\n",
              " 'Linn to find going to have a customer makeup artist there Pedro aviator',\n",
              " 'Dear seeing Linn doing make tiago speak to do equal fantasy',\n",
              " 'Pedro speak leftover eagles know how truth someone confirm',\n",
              " 'Guess where fish',\n",
              " 'Pedro show marry jade know another friend traveling together New Year',\n",
              " 'Brother Sisters new marrying Scooby is there recepting all well come play',\n",
              " 'Now poeminha normal people go gym to work out scooby go to talk rest',\n",
              " 'Pedro count see Floyd Roir stop filming almost take covering Orlhar',\n",
              " 'Good diaaaa team today day prove immunity pro camarote there go twisting Scooby',\n",
              " 'Pedro lie ready to sleep adm go join tá a late time resting for teamscooby',\n",
              " 'Photos are KKKKK BBB22',\n",
              " 'Pedro speak school must look at individuality every student because every person time',\n",
              " 'RT MENDESPIER Finding So Mano Scooby Always Show Interest Subject Person Power Passing 30 Minute Contin',\n",
              " '3h morning Pedro speak over biology adm to impress being able to speak any subject BBB22',\n",
              " 'Scooby so listen to eliezer history',\n",
              " 'Pedro call Gabriel Medina',\n",
              " 'Douglas head so Vyni Scooby explains to praise Carioca having legendar explain',\n",
              " 'Guess it is philosophic send tamo joint always pika final phrase',\n",
              " 'Pedro good to come and talk great people working costs some snacks',\n",
              " 'Pedro Scooby Surfer Businessman Philosopher Now Multifaceted Crisis Manager Call Né BBB22',\n",
              " 'attempt conversion bbb22',\n",
              " 'Pedro explain Brothers arise SCOOBY BBB22',\n",
              " \"Denise Moraes Let's go together\",\n",
              " 'Pedro ugly mistreat vidar different',\n",
              " 'wake up scooby iamo paulo andre athlete olympic beautiful brazil praise huh',\n",
              " 'see video angel pedro say fear pass bad see child send video join cintia',\n",
              " 'Tiago Abravanel',\n",
              " 'Rt artsyeux person wanting to show bad side another clinging minimum detail error person all mun',\n",
              " 'Pedro talk Mary respect creation child relationship said Mear V',\n",
              " 'Play',\n",
              " 'People go wrong people make funny people try to hit',\n",
              " 'Pagodinho want war nobody bbb22',\n",
              " 'Rt vittmirandaa',\n",
              " 'Silva DG Aguiarthur Tiago Abravanel in Paulo Andre Quarteto',\n",
              " 'I lose everything smile bbb22',\n",
              " 'JADEPICON CAUTION D here little Scooby appears swimming talk is cleaning solution',\n",
              " 'titchelx responding own video Scooby speak absolutely nothing Mary just say Like',\n",
              " \"Never deny love I'll make Cupid here like this Scooby do inside\",\n",
              " 'Every beautiful handsome two young man',\n",
              " 'Thousand Utility Scooby Cupid',\n",
              " 'Barbara say vote Scooby because vibe too good',\n",
              " 'Here ask dynamic group TeamScooby',\n",
              " 'RT Vinnizra Mundo Pedrate Scooby Dance Saying Vidar Irar Manir Go Like',\n",
              " 'Tiagoabravanel',\n",
              " 'iamo paulo andre silva dg pedro tidy bank gossip shadow',\n",
              " 'see video angel pedro say fear pass bad see child send video join cintia',\n",
              " 'Rt artsyeux person wanting to show bad side another clinging minimum detail error person all mun',\n",
              " \"I'm fine, I'm going to go there, I'm going to go there.\",\n",
              " 'Bárbara Desgram friend resistance to stay with me BBAHECK BBB22',\n",
              " 'Strong so father reacts full happiness know how to process',\n",
              " 'Besides having to be supporting Laís during anxiety crisis facing today Naiara speak find',\n",
              " 'Convert beautiful exciting true Teamlais BBB22',\n",
              " 'Linzinho Million Official Deodato Redebbb BBB 22 BBB Party',\n",
              " 'Wave Wave Look Wave Redbbb Party BBB BBB22 Viny Of Brunnagoncalves Official Deodato Aguiarthur',\n",
              " 'People make happy people try to hit Redbbb BBB 22 party BBB reproduction globopay',\n",
              " 'People watch party bbb drink bbb22',\n",
              " 'Tiagoabravanel vyniof eusouoeli aaaaaa wonderful heart adm go explode so much love',\n",
              " 'Owwwwwnt tiagoabravanel eusouoeli Redebbb BBB22 Festabbb',\n",
              " 'People Dancing Slow Music Né I Am Eli Redebbb BBB22 Party BBB Reproduction Globlay',\n",
              " 'want to dance then take netbbbbbbbbbbbbbbb vyniof reproduction globopay',\n",
              " 'Lu A_Jessilane Ravar Pistar Redbbb BBB22 Festabbb Globlay Playback',\n",
              " 'Redebbb BBB22 Festabbb FeedBBB',\n",
              " 'Cast Lord Lord Cast Redebbb BBB 22 BBB Party',\n",
              " 'Linn deserves respect transfobia play crime feel to embrace Adms Lu Lindaquebrada ADMS',\n",
              " 'Lu talk to getting married fact everyone beauty redebbb bbb22 reproduction',\n",
              " 'party today find going to be to fear redebbb bbb22',\n",
              " 'Good morning Jubinhas slept well Redebbb BBB22 Teamlucianestavan',\n",
              " 'RT OconVelaes all vanity self-esteem luciano try to show video presentation bbb22 truth hide a lot fr',\n",
              " 'Came there canal telegram netbbb bbb22 teamlucianestevan',\n",
              " 'BBB TVGlobo Redbbbbb BBB22 Time Luciano Estevam',\n",
              " 'People are nervous here Redbbb BBB22 Timelucianoestevan Aguiarthur',\n",
              " 'Break Head Tava Confused Finish BBB22 Redebbb',\n",
              " '_ Thiiih Aguiarthur twist',\n",
              " '191 19 Good Boy Aguiarthur BBB22 Redebbb',\n",
              " 'Luciano want to move comfortable mother uncle mainly remaining speak',\n",
              " 'amazing uncle hate up black person extremely easy to make person decide',\n",
              " 'take video context interpret wanting to ignore every other cit',\n",
              " 'Luciano talk back home where living go feel shit referring city d',\n",
              " 'Incredible pick up stretch isolate to post intention to create grupar play hate free c',\n",
              " 'Lu do rx today find power wait new lu next day huh redebbb bbb22',\n",
              " 'Timelucianoestevan Redebbb BBB22',\n",
              " 'People lu get leader finding to indicate wall was reflecting over',\n",
              " 'Today leader comes redbbbbb bbb22 timelucianoostevan reproduction globopay',\n",
              " 'then sleep well today lu caught leader netbbb bbb22',\n",
              " 'comedir loves dear to demonstrate certainty to go',\n",
              " 'worry we promise to try to joke play laughing know how to promise',\n",
              " 'literally open vidar opportunity grab all forcing dreaming person',\n",
              " 'Dreaming to live comfortable family talk yesterday there marry unfortunately pregue',\n",
              " 'wonderful power zoar maximum future consequence all know artist in',\n",
              " 'people see everything comment post here everything we see numerous to ask to be dare',\n",
              " 'Lu chat other brothers remaining experience naturalization racism childhood snow',\n",
              " 'Rt stxmaycw legend fall wall be able to leave',\n",
              " 'Rt gildovigor katiafonsek love it is dropping there to feel going to go to be able to let leave because v',\n",
              " 'Rt gildovigor brenolcribeiro aahh find jessi linn will be name edition',\n",
              " 'Good morning people seems to party to speak huh',\n",
              " 'first party bbb 22 beautiful intense full emotion adm now being able to go well',\n",
              " 'Rt jadepicon jessi fall up jade father transcended portar bbb21',\n",
              " 'motto n this party drink falling literally teamjessi',\n",
              " 'Rachel____BBB22 Read Once I Will Notify Justificant We Know Gravity',\n",
              " 'fail to reiterate power to avoid here person use comment transfóbic pa',\n",
              " 'Linn nobody in there playing teaching a subject may be learned here',\n",
              " 'Here Sorry Slot in Play We Excuse Falling Knot',\n",
              " 'Alô Receive 3 Car 16 Heart Knot BBB22 Times',\n",
              " 'Attention Barbara warning to want to sue someone marry but still try to decipher motivate cryptogra',\n",
              " 'Bá back to affirm the boy go vote natália swimming to vote somebody',\n",
              " 'vote someone unique person give motivate bullshit bbb22',\n",
              " 'go vote n it natália swimming against bbb22',\n",
              " 'Other social network',\n",
              " 'Like Facebook Gift Every Social Network So Lose Nada Left Bá',\n",
              " 'Bá there wake up naiara ask to get a little monster for eli power resting little bbb22',\n",
              " 'Bárbara find Quartar Lollipop speak only Tweet timeless',\n",
              " 'Bárbara Converto Jade Say Vote Naiara Brunna PA Asserves Go Vote Natalia',\n",
              " 'Barbara arrives Douglas Pedro Arthur Talking Tô papar marrying there compromised hahahaha',\n",
              " 'Natalia find falsehood call everyone friend gets head',\n",
              " 'Pobi Bárbara Traumatizar ask for Linn to be able to call friend',\n",
              " 'Air Edition BBB22',\n",
              " 'Any doubt modeli go enjoy fexxxtinha today alok take care because I love being dj ho',\n",
              " \"I'm trusting Feeling Father BBB22\",\n",
              " 'Rt bheckvideos today to prove drink resistance bbaheck',\n",
              " 'Barbara Saying Dreaming Ludmilla Bravo Bunda Brunna',\n",
              " 'Rt folkgirls to judge anyone naiara still because knowing person judging time to understand',\n",
              " \"Rodrigo find voting go play empty barbara be able to play empty I'm playing\",\n",
              " 'go stay smooth',\n",
              " 'Rodrigo vote go pipocar barbara iriar rodrigo all find naiar',\n",
              " 'Barbara speak find natália so sincere because day bomb talks true motivate having d',\n",
              " 'X-ray today 22 01',\n",
              " 'Basic Race BBB22',\n",
              " 'find incoherent you say protect ally put eli monster seems you want to compromise',\n",
              " 'Barbara punctuar rodrigo left incoherence have to put amigate monster speak over',\n",
              " 'Rodrigo speak over situation occur yesterday fourth leader commenting over Scooby say c',\n",
              " 'like to want to alangle rodrigo left overshe teammussi bbb222',\n",
              " 'watch video angel teammussi bbb22',\n",
              " 'Moment Rodrigo Watch Presenting Angel Teammussi BBB22',\n",
              " 'Rt bbb rodrigo end up watching gift angel reaction smile there bbb22 redebbb',\n",
              " 'Today walls anxious for BBB22 training',\n",
              " 'Ray X Rodrigo Today 23 01 speak leftover first party overlands Teammussi',\n",
              " 'pool teammussi bbb22',\n",
              " 'RT Print Mussi smile to start day today lunch Angel Team Mussi',\n",
              " 'Rodrigo n this moment pool teammussi bbb22',\n",
              " 'Rodrigo get jade because pa cé nerd barbara rodrigo get someone rodrigo nobody barbara',\n",
              " 'Rt evansfls rodrigar barbaro comedir party talk good two brothers rodrigar was involving because f',\n",
              " 'Rodrigo is so happy this party Teammussi BBB22',\n",
              " 'Take Take Vapo Vapo BBB22',\n",
              " 'Reproduction TV GloboPlay',\n",
              " 'Brazil fucking fucking here all happy',\n",
              " 'RT JADEPICON TRAVE BBB22',\n",
              " 'Scooby talk pro rodrigo go help overcome difficulty moving face teammussi bbb22',\n",
              " 'Rt eulucy3 impulsive insecure result trauma pain problem lift hand to sky understand',\n",
              " 'Rodrigo speak paranoid boy playing vidar pedroscooby say',\n",
              " 'kiss triple seal tongue someone send boy naty ahnnn maria kiss triple',\n",
              " 'Naty Quarta Lollipop Personal Chat BBB22 Team Naty',\n",
              " 'Naty now cooking Lucas speak over animal pet animals Team Naty BBB22',\n",
              " 'Triple wall training Today Angel Auto Immune Leader Indicates indicated Leader Contragolpeia Each',\n",
              " 'Natalia ask for forgiveness pro vyni cause provar american error committing to do',\n",
              " 'Subject moment natalia party fell dancing climb up thing person teamnaty bbb22',\n",
              " 'lunch angel voude99 accompany teamnaty bbb22',\n",
              " '_ ThIIIH',\n",
              " 'Naty Invite Lunch Official Angel Mussi Together Viny Of Dra_LAiscaldass Team Naty BBB22',\n",
              " 'moments TeamNaty BBB22',\n",
              " 'Jade Reproduction Network GloboPlay',\n",
              " 'Convert Subject Mary Reproduction Network GloboPlay',\n",
              " 'Continue to deliver everything Boy Reproduction Globo Globlay',\n",
              " 'Joked Twister Maria',\n",
              " 'Angelicaramosof BBB',\n",
              " 'Get Dancing Tiago Jessi Reproduction Network GloboPlay',\n",
              " 'come first seal reproduction network globe globopay',\n",
              " 'Maria deliver everything if at this party Reproduction Globo Globlay Network',\n",
              " 'try pistar dancing reproduction globe globoplay',\n",
              " 'Cat excites so much wheels almost solve Da Tamnaty BBB22 playback r',\n",
              " 'Farra know both BBB22 Teamnaty reproduction Network GloboPlay',\n",
              " 'Thread moments first party BBB22 Follow spin',\n",
              " 'Booom day braaasil looks ray x cat today still climate party teamnaty bbb22 Play',\n",
              " 'RT SAPAVEGANA Identify Falling Natália Even learning turns in bed',\n",
              " 'RT Jacycarvalho Identify Falling Natalia Knowing 22 Year Sacar All Run Doer',\n",
              " 'Dimitra Vulcana Brede bad Thank you Dimitra extreme importance for us',\n",
              " 'Rt dimitravulcana internet seeking to be crystal little defect pattern behavior network there people go see cam',\n",
              " 'Rt amandapinheiir vi people criticize natália have to drink too much Maria be dancing too much person to criticize',\n",
              " 'Tele 7 Babb 22 Team',\n",
              " 'Angelicaramosof bbb naty serve muitoooooo important conversion bathroom',\n",
              " 'Tele 6 Bubb 22 Team',\n",
              " 'Tele 5 Babb 22 Team',\n",
              " 'Part 4 times black woman wanting to be just tasty to want to be clever necessary',\n",
              " 'Rt lolaferreira natália boy 22 year living only since 12 marry 15 lived little time to speak',\n",
              " 'Juliette infos cat to do school',\n",
              " 'thanking profile post information sincere real thank you person to combine that',\n",
              " 'RT Levikaique Natália Follow Equivocar Everybody Love Share To Judge I Want To Force Integration',\n",
              " 'expectation person likes to identify naty some to form with us marry',\n",
              " 'Rt _ doblues maria console natália understanding being black woman is calling to know how to take care of another resp',\n",
              " 'Rt nailaahnv women black love caring pain natalia pain very woman black cry hurt pq people',\n",
              " 'Rt aanonnnyma here dorority black woman understand pain other black woman',\n",
              " 'RT NatalyNeri Maria Single Person Marries can see Truth Nathalia Deep Convert Two Ma',\n",
              " 'RT Nailahnv Wake Up Crying Testimonial Natalia remaining racism Impacting Vidar want to be humanize',\n",
              " 'Rt uaiiuri love see person judging naty want to kiss rodrigo deviate accuse harassment see nobody q',\n",
              " 'RT TVlizando Official_deDato Natália Super coherent Report pain black black woman forgiveness expressing err',\n",
              " 'Rt tvlizando natalia maria get jade to vent because really listen to another person',\n",
              " 'RT Centrality Maria speaks strong thing to mark me rejection Accustom S',\n",
              " 'Rt access_nataliad natália talk to jade time can be fucking be black opportunity equal feel',\n",
              " 'Rt centralreiaity woman black symbology yummy corpuda dancing fucking back person dance to',\n",
              " 'RT Kausiren Natalia Wrong Nothing Yet Yes Go Painting Molten Watching',\n",
              " 'Rt claratuitic woman black symbology tasty corpuda dance fucking back existing woman needed',\n",
              " 'Rt vaidesmaiar natália people need to occupy different place where bourgeoisie very representativeness force say s',\n",
              " 'Rt vaidesmaiar natália woman preaching recognized forcing person forget beyond this to be human',\n",
              " 'Naty never narrative try to build here behind D this crying layer m',\n",
              " 'RT brunotuita Maria needed behind all cry Exist much trauma to be able to look like victimism p',\n",
              " 'RT Muggle Tour Natália give name to this party launch now BBB22',\n",
              " 'Rt markosoliveira lunch angel while natália rodrigo lais explain reason to have to vote naiara vyni say s',\n",
              " 'BBB22 for this party then go',\n",
              " 'Inês Brazil Play Party Now Yes Present BBB InesbrasilTV',\n",
              " 'Arthur Scooby Linn Naiara External Area Chat Cancellation Internet BBB22',\n",
              " 'Personal Xepa raza lunch BBB22',\n",
              " 'boy praise senso collectivity organization arthur bbb22',\n",
              " 'Scooby say next party Arthur go to sleep ending BBB22 order',\n",
              " 'Arthur Naiara Tiago so chatting pool BBB22',\n",
              " 'Jade talk like all cabin to vote nobody room lollipop want to vote woman bbb22 teamjade',\n",
              " 'Jade converts natália yesterday way a lot of pain fuck l',\n",
              " 'Jade external area chat boy left over party yesterday said to love everything eaten',\n",
              " 'Last Mutirão Started Comment Emoji Each 10 Vote Encloses End Voting Vote',\n",
              " 'Mutirão lacking little closed',\n",
              " \"Let's force full now GSHOW is ending important every voting maximum power to go\",\n",
              " 'Communent Edition Continue Vote Ficanaiara Foraluciano',\n",
              " 'Mutirão lack little initiated comment emoji every 10 vote closes 23h vote eliminate',\n",
              " 'Mutirão focus gshow closed',\n",
              " 'Lacks Little Important People Parish Now Focus Mutirão Focus GSHOW Where Voto Valer',\n",
              " 'Mutirão focus gshow started comment emoji every 10 vote closes 22h vote eliminate',\n",
              " 'Mutirão Feat Gabiluthai closed',\n",
              " 'Mutirão Feat Gabiluthai Started Comment Emoji Each 10 Vote Closes 21h Vote Eliminate',\n",
              " 'Mutirão precise embrace closed',\n",
              " 'Still give time to vote GSHOW Participate Mutirão Leave TL Bora Vote Ficanaiara Foraluciano',\n",
              " 'RT Paoladeboche Mutirão Paoni Feat Maceta_monni Started Comment Emoji Each 5 Vote Closes 20h Vote Elimin',\n",
              " 'Mutirão need a hug started comment emoji every 10 vote closes 20h voting',\n",
              " 'Mutirão Feat Irodolffo closed',\n",
              " 'Mutirão Feat Irodolffo Started Comment Emoji Each 10 Vote Closes 19h votes Eliminate',\n",
              " 'Mutirão Hit closed',\n",
              " \"Pride of this team Mirirões Let's together lacking little Focus Total Gshow Ficanaiara Foraluciano Teamnaiara\",\n",
              " 'Mutirão hit started comment emoji every 10 vote ends 18h vote eliminate vote',\n",
              " 'Mutirão Xepa closed',\n",
              " 'Mutirão xepa initiated comment emoji every 10 vote ends 17h votes eliminate vote n',\n",
              " 'Mutirão Cute Cute Closed',\n",
              " 'Time to vote GShow Bora Focus Total there hein Time Ficanaiara Foraluciano Teamnaiaira',\n",
              " 'Mutirão Cute Cute Started Comment Emoji Each 10 Vote Closes 16h Voting Eliminate Vote',\n",
              " 'Mutirão Paraná world closed',\n",
              " 'Mutirão Paraná World Started Comment Emoji Each 10 Vote Closes 15h Vote El',\n",
              " 'Mutirão 3 daughters closed',\n",
              " 'Skirt tl go stay vote gshow guide polls go to vote',\n",
              " 'Mutirão 3 daughters started comment emoji every 10 vote ends 14h vote eliminate',\n",
              " 'Mutirão lunch ready closed',\n",
              " 'Mutirão lunch ready starting emoji every 10 vote closes 13h vote elim',\n",
              " 'Mutirão launch closed',\n",
              " 'Do not ask people post here all steam and continue ficanaiara foraluciano stop',\n",
              " 'Mutirão launch started Comment emoji every 10 vote closes 12h votes eliminate vote',\n",
              " 'Mutirão Pisi cheap closed',\n",
              " 'IMPORTANT FOCK TO BE 100 GSHOW THERE WHERE VOCY FOCUS FOCUS MUTIRÃO FICANAIARA FORALUCICO TEAMNAIARA',\n",
              " 'Mutirão pisi cheap started comment emoji every 10 vote closes 11h voting eliminate',\n",
              " 'Mutirão Coffee ready closed',\n",
              " 'Mutirão Cafe Ready Started Comment Emoji Each 10 Vote Closes 10h Vote Elimin',\n",
              " 'Mutirão real facts closed',\n",
              " 'Run Pro GShow Participate Mutarões Ficanaiara Foraluciano Teamnaiaira',\n",
              " 'Mutirão real facts initiated comment emoji every 10 vote closes 09h vote eliminate vot',\n",
              " 'Mutirão wakes up to vote closed',\n",
              " 'Mutirão Agreement to vote Started Comment Emoji Each 10 Vote Closes 08h Vote Eliminate',\n",
              " 'Mutirão Naiara Fátima closed',\n",
              " 'Bora Remember Valer GShow So Parir Vote There Beauty Time Ficanaiara Foraluciano',\n",
              " 'Mutirão Naiara Fátima Started Comment Emoji Each 10 Vote Closes 07h Vote Eliminate',\n",
              " 'Mutirão composer singer closed',\n",
              " 'Mutirão singer composer initiated comment emoji every 10 vote closes 06h vote eli',\n",
              " 'Mutirão focus gshow closed',\n",
              " \"I'm looking at everyone.\",\n",
              " 'Mutirão focus GSHOW started comment emoji every 10 vote closes 05h vote eliminate v',\n",
              " 'Mirirão dawn closed',\n",
              " 'Mutirão dawn started Comment emoji every 10 vote closes 04h voting to eliminate you',\n",
              " 'Mutirão dear singer closed',\n",
              " 'Remembering to vote every important thing to decide wall day tendinite huh ficanaiara foraluciano teamnaiara',\n",
              " 'Mutirão dear singer started comments emoji every 10 vote closes 03h vote pair',\n",
              " 'Mutirão woke up closed cooking',\n",
              " 'Mutirão woke up cooking Started Comment Emoji Each 10 vote closes 02h vote Eli',\n",
              " 'Mutirão cute beautiful huh closed',\n",
              " 'Vcs firm there people vote boraa focus Total gshow foraluciano ficanaiara',\n",
              " 'BEAUTIFUL MOTIRÃO HEIN CUTE Started Comment Emoji Each 10 Vote Closes 01h Vote',\n",
              " 'Mutirão forgets closed',\n",
              " 'People vote love god vote mmmmmm ficanaiara outside luciano team naiara',\n",
              " 'Mutirão forgets started comment emoji every 10 vote closes 00h vote eliminate vote n',\n",
              " 'Mutirão Mother Single Closed',\n",
              " 'Parir Vote Continue GShow Ficanaiara Foraluciano',\n",
              " 'Mutirão Mother Single Started Comment Emoji Each 10 vote closes 23h voting to eliminate you',\n",
              " 'Best love closed',\n",
              " 'Still gas Total GSHOW Né Time Come on All Show Quantum People Want Nai There Ficanaiara Foraluciano Teamnaiara',\n",
              " 'Mutirão best love started comment emoji every 10 vote closes 22h vote eliminate vot',\n",
              " 'Mutirão Feat Gabiluthai closed',\n",
              " 'Mutirão Feat Gabiluthai Started Comment Emoji Each 10 Vote Closes 21h Vote Eliminate',\n",
              " 'Mutirão Feijoada closed',\n",
              " 'Galera Everything Win Go Gshow Important Intensify Vote There Where Vote Are',\n",
              " 'Vcs too we will continue to force Mirks give chance to Nai focus GSHOW',\n",
              " 'Mutirão feijoada started comment emoji every 10 vote closes 20h vote eliminate vot',\n",
              " 'Mutirão Feat Irodolffo closed',\n",
              " 'Everybody focus on gshow né boora sticking mutirões fiction',\n",
              " 'Mutirão Feat Irodolffo Started Comment Emoji Each 10 Vote Closes 19h votes Eliminate',\n",
              " 'Mutirão arranging bed closed',\n",
              " 'Mutirão arranging bed started comments emoji every 10 vote closes 18h vote eliminate',\n",
              " 'Mutirão first monster closed',\n",
              " 'Focus Total GShow Time Focus Important Believe Polls Always GShow Ficanaiara Foraluciano Teamnaiaira',\n",
              " 'Mutirão First Monster Started Comment Emoji Each 10 Vote Closes 17h Vote Eliminate',\n",
              " 'Going to tell everyone hahaha stays naiara continue to vote the bbb22 foraluciano',\n",
              " 'Mutirão Team Naiara closed',\n",
              " 'all happy because production finds escapulary lose day party bbb22 ficanoia',\n",
              " 'Mutirão Team Naiara Started Comment Emoji Each 10 Vote Closes 16h Voting Eliminate VOT',\n",
              " 'Mutirão Espação closed',\n",
              " 'Focus gshow time mami need all bora stays naiara foraluciano team naiara',\n",
              " 'Mutirão Emitão started comment emoji every 10 vote ends 15h vote eliminate vote',\n",
              " 'Mutirão Chef Cuisine Closed',\n",
              " 'Thanks all so vote to go continue all decisive vote ficanaiara foraluciano teamnaiara',\n",
              " 'Mutirão Chef Cuisine Started Comment Emoji Each 10 Vote Closes 14h Vote Eliminate',\n",
              " 'Mutirão contrast closed',\n",
              " 'Boraaaa team certainty everything go valer foco foco total gshow ficanaiara foraluciano teamnaiara',\n",
              " 'Mutirão Contrast Started Comment Emoji Each 10 vote Closes 13h vote Eliminate vote',\n",
              " 'Best love closed',\n",
              " 'adm here give a good laugh kkkkkkkkk bbb22',\n",
              " 'Too much disturb Pedro dear kkkkkkkkkkkkkkkk',\n",
              " 'Dear Cute Scooby Want To Know Beautiful Level',\n",
              " 'Follow Spree Give RT Fav N This Tweet Follow All Derem RT Fav Comment TeamScooby Follow',\n",
              " 'Follow Spree Tardar for',\n",
              " \"scene here mother brother father so here I'll put two\",\n",
              " 'Vidar Irar Manir to go tan dreamed tomorrow go out',\n",
              " 'Motivational speech Linn is kind of sad Scooby try to cheer',\n",
              " 'Dear train serve swimming take 7 Vote expensive to remain everything heart',\n",
              " 'Scooby Podium Count Tiago Abravanel Paulo André Right Honorable Mention Pro Douglas',\n",
              " 'podium tiagoabravanel still roll speaking foder pro scooby',\n",
              " 'A lot of people fame granir depression problem put front anything happiness principle',\n",
              " 'think going to get rid of me so easy week go have put up bbb22',\n",
              " \"Let's escape little EIN BBB22\",\n",
              " 'moment Scooby speak silva dg people love friendship bbb22',\n",
              " 'Almost 20,000 Tweets Wonderful Tag Follow Brunnagoncles Ig',\n",
              " 'Follow Trick Spree Give RT fav n this tweet comment Follow Brunnagoncles IG Follow all Like',\n",
              " 'RT all brunna and eighth challenge ten country discouraging ten singer singer ten actors actresses use tag follow brunnagoncalves',\n",
              " 'Follow Brunnagoncles Ig',\n",
              " 'RT infosbrunna seventh challenge ten applications ten colors ten national singers ten states use tag follow brunnagonclescalves n',\n",
              " 'Sixth challenge Comment ten name country Comment ten musician Comment ten movie Comment ten series',\n",
              " 'RT qg brunna challenge black and white banana x mace ios x android cold x heat beach x cell phone x computer night',\n",
              " 'RT Brunnatam Room Challenge Comment Ten Name Himself Comment Ten Objects Comment Ten Celebrities Comment Ten Blush C',\n",
              " 'Rt portalbrunna brunna speak maria form demonstrate love caring person want well bbbrunna htt',\n",
              " 'Brunna telling you to ask Naiara Luciano to leave BBBrunna BBB22',\n",
              " 'Come now make off official grupar Brunna Telegram we tell',\n",
              " 'RT Hugogloss Torture Psychological Entertainment Save Brunnagoncalves Joyce Kkkkk Mear Both Ciranda Singing BBB22',\n",
              " \"It's so beautiful, bbbrunna.\",\n",
              " 'Bru 6 Dear Dear Happy 14 BBBrunna Heart',\n",
              " 'Smell perfume Ludmilla second Naiara Azevedo smelling Brunna BBB22',\n",
              " 'RT Infosbrunna PPV In Love Bru Show Naiara Lud Shirt Perfume BBB22',\n",
              " 'Securing obvious place to go Brunna always with me from beginning Eumaria BBB22',\n",
              " 'Mary put brunna 2nd podium place where mabru bbb22 bbbrunna',\n",
              " 'person always with me since arrive Eumaria bbb22',\n",
              " \"Follow Spree 100k Let's Join Give RT Fav N This Tweet Follow All Derem RT Fav\",\n",
              " '100 k shot blinking family is growing every day forcing to reach gratitude q',\n",
              " 'RT Debcoments compiled PA Sing Iampauloandre',\n",
              " 'RT Madwsly iampauloandre to find to find paulo being amazing light sing sincere calmar',\n",
              " 'easy thing brazil planning family things speak understand',\n",
              " 'Knows how to feel bad for the time natália speak and create alliance to speak swimming',\n",
              " 'Maria chat remedy vote natália say now tranquil vote teammaria bbb22',\n",
              " 'Queriday Today Mary Receive 5 14 Teammaria BBB22',\n",
              " \"people be able to weaken this form I'm talking to try to want to please\",\n",
              " 'Rodrigo talk Maria left is to exclude Maria say find there',\n",
              " 'want to be afraid to want to be afraid I want to discredit Brazil Lina',\n",
              " 'Love Calabresadani.',\n",
              " 'put ines place naiara is happening bbb22',\n",
              " 'relationship happen to marry after training wall today jade affirm to come destroy dreaming',\n",
              " 'Believe Jade is making people defend Rico BBB22',\n",
              " 'Mano defending permanence naiara bbb can be joking send other travest',\n",
              " 'ELIEZER final count making drama differentiate almost symbolic BBB22',\n",
              " 'Luciano Unsecurgo Rodrigo Paranoico Convert famous successful is to be horrible for BBB22',\n",
              " 'Finding Grace Participant Skip Charisma Happiness BBB22 arrives',\n",
              " 'I Eliminate Popocar Time Transform BBB Home Artists',\n",
              " 'prefer to hate BBB22',\n",
              " 'Comedir BBB Make Fucking Manir Antipatic Kkkkk',\n",
              " 'People have bbb poor poor happy too kkkk love poor effort poor valorizaaaaa poor shine k',\n",
              " 'heterotops bbb all come in ask apologize be straight is happy tiago iorc',\n",
              " 'brother bbb 22h45 boninho you hate proletariat',\n",
              " 'God get some participant BBB22',\n",
              " 'Cadê troops closed Douglas bora up knockout Cautada Use closed comdouglas',\n",
              " 'Tiagoabravanel tmj',\n",
              " 'appear section completing balcony confer there is a bit of a papar',\n",
              " 'Rt liliibrito another thing because DG explain play for everyone to pay player',\n",
              " 'Hey Jadepicon to pass there to DG tip cap',\n",
              " 'Douglas against manipulating play there expensive to speak ah because help be leader then',\n",
              " 'Douglas left Rodrigo Scooby spade because playing because put poison eliezer jessylane',\n",
              " 'X-ray today enemy end TV globe BBB Teamdouglas BBB22 Redebbb',\n",
              " 'GABRIELLA_MENDI 4 Indicate Prove Beat Back Saved 1',\n",
              " 'Remembering Dynamic Week Rodrigo Go Immunize Tiago Indicating Scooby Indicate person r',\n",
              " 'Liked to Prove Rodrigo Angel Jade Brunna Pa Monster Teamdouglas',\n",
              " 'Douglas End participation Provate angel Amstelbrasil 5 point Download Go directly',\n",
              " 'Douglas Make 1 Ponta n this second Rotate Teamdouglas',\n",
              " 'Douglas Pontoar First Rotate Prove Angel Teamdouglas',\n",
              " 'Finish Last Placement Go Direct Monster Teamdouglas',\n",
              " 'Cezar give fear it is difficult to hear Lina know how to motivate why person deal well',\n",
              " 'Ludmilla Cute Convida Mami Show Lud Go Love',\n",
              " 'very cute next to jessylane',\n",
              " 'Lina represents two brazil loved video gamar live create blow childhood adolescence',\n",
              " 'happy to perceive repercussion over day visibility transmission',\n",
              " 'Friend Linn here',\n",
              " 'Linn ta always making lemon lemonade né play still do jar',\n",
              " 'Playful Mom Play Little Vyniof Join Monster Fliperama',\n",
              " 'Congratulations Official Mussi New Angel Marry Jade Picon New Monster Go Give everything Certain friend Teamlinn',\n",
              " 'Lina shutting down participation prove angel 3 point free monster Teamlinn',\n",
              " 'MAMI FEZ PONTOOOO TeamLinn',\n",
              " 'Important less punctuate proof angel go straight monster Teamlinn',\n",
              " 'Lina Mark Point First Templic Teamlinn',\n",
              " 'go provodromo make taste twist withdraw mami teamlinn',\n",
              " 'Tadeu appear to prove angel go start going to be amstelbrasil comes there angel mami teamlinn',\n",
              " 'Day 29 January Day National Visibility Transability Word Linn Broken Teamlinn',\n",
              " 'Sitting Choca Know Everything Roll So Wait To Record Ray X Teamlinn',\n",
              " 'Linn to ask everyone to make party because they remember Teamlinn',\n",
              " 'Good morning Linndanas Teamlinn',\n",
              " 'Linn sleep enjoy enough party I need to rest Bora adms go lie on ok',\n",
              " 'Camera Quartar Camera View Lina',\n",
              " 'Lina go to sleep naiara fourth help to put her sleep forcing naiarazevedo',\n",
              " 'Adms stay super sensitive Mami crying all transvestite know how to be feeling',\n",
              " 'Leave here Message expensive lina go show the exit winner d this play',\n",
              " 'brunnagoncalves oblige dear bruna adms thrill',\n",
              " 'Lina start crying fear judgment here is quiet lina',\n",
              " 'Lina crying talk naiara',\n",
              " 'honey good bee always back nd in this marriage to go back to abelhar',\n",
              " 'People photo second kiss outsource no pear will be last lost here',\n",
              " 'Nobody lina',\n",
              " 'Tree Sounding Party Here',\n",
              " 'Can play water Linn is passing to run promise pocah',\n",
              " 'Finding hesitate to show kiss bald edition',\n",
              " 'Silva beach leader acerola god pagoda beer friendship closed douglas',\n",
              " 'LindaBrada tdetravesti icon',\n",
              " '100 Close Aguiarthur',\n",
              " 'ChicoBarney you want to play pearl entertainment',\n",
              " 'Talk dear to play essential clean',\n",
              " 'there people ask angel immunize immunize indicate',\n",
              " 'Lindaquebrada rumor Bolar is going to laugh January now adm kkkk forcing Mami',\n",
              " \"Figure today's leading album\",\n",
              " 'taste adm needing to prove today',\n",
              " 'shuffle',\n",
              " 'Look Running x Bear There Little Suit Powder Party Never Né Video Reproduction Globlay',\n",
              " 'see bear like this so time to give bye sleep well personal caring xero',\n",
              " 'Tiago give bye kiss to go take a shower sleep, can not love man',\n",
              " 'Rt simple linnade to be travestir brazil allow kissing to relate someone to say',\n",
              " 'Rt realitysocial tiago for linn world full people pointing finger people get worrying go pointing finger p',\n",
              " 'Bear Amo Click Photo Playback Globlay BBB22 Teamabrava',\n",
              " 'RT BBB Beijoooo Maria Eliezer Kissing Party Redbbb BBB22',\n",
              " 'RT bbb RedeBBB BBB22',\n",
              " 'RT Lindaquebrada nobody lina',\n",
              " 'Good guys 0305 Need to say Truth to popsicle Ice ice Chu Chu',\n",
              " 'God has already taken all ice cream now only sleeve nobody likes aaaaaa video reproduction globopl',\n",
              " 'shaping bald kkkkkkkkkkkkkk people video reproduction globopay',\n",
              " 'Finish pandemic See friend Losing bv pandemic video reproduction globoplay',\n",
              " 'Goes linaaaaaaaaaaaaaa video reproduction globoplay lindaquebrada bbb22',\n",
              " 'Lindaquebrada adm literally Tiago see perfect kiss',\n",
              " 'Simply adm the same tiago kkkkkkkk to here video reproduction globopay',\n",
              " 'Where Meme Maria Play Play',\n",
              " 'flame',\n",
              " 'End of this sequence legend serve all gif reproduction globopay',\n",
              " 'Buddy Ass Brazes Gif Globlay Reproduction',\n",
              " 'Singing Cult GIF Reproduction Globlay',\n",
              " 'Tiago Performance beating Cabelar Gif Globlay Playback',\n",
              " 'step here adm is laughing now gif reproduction globopay',\n",
              " 'Kick Million Lindaquebrada Gif Globlay Playback',\n",
              " 'Swing Raba Gif Globlay Playback',\n",
              " 'Korea Vynioph Gif Globlay Playback',\n",
              " 'Son Cadê Smile for Mommy Take Photo Gif Reproduction Globlay BBB22 Teamabrava',\n",
              " 'bbb serve trollagem camera up tv poor pose hahahahahaha video playback',\n",
              " 'Rustlegemoficial served both excitement tudooooo we want',\n",
              " 'Ballerinas warn brunnagoncalves gif reproduction globoplay bbb22',\n",
              " '1 morning adm is linnaquebrada ps makes camouflage photo reproduction gl',\n",
              " 'adm look so much to cry James realize third time see hahahahahaha',\n",
              " 'Look to make bear rust official mistreat longing heart pagoda obliges',\n",
              " 'We find crying owwwwwww',\n",
              " 'find hahahahahaha bbb22',\n",
              " 'Show rust good value training, people dripping so much enjoy bbb22',\n",
              " 'Explain BBB22',\n",
              " 'Avisa Tiago Abravanel New leader BBB loving face',\n",
              " 'bad video jessi natália talk tennis 150 real expensive for nat still say',\n",
              " 'People dream See Paulo André Run to meet Big Phone BBB22',\n",
              " 'find jade cool I feel exaggerate personal top everything do bbb22',\n",
              " 'Lina people discuss love to get out of this untouchable terrain to realize why some co',\n",
              " 'Millionaire Fiscal Creativity Poor Holding Boy Super Grace Have Insist Explo',\n",
              " 'Jade people are literally to be a matter to get married BBB22',\n",
              " 'BBB22 TeamMaria',\n",
              " 'SBT Online Tiago Abravamanel BBB can leave fantastic boy home now waiting Cauldron Fe',\n",
              " 'EVERYTHING Sprinting Jade Picon Bichar Singing Redebbb BBB22',\n",
              " 'Jade do some crap n this BBB',\n",
              " 'Kissing Kissing Kissing Pras Travestir BBB22 Redebbb',\n",
              " 'Thread Even Madrugar BBB22 BBB Love',\n",
              " 'Jade Minor Score Provate Direct Monster Leave VIP and Xepa',\n",
              " 'Thank you Boninho wonderful image Arthur run giant bread BBB22',\n",
              " 'Time Smoker Vencer Athlete Existing Thing Only BBB Explain',\n",
              " 'Go Forcing Crush BBB22 Redebbb',\n",
              " 'Jade and discover for VIP Xepa BBB 22',\n",
              " 'Silvio Santos BBB22 Real',\n",
              " 'know how to need double lynn jade having double wonderful energy is this two together bbb22',\n",
              " 'Leader Super Well Linna Jade Date NameEeeeeeeee BBB22',\n",
              " 'People Naiara Kids Bread Tô Mal Kkkk BBB22',\n",
              " 'Best moment proof BBB22',\n",
              " 'Current prayer BBB22',\n",
              " 'Tadeu send too much narration prove missing emotion BBB BBB22',\n",
              " 'Tadeu narrating competition horse kkkk bbb22',\n",
              " 'Naiara needs to hurt everything Linn Naiara 40 BBB22',\n",
              " 'NAIARA required N this prove BBB22',\n",
              " 'First Double History BBB22',\n",
              " 'Gente Kkkkkkkkk BBB22',\n",
              " 'People Focus Prove BBB22',\n",
              " 'Thinking 9 vote go be made living sofa front all thank you little boni bbb22',\n",
              " 'Maira is coming run arthur bbb22',\n",
              " 'Naiara want to help unfortunately impractive coexistence corporate LINN Broken',\n",
              " 'Douglas take Rodrigo take Bárbara BBB22',\n",
              " 'School hide to teach call to talk Front Room BBB22',\n",
              " 'Dynamic morning couple giving bbb22 stick',\n",
              " 'Tomato Organic Fair Here Near Marriage Chemical Double Slovenia Lucas Symphus BBB22',\n",
              " 'God Sunday go have vote open forcing cake people want chaos bbb22',\n",
              " 'BBB22',\n",
              " 'Week Pass Winning Proving Leader DG Other Being Veto BBB22',\n",
              " 'Jade Linn Leadership Wanting BBB22',\n",
              " 'Jade will participate leader proof begin prayer BBB22',\n",
              " 'Holding Edit Couple BBB22',\n",
              " 'Come there Lider Arthur Aguiar BBB22',\n",
              " 'Scroll here BBB22',\n",
              " 'person stay ah say something wrong person to worry about me wanting',\n",
              " 'Jade Brás Buy Vyni Buy Brás BBB22',\n",
              " 'happiness arthur count eating party bbb22',\n",
              " 'History cool Lina explain reproduction redar globe BBB',\n",
              " 'Meteu Brasil is see Casimiro Redebbb BBB22',\n",
              " 'Pedro Scooby Show Happiness Bothering BBB22',\n",
              " 'man bread want war nobody bbb22',\n",
              " 'Attention nobody better mamma to remember brazil is see all roll bbb22 cam',\n",
              " 'For me word bubing serve person try to justify error fault conscious information',\n",
              " 'Here Linn Perspicaz BBB22',\n",
              " 'Prints now find deserving to be sharing BBB22 Teammaria',\n",
              " 'Word Quebrada Linn Redbbb BBB22',\n",
              " 'behavior nayara bbb is to make want to hear another side musician musician 50 real bbb22',\n",
              " 'jade picon do something to make bathing BBB',\n",
              " 'BBB telecurso',\n",
              " 'Printing Rodrigo Mal Person Login BBB Fajuto Pretend Equivocar BBB22',\n",
              " 'Naiara try to escape be voting n this week BBB22',\n",
              " 'planar bbb destroy class awareness cheer for rich millionaire during editing change r',\n",
              " 'Low game cunning pass 22 edition 36 expensive year do',\n",
              " 'Rodrigo Imbecile BBB22',\n",
              " 'love to see boy tidy bbb because getting fascinating top out dress look qua',\n",
              " 'Look at the fourth leader new cheeeio spacing bbb22 expect new leader to get c',\n",
              " 'Year come enrolling BBB to learn remaining mathematical physical chemist Never learn Law',\n",
              " 'Eliezer wheel asking to make mistakes talking face woman juice transfobia person graduate',\n",
              " 'Bárbara I will vote n it natália rodrigo people combine barbara combine nobody',\n",
              " 'Images official fourth leader BBB22 remembering inaugurate today',\n",
              " 'Holding Milesimo Edition Marmanjo Form to Go to Reality Show Learn Basic',\n",
              " 'Maria eliezer expensive leftomacho bbb22',\n",
              " 'converter naiara azevedo singer say fear be frustrating person here arthur vo',\n",
              " 'Simply listen to leave your own Bocar think type 1 Second to talk BBB22',\n",
              " 'all beautiful count to win car go give father bbb22 timeeslô',\n",
              " 'Saiuu new episode BBBtáon Kerlinecardoso ex sister give tip be good first',\n",
              " 'Naiara chat Arthur garden area accompany BBB22',\n",
              " 'Laís Barbara Combine Slop Sit together today Live prisons equal BBB22',\n",
              " 'Cooking Silva DG Canto English Scooby BBB22',\n",
              " 'Topic Scooby Random Personal BBB22',\n",
              " 'two power to have program variety recipes movie musician travel show next ass',\n",
              " 'All beautiful red teamlais bbb22',\n",
              " 'People group historically marginalize power to use time to teach person overwrite intolerance p',\n",
              " 'Because someone speaks face woman intention to be scrotal eliezer speak something like so pa']"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "processar_bases = processar_bases_de_treino()\n",
        "jogo = [x for x,_ in list(filter(lambda x: x[1] ==1, processar_bases.train_fala_do_jogo()))]\n",
        "not_jogo = [x for x,_ in list(filter(lambda x: x[1] ==0, processar_bases.train_fala_do_jogo()))]\n",
        "jogo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = PositiveNaiveBayesClassifier(positive_set=jogo, unlabeled_set=not_jogo)\n",
        "classifier.classify(\"Why is that participant still on the house?\"), classifier.classify(\"Answer this tweet with #TeamLucas and lets put the tag up!\")"
      ],
      "metadata": {
        "id": "s8rH-BmFo9Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsWDBn_KAHkV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20,20))\n",
        "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
        "               collocations=False).generate(\" \".join(data_neg))\n",
        "plt.imshow(wc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sheet_id = '15_1pGfHuKuLsqbhXLMQnCP5l7LCTyxYktG2gi0ezt44'\n",
        "sheet_name = 'consolidado_Publico'\n",
        "url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
        "DfConsolidadoPublico = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "4MV9g9WQhWJc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Sentiment_analysis.ipynb",
      "provenance": [],
      "mount_file_id": "1SR-3i9Wrab9mlb7RdoAnO_79DAVvlJaf",
      "authorship_tag": "ABX9TyNi53MVxHSvrAQ3eMkXAXfk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}