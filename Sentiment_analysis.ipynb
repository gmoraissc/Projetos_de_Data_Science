{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1SR-3i9Wrab9mlb7RdoAnO_79DAVvlJaf",
      "authorship_tag": "ABX9TyOvd8PBQ+RAONTrFIYGxBB8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmoraissc/Projetos_de_Data_Science/blob/main/Sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pip freeze --local > /content/gdrive/My\\ Drive/colab_installed.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1A1VTIckbwd",
        "outputId": "36e5fb8a-af78-4d25-c251-d916d75384e4"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSTALLATIONS"
      ],
      "metadata": {
        "id": "-R-W39LWMbod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji, !pip install xlsxwriter, ! python -m spacy download pt_core_news_sm, !pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DV_e0ztqQxk",
        "outputId": "ef861696-5524-47e8-fd5a-07128b92d145"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.6.3.tar.gz (174 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 19.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 21.4 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 61 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 71 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 92 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 102 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 112 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 122 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 133 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 143 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 153 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 163 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174 kB 8.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.3-py3-none-any.whl size=170298 sha256=258c9f07b7af494de2db8b5a69812cff964a9016675aecd781a9c9dabf142754\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/8b/d7/ad579fbef83c287215c0caab60fb0ae0f30c4d7ce5f580eade\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTS"
      ],
      "metadata": {
        "id": "mdlamNraMahj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "A2De13A6eBh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f5f8827-9065-456b-cc13-eaede45b0bc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tweepy, nltk, json, os, csv, xlsxwriter, pytz, re, heapq, string, spacy\n",
        "from pandas import ExcelWriter as ExcelWriter\n",
        "from pathlib import Path\n",
        "from datetime import datetime, date, timedelta\n",
        "from csv import writer\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class auth_twitter(object):\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def parametros_de_acesso(self, arquivo):\n",
        "    \n",
        "    self.__arquivo = arquivo\n",
        "\n",
        "    with open (arquivo, 'r') as arquivo:\n",
        "      texto = arquivo.readlines()[0].split(',')\n",
        "    \n",
        "    access_token = texto[0].split('=')[1]\n",
        "    access_token_secret = texto[1].split('=')[1]\n",
        "    bearer_token = texto[2].split('=')[1]\n",
        "    consumer_key = texto[3].split('=')[1]\n",
        "    consumer_secret = texto[4].split('=')[1]\n",
        "\n",
        "    arquivo.close()\n",
        "\n",
        "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_token, access_token_secret)\n",
        "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "    \n",
        "    return api"
      ],
      "metadata": {
        "id": "gmuNrfoS7pC-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA EXCTRACTION"
      ],
      "metadata": {
        "id": "lzWKDB6gMX6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class pesquisar_tweets(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.resultados_da_pesquisa = []\n",
        "    self.historico_info_participantes = []\n",
        "    self.informacoes_hist_participante = []\n",
        "\n",
        "  def por_termo(self, query: tuple, result_type='recent', count=int(100)):\n",
        "    \n",
        "    \"\"\"Este método faz a consulta de tweets a partir de termos. \n",
        "    Query: a query deve ser uma tupla (str) utilizando-se de operadores lógicos\n",
        "    tais como OR, AND. Ex. BBB AND #REDEBBB. \n",
        "    Result_type: pode ser ou popular ou recent. O primeiro buscará os tweets\n",
        "    que corresponderem a query publicados recentemente. O segundo será aqueles\n",
        "    tweets mais populares. O padrão é recent.\n",
        "    Count: recebe um valor inteiro para limitar o número de tweets coletados por consulta. O padrão é 100.\n",
        "    Retorna os resultados da pesquisa (lista) e o tipo de resultado (popular ou recent).\n",
        "    \"\"\"\n",
        "\n",
        "    global api\n",
        "\n",
        "    self.__query = query\n",
        "    self.__result_type = result_type\n",
        "    self.__count = count\n",
        "\n",
        "    for status in tweepy.Cursor(api.search,\n",
        "                                q=self.__query,\n",
        "                                result_type=self.__result_type,\n",
        "                                count=self.__count).items():\n",
        "                                \n",
        "                                self.resultados_da_pesquisa.append(status)\n",
        "\n",
        "    return self.resultados_da_pesquisa, self.__result_type\n",
        " \n",
        "  def por_usuario(self, participante: str, result_type='recent', count=int(100)):\n",
        "    \n",
        "    \"\"\"Este método faz a consulta de tweets a partir do nome de um usuário. \n",
        "    Participante: recebe o screen_name do usuário do twitter. \n",
        "    Result_type: pode ser ou popular ou recent. O primeiro buscará os tweets\n",
        "    que corresponderem a query publicados recentemente. O segundo será aqueles\n",
        "    tweets mais populares. O padrão é recent.\n",
        "    Count: recebe um valor inteiro para limitar o número de tweets coletados por consulta. O padrão é 100.\n",
        "    Retorna os resultados da pesquisa em uma lista, o tipo de resultado (popular ou recent) e o tipo de\n",
        "    participante (camarote, pipoca, público ou perfil oficial).\n",
        "    \"\"\"\n",
        "\n",
        "    global api, participantes_camarote, participantes_pipoca # ajustar para quando as informações de pesquisa estiverem na classe\n",
        "\n",
        "    self.__participante = participante\n",
        "    self.__result_type = result_type\n",
        "    self.__count = count\n",
        "    \n",
        "    tipo_participante = informacoes_de_participantes().checar_tipo_participante(participante)\n",
        "\n",
        "    for status in tweepy.Cursor(api.user_timeline,\n",
        "                                screen_name=self.__participante,\n",
        "                                result_type=self.__result_type,\n",
        "                                exclude_replies=False).items(self.__count):\n",
        "                                \n",
        "                                self.resultados_da_pesquisa.append(status)\n",
        "\n",
        "    return self.resultados_da_pesquisa, self.__result_type, tipo_participante\n",
        "\n",
        "  def por_usuarios(self, participantes: list, result_type='recent', count=int(100)):\n",
        "    \n",
        "    \"\"\"Este método faz a consulta de tweets a partir do nome de pelo menos um usuário. \n",
        "    Participante: recebe o screen_name dos usuários do twitter que deseja ser consultado. \n",
        "    Result_type: pode ser ou popular ou recent. O primeiro buscará os tweets\n",
        "    que corresponderem a query publicados recentemente. O segundo será aqueles\n",
        "    tweets mais populares. O padrão é recent.\n",
        "    Count: recebe um valor inteiro para limitar o número de tweets coletados por consulta. O padrão é 100.\n",
        "    Retorna uma lista com os resultados da pesquisa, o tipo de resultado (popular ou recent) e o tipo de pesquisa (única ou múltipla).\n",
        "    \"\"\"\n",
        "\n",
        "    global api\n",
        "\n",
        "    self.__participantes = participantes\n",
        "    self.__result_type = result_type\n",
        "    self.__count = count\n",
        "    \n",
        "    for participante in self.__participantes:\n",
        "\n",
        "      for status in tweepy.Cursor(api.user_timeline,\n",
        "                                  screen_name=participante,\n",
        "                                  result_type=self.__result_type,\n",
        "                                  exclude_replies=False).items(self.__count):\n",
        "                                  \n",
        "                                  self.resultados_da_pesquisa.append(status)\n",
        "\n",
        "    return self.resultados_da_pesquisa, self.__result_type, 'Todos'\n",
        "\n",
        "  def infos_do_usuario(self, usuario: str):\n",
        "  \n",
        "    \"\"\"Este método faz a consulta de informações do usuário (número de seguidores (int), número de perfis que segue (int),\n",
        "    número de  (int), número de tweets favoritados (int), número de tweets totais (int), verificado (booleano) e a data\n",
        "    é uma informação gerada pelo método \"corrigir_timezone\" cujo padrão é a hora, a fim de organizar os arquivos nas devidas pastas). \n",
        "    Usuario: recebe o screen_name do usuário do twitter que deseja ser consultado.\n",
        "    Retorna uma lista.\n",
        "    \"\"\"\n",
        "\n",
        "    global api\n",
        "\n",
        "    self.__user_results = api.get_user(usuario)\n",
        "\n",
        "    extracao = corrigir_timezone.__func__(tipo='hora')\n",
        "    \n",
        "    participante = self.__user_results.name\n",
        "    twitter = self.__user_results.screen_name\n",
        "    seguidores = self.__user_results.followers_count\n",
        "    seguindo = self.__user_results.friends_count\n",
        "    numero_de_listados = self.__user_results.listed_count\n",
        "    n_tweets_favoritados = self.__user_results.favourites_count\n",
        "    num_de_tweets_totais = self.__user_results.statuses_count\n",
        "    verificado = self.__user_results.verified\n",
        "\n",
        "    self.informacoes_hist_participante.append({'participante': str(participante),\n",
        "                                          'twitter': str(twitter),\n",
        "                                          'n_seguidores': int(seguidores),\n",
        "                                          'n_seguindo': int(seguindo),\n",
        "                                          'n_de_listados': int(numero_de_listados),\n",
        "                                          'n_tweets_favoritados': int(n_tweets_favoritados),\n",
        "                                          'num_de_tweets_totais': int(num_de_tweets_totais),\n",
        "                                          'verificado': bool(verificado),\n",
        "                                          'data': extracao})\n",
        "    \n",
        "    return self.informacoes_hist_participante\n",
        "\n",
        "  def historico_participante(self, participantes: list):\n",
        "\n",
        "    \"\"\"Este métood visa, a partir da busa por usuario, iterar em uma lista de usuários e fazer uma varredura por todos os participantes\n",
        "    buscando as informações históricas (citadas em infos_do_usuario).\n",
        "    Participantes: recebe uma lista de screen_names para iteração.\n",
        "    Retorna uma lista com todas as informações coletadas.\n",
        "    \"\"\"\n",
        "\n",
        "    for participante in participantes:\n",
        "      _ = pesquisar_tweets().infos_do_usuario(participante)\n",
        "      self.historico_info_participantes.extend(_)\n",
        "    \n",
        "    return self.historico_info_participantes, 'historico', 'todos'"
      ],
      "metadata": {
        "id": "RL_XrvFE1GBp"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA ORGANIZATION/FILE HANDLING"
      ],
      "metadata": {
        "id": "oi2o1bokMVPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class organizar_coletas_de_tweets(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.dicionario = {}\n",
        "    self.lista = []\n",
        "\n",
        "  def criar_dicionario(self, tweets_status, tipo):\n",
        "      \n",
        "    d1 = corrigir_timezone.__func__(tipo='data')\n",
        "\n",
        "    self.__tweets_status = tweets_status\n",
        "    self.__tipo = tipo\n",
        "\n",
        "    for each_json_tweet in tweets_status:\n",
        "      _ = json.dumps(each_json_tweet._json)\n",
        "      \n",
        "      if d1 not in self.dicionario.keys():\n",
        "        tweet = {d1: [json.loads(_)]}\n",
        "        self.dicionario.update(tweet)\n",
        "        \n",
        "      else:\n",
        "        self.dicionario[d1].append(json.loads(_))\n",
        "      \n",
        "    return self.dicionario, self.__tipo\n",
        "\n",
        "  def dataframe_tweets(self, dicionario, tipo_de_pesquisa, multipla=False):\n",
        "    \n",
        "    d1 = corrigir_timezone.__func__(tipo='data')\n",
        "    \n",
        "    self.__dicionario = dicionario\n",
        "    self.__tipo_de_pesquisa = tipo_de_pesquisa\n",
        "\n",
        "    for i in range(0, len(dicionario[d1])):\n",
        "\n",
        "          tweet_id = self.__dicionario[d1][i]['id']\n",
        "          text = self.__dicionario[d1][i]['text']\n",
        "          favorite_count = self.__dicionario[d1][i]['favorite_count']\n",
        "          retweet_count = self.__dicionario[d1][i]['retweet_count']\n",
        "          created_at = self.__dicionario[d1][i]['created_at']\n",
        "          hashtags = self.__dicionario[d1][i]['entities']['hashtags']\n",
        "          user_mentions = self.__dicionario[d1][i]['entities']['user_mentions']\n",
        "          name = self.__dicionario[d1][i]['user']['name']\n",
        "          screen_name = self.__dicionario[d1][i]['user']['screen_name']\n",
        "\n",
        "          self.lista.append({'tweet_id': str(tweet_id),\n",
        "                        'text': str(text),\n",
        "                        'favorite_count': int(favorite_count),\n",
        "                        'retweet_count': int(retweet_count),\n",
        "                        'created_at': created_at,\n",
        "                        'user_mentions': user_mentions,\n",
        "                        'name': name,\n",
        "                        'screen_name': screen_name})\n",
        "          \n",
        "    self.__tweet_json_ = pd.DataFrame(self.lista, columns = \n",
        "                              ['tweet_id', 'text', \n",
        "                                'favorite_count', 'retweet_count', \n",
        "                                'created_at', 'hashtags', \n",
        "                                'user_mentions', 'name',\n",
        "                                'screen_name'])\n",
        "    \n",
        "    if multipla == False:\n",
        "      \n",
        "      self.__tipo_participante = informacoes_de_participantes().checar_tipo_participante(screen_name)\n",
        "    \n",
        "    else:\n",
        "      \n",
        "      self.__tipo_participante = 'Todos'\n",
        "\n",
        "    return self.__tweet_json_, self.__tipo_participante, self.__tipo_de_pesquisa\n",
        "\n",
        "  def dataframe_dos_participantes(self, informacoes_hist_participante):\n",
        "\n",
        "      self.__informacoes_hist_participante = informacoes_hist_participante\n",
        "\n",
        "      self.__dados_participantes = pd.DataFrame(self.__informacoes_hist_participante, columns = \n",
        "                                      ['participante', 'twitter', 'n_seguidores', \n",
        "                                      'n_seguindo','n_de_listados', 'n_tweets_favoritados',\n",
        "                                      'num_de_tweets_totais', 'verificado', 'data'])\n",
        "      \n",
        "      return self.__dados_participantes, 'historico', None\n",
        "  \n",
        "  @staticmethod\n",
        "  def salvar_arquivo(dataframe, tipo_pesquisa, tipo_participante):\n",
        "    \n",
        "    data_extracao = corrigir_timezone.__func__(tipo='data')\n",
        "    dir_csvs = f'/content/drive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/DataFrames/'\n",
        "\n",
        "    if tipo_pesquisa == 'historico':\n",
        "           \n",
        "      file_variable_ = dir_csvs + 'Participantes'\n",
        "      file_name = 'historico.csv'\n",
        "      file_path_ = file_variable_ + \"/\" + file_name\n",
        "      diretorios(file_variable=file_variable_, file_path=file_path_, dataframe=dataframe)\n",
        "    \n",
        "    else:\n",
        "\n",
        "      file_variable_ = dir_csvs + \"/\" + tipo_participante  + \"/\" + tipo_pesquisa + \"/\" + data_extracao\n",
        "      file_name = tipo_participante[0:3] + \"_\" + tipo_pesquisa[0:3] + \"_\" + data_extracao + '.csv'\n",
        "      file_path_ = file_variable_ + \"/\" + file_name\n",
        "      diretorios(file_variable=file_variable_, file_path=file_path_)\n",
        "\n",
        "  @staticmethod     \n",
        "  def diretorios(file_variable, file_path, dataframe):\n",
        "\n",
        "    if not os.path.exists(file_variable):\n",
        "      os.makedirs(file_variable)\n",
        "      dataframe.to_csv(file_path, index=False)\n",
        "\n",
        "    else:\n",
        "      with open(file_path, 'a') as f_object:\n",
        "        writer_object = writer(f_object, delimiter=',')\n",
        "        \n",
        "        linhas = []\n",
        "        \n",
        "        for row in range(0, len(dataframe)):\n",
        "          linhas.append([row for row in dataframe.iloc[row]])\n",
        "          writer_object.writerow(linhas[row])\n",
        "\n",
        "        f_object.close()\n",
        "    \n",
        "  @staticmethod\n",
        "  def corrigir_timezone(tipo):\n",
        "    \n",
        "    utcmoment_naive = datetime.utcnow()\n",
        "    utcmoment = utcmoment_naive.replace(tzinfo=pytz.utc)\n",
        "    tz = 'America/Sao_Paulo'\n",
        "    extracao = utcmoment.astimezone(pytz.timezone(tz)) - timedelta(hours=0, minutes=60)\n",
        "    data_de_extracao = extracao.date().strftime(\"%d%m%Y\")\n",
        "\n",
        "    if tipo == 'data':\n",
        "      return data_de_extracao\n",
        "    \n",
        "    elif tipo == 'hora':\n",
        "      return extracao"
      ],
      "metadata": {
        "id": "VUzJV9SfHLw5"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PARTICIPANTS INFORMATIONS"
      ],
      "metadata": {
        "id": "-tWq-LFkMPmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class informacoes_de_participantes(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  @staticmethod\n",
        "  def checar_tipo_participante(usuario):\n",
        "      if usuario in participantes_camarote:\n",
        "        return 'Camarote'\n",
        "      elif usuario in participantes_pipoca:\n",
        "        return 'Pipoca'\n",
        "      elif usuario in perfis_oficiais_bbb:\n",
        "        return 'PerfilOficial'\n",
        "      else:\n",
        "        return 'Publico'\n",
        "  \n",
        "  #def status_do_participante -> eliminado? anjo? lider? acompanhar ao longo do tempo e comparar com o \"desempenho\" no twitter"
      ],
      "metadata": {
        "id": "Lu2x5fyPALbL"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEXT OPERATIONS"
      ],
      "metadata": {
        "id": "jdXXGgr9MKZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# metodo para adicionar termos mais relevantes para o modelo *publico e por participante, exemplo, memes\n",
        "#termos mais frequentes para determinado participante\n",
        "class termos_de_pesquisa(object):\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def termos(self):\n",
        "    \n",
        "    dicionario_de_termos_de_pesquisa = {'BigBrotherBrasil': {'BBB', 'BBB22', '#REDEBBB', '#BBB', '#BBB22'},\n",
        "         'Participantes': {'Arthur': ['Arthur', 'Artur', 'Aguiar', 'Arthur Aguiar', '\\u2747\\ufe0f', '#TeamAguiar', '#TeamArthurAguiar'],\n",
        "                           'Naiara Azevedo': ['Naiara', 'Azevedo', '1f4b8', '#TeamNaiara', 'Nai', 'cantora'],\n",
        "                           'Pedro Scooby': ['Pedro', 'Scooby', '1f30a', '#TeamScooby', '#TimeScooby', '1f499'],\n",
        "                           'Brunna Gonçalves': ['Brunna', 'Gonçalves', '1f984', '#BBBRUNNA', 'BBBrunna'],\n",
        "                           'Paulo André': ['Paulo', 'André', '1f3c1', '#TeamPauloAndré', 'TeamPauloAndre'],\n",
        "                           'Maria': ['Maria', '1f40d\t', '#TEAMMARIA', 'TIME MARIA', 'MARICONAS'],\n",
        "                           'Jade Picon': ['Jade', 'Picon', '1f32a\\ufe0f', 'furacão', '#TeamJade', 'Picão', 'KdJade', 'Picões', 'Furacao'],\n",
        "                           'Douglas Silva': ['Douglas', 'Silva', '1f3b2', 'dado', 'dadinho', '#TeamDouglasSilva', '1f44a\\U0001f3ff'],\n",
        "                           'Linn da Quebrada': ['Linn', 'Quebrada', '1f9dc\\u200d\\u2640\\ufe0f', '1f9dc\\U0001f3ff\\u200d\\u2640\\ufe0f',\n",
        "                                                '1f9dc\\U0001f3fb\\u200d\\u2640\\ufe0f', '1f9dc\\U0001f3fe\\u200d\\u2640\\ufe0f', '1f9dc\\U0001f3fc\\u200d\\u2640\\ufe0f',\n",
        "                                                '1f9dc\\U0001f3fd\\u200d\\u2640\\ufe0f', '1f9dc', '1f9dc\\U0001f3ff', '1f9dc\\U0001f3fb', '1f9dc\\U0001f3fe',\n",
        "                                                '1f9dc\\U0001f3fc', '1f9dc\\U0001f3fd', '#TeamLinn', '#linndonas', '#LinnDonas'],\n",
        "                           'Tiago Abravanel': ['Tiago', 'Abravanel', '1f43b', '#TeamAbrava', '#TeamAbravanel', '1f9f8'],\n",
        "                           'Laís Caldas': ['Laís', 'Caldas', '1f462', '#TeamLais', '#TeamLaís', 'Time Lais', '1f49a'],\n",
        "                           'Luciano Estevan': ['Luciano', 'Estevan', '1f981', '#TeamLucianoEstevan', 'Lu', '1f346'],\n",
        "                           'Jessilane': ['Jessilane', '1f9ec', 'Jessi', '#TeamJessi', '#TimeJessi', '1f49c', 'Charmanders'],\n",
        "                           'Eliezer': ['Eliezer', '1F437', 'Eli', '#TeamEli', '1F953', '1F416', '1F43D'],\n",
        "                           'Eslovênia Marques': ['Eslovênia', 'Eslováquia', 'Marques', 'Eslô', '1f1f8\\U0001f1ee',\n",
        "                                                 'Time Eslo', 'Team Eslo', '#TimeEslô', '#TeamEslô', 'Eslovenia'],\n",
        "                           'Bábara Heck': ['Bárbara', 'Heck', '1f980', '#TeamBá', 'TeamBá', 'TimeBá',\n",
        "                                           'TimeBa', 'TeamBa', 'TeamBah', 'BBBah', 'Bá', 'Ba', '1f9a6'],\n",
        "                           'Rodrigo Mussi': ['Rodrigo Mussi', 'Mussi', 'Rodrigo', 'TEAM MUSSI', '#TeamMussi',\n",
        "                                             '#TimeMussi', '1f977\\U0001f3ff', '1f977', '1f977\\U0001f3fb',\n",
        "                                             '1f977\\U0001f3fe', '1f977\\U0001f3fc', '1f977\\U0001f3fd', 'ninja', 'ninjas', 'TEAM NINJA'],\n",
        "                           'Natália Deodato': ['Natália', 'Deodato', '#TeamNaty', '#TimeNaty', 'Naty', 'vitilindos',\n",
        "                                               '1f483', '1f483\\U0001f3ff', '1f483\\U0001f3fb',\n",
        "                                               '1f483\\U0001f3fe', '1f483\\U0001f3fc', '1f483\\U0001f3fd'],\n",
        "                           'Vinicius': ['Vinicius', 'TeamVyni', '#TimeVyni', '#TeamVyni', '1f4a1'],\n",
        "                           'Lucas Bissoli': ['Lucas', 'Bissoli', '#TimeBissoli', '#TeamBissoli', \n",
        "                                             '1f3c4\\u200d\\u2642\\ufe0f', '1f3c4\\U0001f3ff\\u200d\\u2642\\ufe0f', \n",
        "                                             '1f3c4\\U0001f3fb\\u200d\\u2642\\ufe0f', '1f3c4\\U0001f3fe\\u200d\\u2642\\ufe0f',\n",
        "                                             '1f3c4\\U0001f3fc\\u200d\\u2642\\ufe0f', '1f3c4\\U0001f3fd\\u200d\\u2642\\ufe0f',\n",
        "                                             '1f3c4', '1f3c4\\U0001f3ff', '1f3c4\\U0001f3fb', '1f3c4\\U0001f3fe',\n",
        "                                             '1f3c4\\U0001f3fc', '1f3c4\\U0001f3fd', '1f3c4\\u200d\\u2640\\ufe0f',\n",
        "                                             '1f3c4\\U0001f3ff\\u200d\\u2640\\ufe0f', '1f3c4\\U0001f3fb\\u200d\\u2640\\ufe0f',\n",
        "                                             '1f3c4\\U0001f3fe\\u200d\\u2640\\ufe0f', '1f3c4\\U0001f3fc\\u200d\\u2640\\ufe0f',\n",
        "                                             '1f3c4\\U0001f3fd\\u200d\\u2640\\ufe0f']}}\n",
        "    return dicionario_de_termos_de_pesquisa\n",
        "    #termos_a_analisar = []\n",
        "\n",
        "    #avaliador_de_termos()\n",
        "\n",
        "    #return termos_de_pesquisa_BBB\n",
        "\n",
        "    #analisador_de_termos(termos_de_pesquisa_participantes)\n",
        "\n",
        "  def analisador_de_termos(self, termos_atuais, termos_novos):\n",
        "\n",
        "    self.__termos_atuais = termos_atuais\n",
        "    self.__termos_novos = termos_novos\n",
        "\n",
        "    #for termo in termos_atuais:\n",
        "      # if novo termo ajudar a explicar:\n",
        "      # append\n",
        "      # if len(termos) > limite:\n",
        "      # termo_a_remover = ()\n",
        "      # for termo in termos atuais atualizado:\n",
        "      # termo_a_remover['explicacao'] < min_a_explicar:\n",
        "      # termos_atuais.pop(termo_a_remover)\n",
        "\n",
        "  def tendencia(self, termo_novo):\n",
        "\n",
        "    self.termo_novo = termo_novo\n",
        "    pass\n",
        "\n",
        "  def correlacao_participantes(self):\n",
        "    pass"
      ],
      "metadata": {
        "id": "SbQrOBweyleK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dicionario_participantes = {'@Aguiarthur': 'Arthur Aguiar', \n",
        "                            '@Naiarazevedo': 'Naiara Azevedo',\n",
        "                            '@PedroScooby': 'Pedro Scooby',\n",
        "                            '@brunnagoncalves': 'Bruna Gonçalves',\n",
        "                            '@iampauloandre': 'Paulo André',\n",
        "                            '@eumaria': 'Maria',\n",
        "                            '@jadepicon': 'Jade Picon',\n",
        "                            '@Silva_DG': 'Douglas Silva',\n",
        "                            '@linndaquebrada': 'Linn da Quebrada',\n",
        "                            '@TiagoAbravanel': 'Tiago Abravanel',\n",
        "                            '@Dra_laiscaldass': 'Laís Caldas',\n",
        "                            '@LucianoEstevan': 'Luciano Estevan',\n",
        "                            '@a_jessilane': 'Jessilane',\n",
        "                            '@eusouoeli': 'Eliezer',\n",
        "                            '@eslomarques': 'Eslovênia Maques',\n",
        "                            '@bbaheck': 'Bábara Heck',\n",
        "                            '@oficialmussi': 'Rodrigo Mussi',\n",
        "                            '@oficial_deodato': 'Natália Deodato',\n",
        "                            '@vyniof': 'Vinicius',\n",
        "                            '@LucasBissoli_': 'Lucas Bissoli'}\n",
        "\n",
        "participantes_camarote = ['Aguiarthur', 'Naiarazevedo', 'PedroScooby', 'brunnagoncalves', \n",
        "                          'iampauloandre', 'eumaria', 'jadepicon', 'Silva_DG', 'linndaquebrada', 'TiagoAbravanel']\n",
        "\n",
        "participantes_pipoca = ['Dra_laiscaldass', 'LucianoEstevan', 'a_jessilane', 'Eli', 'eslomarques', \n",
        "                        'bbaheck', 'oficialmussi', 'oficial_deodato', 'vyniof', 'LucasBissoli_']\n",
        "\n",
        "participantes_camarote_twitter = ['@Aguiarthur', '@Naiarazevedo', '@PedroScooby', '@brunnagoncalves', \n",
        "                          '@iampauloandre', '@eumaria', '@jadepicon', '@Silva_DG', '@linndaquebrada', '@TiagoAbravanel']\n",
        "\n",
        "participantes_pipoca_twitter = ['@Dra_laiscaldass', '@LucianoEstevan', '@a_jessilane', '@eusouoeli', '@eslomarques', \n",
        "                        '@bbaheck', '@oficialmussi', '@oficial_deodato', '@vyniof', '@LucasBissoli_']\n",
        "\n",
        "perfis_oficiais_bbb = ['bbb', 'globoplay', 'tadeuschmidt']\n",
        "\n",
        "participantes_total = []\n",
        "participantes_total.extend(participantes_pipoca)\n",
        "participantes_total.extend(participantes_camarote)"
      ],
      "metadata": {
        "id": "WngU40ePkg1c"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTES"
      ],
      "metadata": {
        "id": "zUaqg0LXjs8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api = auth_twitter().parametros_de_acesso('/content/drive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/auth.txt')"
      ],
      "metadata": {
        "id": "af5Ep4TuZbTp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PESQUISAS POR NOME DE USUARIO(S)"
      ],
      "metadata": {
        "id": "9m2OSKZl5MeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pesquisar = pesquisar_tweets()\n",
        "organizar = organizar_coletas_de_tweets()"
      ],
      "metadata": {
        "id": "w_YWScvPN0pu"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets, tipo_de_pesquisa, tipo_de_participante = pesquisar.por_usuario('@PedroScooby')\n",
        "dicionario, tipo_de_pesquisa = organizar.criar_dicionario(tweets, tipo_de_pesquisa)\n",
        "dataframe, tipo_de_participante, tipo_de_pesquisa = organizar.dataframe_tweets(dicionario, tipo_de_pesquisa, multipla=False)\n",
        "salvar_arquivo(tipo_pesquisa=tipo_de_pesquisa, dataframe=dataframe, tipo_participante=tipo_de_participante)"
      ],
      "metadata": {
        "id": "rDl4iAtVCw-Y"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets, tipo_de_pesquisa, tipo_de_participante = pesquisar.por_usuario('@bbb')\n",
        "dicionario, tipo_de_pesquisa = organizar.criar_dicionario(tweets, tipo_de_pesquisa)\n",
        "dataframe, tipo_de_participante, tipo_de_pesquisa = organizar.dataframe_tweets(dicionario, tipo_de_pesquisa, multipla=False)\n",
        "organizar.salvar_arquivo(tipo_pesquisa=tipo_de_pesquisa, dataframe=dataframe, tipo_participante=tipo_de_participante)"
      ],
      "metadata": {
        "id": "n7oXnKqfIzYR"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets, tipo_de_pesquisa, tipo_de_participante = pesquisar.por_usuarios(participantes_total)\n",
        "dicionario, tipo_de_pesquisa = organizar.criar_dicionario(tweets, tipo_de_pesquisa)\n",
        "dataframe, tipo_de_participante, tipo_de_pesquisa = organizar.dataframe_tweets(dicionario, tipo_de_pesquisa, multipla=True)\n",
        "organizar.salvar_arquivo(tipo_pesquisa=tipo_de_pesquisa, dataframe=dataframe, tipo_participante=tipo_de_participante)"
      ],
      "metadata": {
        "id": "HyDlkJwE5O2I"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HISTÓRICO DOS PARTICIPANTES"
      ],
      "metadata": {
        "id": "YnL3PgPJMV_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets, tipo_de_pesquisa, tipo_de_participante = pesquisar.historico_participante(participantes_total)\n",
        "dataframe, tipo_de_pesquisa, tipo_de_participante = organizar.dataframe_dos_participantes(tweets)\n",
        "organizar.salvar_arquivo(dataframe=dataframe, tipo_pesquisa=tipo_de_pesquisa, tipo_participante=tipo_de_participante)"
      ],
      "metadata": {
        "id": "ilwiR0-WvgMZ"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PESQUISA POR TERMO"
      ],
      "metadata": {
        "id": "B7ANX5byMa0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "termos = termos_de_pesquisa().termos()\n",
        "dicionario_de_termos = termos['BigBrotherBrasil']\n",
        "query = (' '.join([str(item) for item in dicionario_de_termos]).replace(' ', ' OR ')) #metodo estático para criar a query a partir de um dicionario dado\n",
        "tweets, tipo_de_pesquisa = pesquisar.por_termo(query=query, result_type='popular')\n",
        "dicionario, tipo_de_pesquisa = organizar.criar_dicionario(tweets, tipo_de_pesquisa)\n",
        "dataframe, tipo_participante, tipo_de_pesquisa = organizar.dataframe_tweets(dicionario, tipo_de_pesquisa)\n",
        "organizar.salvar_arquivo(tipo_participante=tipo_participante, tipo_pesquisa=tipo_de_pesquisa, dataframe=dataframe)"
      ],
      "metadata": {
        "id": "Du_UrPpInk5X"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PESQUISA PERFIL OFICIAL"
      ],
      "metadata": {
        "id": "pn0nzU5z4RvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets, tipo_de_pesquisa, tipo_participante = pesquisar.por_usuario(participante='@bbb', result_type='popular', count=200)\n",
        "dicionario, tipo_de_pesquisa = organizar.criar_dicionario(tweets, tipo_de_pesquisa)\n",
        "dataframe, tipo_de_participante, tipo_de_pesquisa = organizar.dataframe_tweets(dicionario, tipo_de_pesquisa)\n",
        "organizar.salvar_arquivo(tipo_participante=tipo_de_participante, tipo_pesquisa=tipo_de_pesquisa, dataframe=dataframe)"
      ],
      "metadata": {
        "id": "xtgOxeFsOgfr"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MELHORIAS"
      ],
      "metadata": {
        "id": "O7EIhyB3FjM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funcionalidades\n",
        "\n",
        "# property para não precisar \"herdar\" as informações de tipo de pesquisa e tipo de consulta (GETTER)\n",
        "# coletas automáticas/schedule!\n",
        "# criar uma base de textos de todos os tweets/csvs\n",
        "# rf para definir se tweet é spam de tweet ou campanha de ganhar seguidores para excluir da base\n",
        "# usar texto do perfil oficial do bbb para \"lider\" anjo provas etc\n",
        "\n",
        "# Analises\n",
        "\n",
        "# em média, quantos seguidores ganha após um tweet positivo? e quantos perde após um tweet negativo?\n",
        "# modelo de classificação baseado na popularidade do participante\n",
        "# prob de ser eliminado no paredão de acordo com o atual nível de popularidade"
      ],
      "metadata": {
        "id": "aTkkDYfUpFWI"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP"
      ],
      "metadata": {
        "id": "emUJ6NgeWJbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class processamento_de_texto(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.texto_processado = []\n",
        "    self.texto_sendo_analisado = []\n",
        "    self.texto_sem_pontuacao = []\n",
        "    self.texto_sem_stopwords = []\n",
        "    self.texto_tokenizado = []\n",
        "\n",
        "  def remover_https_e_espacamento(self, base_de_textos):\n",
        "    \n",
        "    for tweet in base_de_textos:\n",
        "        self.__sem_https = re.sub(r'https.+', '', str(tweet))\n",
        "        self.__sem_caractere_de_espacamento = re.sub(r'\\n+', ' ', self.__sem_https)\n",
        "        self.texto_sendo_analisado.append(self.__sem_caractere_de_espacamento)\n",
        "    \n",
        "    return self.texto_sendo_analisado\n",
        "\n",
        "  def remover_pontuacao(self, base_de_textos):\n",
        "    \n",
        "    for _ in base_de_textos:\n",
        "      self.__s = re.sub(r'\\W', ' ', _)\n",
        "      self.__x = re.sub(' +', ' ', self.__s)\n",
        "      self.texto_sem_pontuacao.append(self.__x)\n",
        "      \n",
        "    return self.texto_sem_pontuacao\n",
        "\n",
        "  def remover_stopwords(self, base_de_textos):\n",
        "    \n",
        "    for _ in base_de_textos:\n",
        "      self.__x = [palavra for palavra in _.split() if palavra.lower() not in nltk.corpus.stopwords.words('portuguese')]\n",
        "      self.texto_sem_stopwords.append(self.__x)\n",
        "\n",
        "    return self.texto_sem_stopwords\n",
        "\n",
        "  def tokenizar_texto(self, base_de_textos):\n",
        "\n",
        "    for i in range(0, len(base_de_textos)):\n",
        "      \n",
        "      for _ in base_de_textos[i]:\n",
        "        \n",
        "        self.__x = word_tokenize(_)\n",
        "        self.texto_tokenizado.append(self.__x)\n",
        "    \n",
        "    return self.texto_tokenizado\n",
        "\n",
        "  def preprocessar(self, tweets, column):\n",
        "\n",
        "    self.__base_de_tweets = [row for row in tweets.iloc[:,column]]\n",
        "\n",
        "    self.__sem_link_nem_espaco = self.remover_https_e_espacamento(self.__base_de_tweets)\n",
        "    self.__sem_pontuacao = self.remover_pontuacao(self.__sem_link_nem_espaco)  \n",
        "    self.__sem_stopwords = self.remover_stopwords(self.__sem_pontuacao)\n",
        "    self.__tokenizado = self.tokenizar_texto(self.__sem_stopwords)\n",
        "    self.texto_processado.append(self.__tokenizado)\n",
        "    self.texto_processado_ = self.list_reduction(self.texto_processado)\n",
        "    \n",
        "    return self.texto_processado_\n",
        "  \n",
        "  def list_reduction(self, texto_processado):\n",
        "\n",
        "    self.__texto_processado = texto_processado\n",
        "\n",
        "    return [item for sublist in self.__texto_processado[0] for item in sublist]"
      ],
      "metadata": {
        "id": "x2TvzwPJ3NJW"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(f'/content/drive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/DataFrames/PerfilOficial/popular/23012022/Per_pop_23012022.csv',\n",
        "                 usecols=[1])\n",
        "df.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "f5UjDznNQG1E",
        "outputId": "7782537f-ca0c-4e85-b039-250172774fa8"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6024628c-336d-417c-9793-444b0635d6b4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>E vamos de música no #BBB22 🎤🎶\\n\\n#RedeBBB htt...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6024628c-336d-417c-9793-444b0635d6b4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6024628c-336d-417c-9793-444b0635d6b4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6024628c-336d-417c-9793-444b0635d6b4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                text\n",
              "0  E vamos de música no #BBB22 🎤🎶\\n\\n#RedeBBB htt..."
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(f'/content/gdrive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/BaseSpacyPT/pt_core_news_sm-2.2.5/pt_core_news_sm/pt_core_news_sm-2.2.5')"
      ],
      "metadata": {
        "id": "aGOsjLh-dbm9"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = processamento_de_texto().preprocessar(tweets=df, column=0)"
      ],
      "metadata": {
        "id": "ld5cOF4UrJTc"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcaN51iAsF2I",
        "outputId": "2f026ecc-9ad9-43d4-e64d-1a94ff95c73c"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(a[0])"
      ],
      "metadata": {
        "id": "RUk9UXtFsCBy"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hWKpr6PrnZA0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}