{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1SR-3i9Wrab9mlb7RdoAnO_79DAVvlJaf",
      "authorship_tag": "ABX9TyOvd8PBQ+RAONTrFIYGxBB8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmoraissc/Projetos_de_Data_Science/blob/main/Sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pip freeze --local > /content/gdrive/My\\ Drive/colab_installed.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1A1VTIckbwd",
        "outputId": "36e5fb8a-af78-4d25-c251-d916d75384e4"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSTALLATIONS"
      ],
      "metadata": {
        "id": "-R-W39LWMbod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji, !pip install xlsxwriter, ! python -m spacy download pt_core_news_sm, !pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DV_e0ztqQxk",
        "outputId": "ef861696-5524-47e8-fd5a-07128b92d145"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.6.3.tar.gz (174 kB)\n",
            "\u001b[?25l\r\u001b[K     |â–ˆâ–‰                              | 10 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–Š                            | 20 kB 19.1 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 30 kB 21.4 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 40 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 51 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                    | 61 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 71 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 81 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 92 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 102 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 112 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 122 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 133 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 143 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 153 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 163 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 174 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 174 kB 8.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.3-py3-none-any.whl size=170298 sha256=258c9f07b7af494de2db8b5a69812cff964a9016675aecd781a9c9dabf142754\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/8b/d7/ad579fbef83c287215c0caab60fb0ae0f30c4d7ce5f580eade\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTS"
      ],
      "metadata": {
        "id": "mdlamNraMahj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "A2De13A6eBh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f5f8827-9065-456b-cc13-eaede45b0bc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tweepy, nltk, json, os, csv, xlsxwriter, pytz, re, heapq, string, spacy\n",
        "from pandas import ExcelWriter as ExcelWriter\n",
        "from pathlib import Path\n",
        "from datetime import datetime, date, timedelta\n",
        "from csv import writer\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class auth_twitter(object):\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def parametros_de_acesso(self, arquivo):\n",
        "    \n",
        "    self.__arquivo = arquivo\n",
        "\n",
        "    with open (arquivo, 'r') as arquivo:\n",
        "      texto = arquivo.readlines()[0].split(',')\n",
        "    \n",
        "    access_token = texto[0].split('=')[1]\n",
        "    access_token_secret = texto[1].split('=')[1]\n",
        "    bearer_token = texto[2].split('=')[1]\n",
        "    consumer_key = texto[3].split('=')[1]\n",
        "    consumer_secret = texto[4].split('=')[1]\n",
        "\n",
        "    arquivo.close()\n",
        "\n",
        "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_token, access_token_secret)\n",
        "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "    \n",
        "    return api"
      ],
      "metadata": {
        "id": "gmuNrfoS7pC-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA EXCTRACTION"
      ],
      "metadata": {
        "id": "lzWKDB6gMX6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class pesquisar_tweets(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.resultados_da_pesquisa = []\n",
        "    self.historico_info_participantes = []\n",
        "    self.informacoes_hist_participante = []\n",
        "\n",
        "  def por_termo(self, query: tuple, result_type='recent', count=int(100)):\n",
        "    \n",
        "    \"\"\"Este mÃ©todo faz a consulta de tweets a partir de termos. \n",
        "    Query: a query deve ser uma tupla (str) utilizando-se de operadores lÃ³gicos\n",
        "    tais como OR, AND. Ex. BBB AND #REDEBBB. \n",
        "    Result_type: pode ser ou popular ou recent. O primeiro buscarÃ¡ os tweets\n",
        "    que corresponderem a query publicados recentemente. O segundo serÃ¡ aqueles\n",
        "    tweets mais populares. O padrÃ£o Ã© recent.\n",
        "    Count: recebe um valor inteiro para limitar o nÃºmero de tweets coletados por consulta. O padrÃ£o Ã© 100.\n",
        "    Retorna os resultados da pesquisa (lista) e o tipo de resultado (popular ou recent).\n",
        "    \"\"\"\n",
        "\n",
        "    global api\n",
        "\n",
        "    self.__query = query\n",
        "    self.__result_type = result_type\n",
        "    self.__count = count\n",
        "\n",
        "    for status in tweepy.Cursor(api.search,\n",
        "                                q=self.__query,\n",
        "                                result_type=self.__result_type,\n",
        "                                count=self.__count).items():\n",
        "                                \n",
        "                                self.resultados_da_pesquisa.append(status)\n",
        "\n",
        "    return self.resultados_da_pesquisa, self.__result_type\n",
        " \n",
        "  def por_usuario(self, participante: str, result_type='recent', count=int(100)):\n",
        "    \n",
        "    \"\"\"Este mÃ©todo faz a consulta de tweets a partir do nome de um usuÃ¡rio. \n",
        "    Participante: recebe o screen_name do usuÃ¡rio do twitter. \n",
        "    Result_type: pode ser ou popular ou recent. O primeiro buscarÃ¡ os tweets\n",
        "    que corresponderem a query publicados recentemente. O segundo serÃ¡ aqueles\n",
        "    tweets mais populares. O padrÃ£o Ã© recent.\n",
        "    Count: recebe um valor inteiro para limitar o nÃºmero de tweets coletados por consulta. O padrÃ£o Ã© 100.\n",
        "    Retorna os resultados da pesquisa em uma lista, o tipo de resultado (popular ou recent) e o tipo de\n",
        "    participante (camarote, pipoca, pÃºblico ou perfil oficial).\n",
        "    \"\"\"\n",
        "\n",
        "    global api, participantes_camarote, participantes_pipoca # ajustar para quando as informaÃ§Ãµes de pesquisa estiverem na classe\n",
        "\n",
        "    self.__participante = participante\n",
        "    self.__result_type = result_type\n",
        "    self.__count = count\n",
        "    \n",
        "    tipo_participante = informacoes_de_participantes().checar_tipo_participante(participante)\n",
        "\n",
        "    for status in tweepy.Cursor(api.user_timeline,\n",
        "                                screen_name=self.__participante,\n",
        "                                result_type=self.__result_type,\n",
        "                                exclude_replies=False).items(self.__count):\n",
        "                                \n",
        "                                self.resultados_da_pesquisa.append(status)\n",
        "\n",
        "    return self.resultados_da_pesquisa, self.__result_type, tipo_participante\n",
        "\n",
        "  def por_usuarios(self, participantes: list, result_type='recent', count=int(100)):\n",
        "    \n",
        "    \"\"\"Este mÃ©todo faz a consulta de tweets a partir do nome de pelo menos um usuÃ¡rio. \n",
        "    Participante: recebe o screen_name dos usuÃ¡rios do twitter que deseja ser consultado. \n",
        "    Result_type: pode ser ou popular ou recent. O primeiro buscarÃ¡ os tweets\n",
        "    que corresponderem a query publicados recentemente. O segundo serÃ¡ aqueles\n",
        "    tweets mais populares. O padrÃ£o Ã© recent.\n",
        "    Count: recebe um valor inteiro para limitar o nÃºmero de tweets coletados por consulta. O padrÃ£o Ã© 100.\n",
        "    Retorna uma lista com os resultados da pesquisa, o tipo de resultado (popular ou recent) e o tipo de pesquisa (Ãºnica ou mÃºltipla).\n",
        "    \"\"\"\n",
        "\n",
        "    global api\n",
        "\n",
        "    self.__participantes = participantes\n",
        "    self.__result_type = result_type\n",
        "    self.__count = count\n",
        "    \n",
        "    for participante in self.__participantes:\n",
        "\n",
        "      for status in tweepy.Cursor(api.user_timeline,\n",
        "                                  screen_name=participante,\n",
        "                                  result_type=self.__result_type,\n",
        "                                  exclude_replies=False).items(self.__count):\n",
        "                                  \n",
        "                                  self.resultados_da_pesquisa.append(status)\n",
        "\n",
        "    return self.resultados_da_pesquisa, self.__result_type, 'Todos'\n",
        "\n",
        "  def infos_do_usuario(self, usuario: str):\n",
        "  \n",
        "    \"\"\"Este mÃ©todo faz a consulta de informaÃ§Ãµes do usuÃ¡rio (nÃºmero de seguidores (int), nÃºmero de perfis que segue (int),\n",
        "    nÃºmero de  (int), nÃºmero de tweets favoritados (int), nÃºmero de tweets totais (int), verificado (booleano) e a data\n",
        "    Ã© uma informaÃ§Ã£o gerada pelo mÃ©todo \"corrigir_timezone\" cujo padrÃ£o Ã© a hora, a fim de organizar os arquivos nas devidas pastas). \n",
        "    Usuario: recebe o screen_name do usuÃ¡rio do twitter que deseja ser consultado.\n",
        "    Retorna uma lista.\n",
        "    \"\"\"\n",
        "\n",
        "    global api\n",
        "\n",
        "    self.__user_results = api.get_user(usuario)\n",
        "\n",
        "    extracao = corrigir_timezone.__func__(tipo='hora')\n",
        "    \n",
        "    participante = self.__user_results.name\n",
        "    twitter = self.__user_results.screen_name\n",
        "    seguidores = self.__user_results.followers_count\n",
        "    seguindo = self.__user_results.friends_count\n",
        "    numero_de_listados = self.__user_results.listed_count\n",
        "    n_tweets_favoritados = self.__user_results.favourites_count\n",
        "    num_de_tweets_totais = self.__user_results.statuses_count\n",
        "    verificado = self.__user_results.verified\n",
        "\n",
        "    self.informacoes_hist_participante.append({'participante': str(participante),\n",
        "                                          'twitter': str(twitter),\n",
        "                                          'n_seguidores': int(seguidores),\n",
        "                                          'n_seguindo': int(seguindo),\n",
        "                                          'n_de_listados': int(numero_de_listados),\n",
        "                                          'n_tweets_favoritados': int(n_tweets_favoritados),\n",
        "                                          'num_de_tweets_totais': int(num_de_tweets_totais),\n",
        "                                          'verificado': bool(verificado),\n",
        "                                          'data': extracao})\n",
        "    \n",
        "    return self.informacoes_hist_participante\n",
        "\n",
        "  def historico_participante(self, participantes: list):\n",
        "\n",
        "    \"\"\"Este mÃ©tood visa, a partir da busa por usuario, iterar em uma lista de usuÃ¡rios e fazer uma varredura por todos os participantes\n",
        "    buscando as informaÃ§Ãµes histÃ³ricas (citadas em infos_do_usuario).\n",
        "    Participantes: recebe uma lista de screen_names para iteraÃ§Ã£o.\n",
        "    Retorna uma lista com todas as informaÃ§Ãµes coletadas.\n",
        "    \"\"\"\n",
        "\n",
        "    for participante in participantes:\n",
        "      _ = pesquisar_tweets().infos_do_usuario(participante)\n",
        "      self.historico_info_participantes.extend(_)\n",
        "    \n",
        "    return self.historico_info_participantes, 'historico', 'todos'"
      ],
      "metadata": {
        "id": "RL_XrvFE1GBp"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA ORGANIZATION/FILE HANDLING"
      ],
      "metadata": {
        "id": "oi2o1bokMVPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class organizar_coletas_de_tweets(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.dicionario = {}\n",
        "    self.lista = []\n",
        "\n",
        "  def criar_dicionario(self, tweets_status, tipo):\n",
        "      \n",
        "    d1 = corrigir_timezone.__func__(tipo='data')\n",
        "\n",
        "    self.__tweets_status = tweets_status\n",
        "    self.__tipo = tipo\n",
        "\n",
        "    for each_json_tweet in tweets_status:\n",
        "      _ = json.dumps(each_json_tweet._json)\n",
        "      \n",
        "      if d1 not in self.dicionario.keys():\n",
        "        tweet = {d1: [json.loads(_)]}\n",
        "        self.dicionario.update(tweet)\n",
        "        \n",
        "      else:\n",
        "        self.dicionario[d1].append(json.loads(_))\n",
        "      \n",
        "    return self.dicionario, self.__tipo\n",
        "\n",
        "  def dataframe_tweets(self, dicionario, tipo_de_pesquisa, multipla=False):\n",
        "    \n",
        "    d1 = corrigir_timezone.__func__(tipo='data')\n",
        "    \n",
        "    self.__dicionario = dicionario\n",
        "    self.__tipo_de_pesquisa = tipo_de_pesquisa\n",
        "\n",
        "    for i in range(0, len(dicionario[d1])):\n",
        "\n",
        "          tweet_id = self.__dicionario[d1][i]['id']\n",
        "          text = self.__dicionario[d1][i]['text']\n",
        "          favorite_count = self.__dicionario[d1][i]['favorite_count']\n",
        "          retweet_count = self.__dicionario[d1][i]['retweet_count']\n",
        "          created_at = self.__dicionario[d1][i]['created_at']\n",
        "          hashtags = self.__dicionario[d1][i]['entities']['hashtags']\n",
        "          user_mentions = self.__dicionario[d1][i]['entities']['user_mentions']\n",
        "          name = self.__dicionario[d1][i]['user']['name']\n",
        "          screen_name = self.__dicionario[d1][i]['user']['screen_name']\n",
        "\n",
        "          self.lista.append({'tweet_id': str(tweet_id),\n",
        "                        'text': str(text),\n",
        "                        'favorite_count': int(favorite_count),\n",
        "                        'retweet_count': int(retweet_count),\n",
        "                        'created_at': created_at,\n",
        "                        'user_mentions': user_mentions,\n",
        "                        'name': name,\n",
        "                        'screen_name': screen_name})\n",
        "          \n",
        "    self.__tweet_json_ = pd.DataFrame(self.lista, columns = \n",
        "                              ['tweet_id', 'text', \n",
        "                                'favorite_count', 'retweet_count', \n",
        "                                'created_at', 'hashtags', \n",
        "                                'user_mentions', 'name',\n",
        "                                'screen_name'])\n",
        "    \n",
        "    if multipla == False:\n",
        "      \n",
        "      self.__tipo_participante = informacoes_de_participantes().checar_tipo_participante(screen_name)\n",
        "    \n",
        "    else:\n",
        "      \n",
        "      self.__tipo_participante = 'Todos'\n",
        "\n",
        "    return self.__tweet_json_, self.__tipo_participante, self.__tipo_de_pesquisa\n",
        "\n",
        "  def dataframe_dos_participantes(self, informacoes_hist_participante):\n",
        "\n",
        "      self.__informacoes_hist_participante = informacoes_hist_participante\n",
        "\n",
        "      self.__dados_participantes = pd.DataFrame(self.__informacoes_hist_participante, columns = \n",
        "                                      ['participante', 'twitter', 'n_seguidores', \n",
        "                                      'n_seguindo','n_de_listados', 'n_tweets_favoritados',\n",
        "                                      'num_de_tweets_totais', 'verificado', 'data'])\n",
        "      \n",
        "      return self.__dados_participantes, 'historico', None\n",
        "  \n",
        "  @staticmethod\n",
        "  def salvar_arquivo(dataframe, tipo_pesquisa, tipo_participante):\n",
        "    \n",
        "    data_extracao = corrigir_timezone.__func__(tipo='data')\n",
        "    dir_csvs = f'/content/drive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/DataFrames/'\n",
        "\n",
        "    if tipo_pesquisa == 'historico':\n",
        "           \n",
        "      file_variable_ = dir_csvs + 'Participantes'\n",
        "      file_name = 'historico.csv'\n",
        "      file_path_ = file_variable_ + \"/\" + file_name\n",
        "      diretorios(file_variable=file_variable_, file_path=file_path_, dataframe=dataframe)\n",
        "    \n",
        "    else:\n",
        "\n",
        "      file_variable_ = dir_csvs + \"/\" + tipo_participante  + \"/\" + tipo_pesquisa + \"/\" + data_extracao\n",
        "      file_name = tipo_participante[0:3] + \"_\" + tipo_pesquisa[0:3] + \"_\" + data_extracao + '.csv'\n",
        "      file_path_ = file_variable_ + \"/\" + file_name\n",
        "      diretorios(file_variable=file_variable_, file_path=file_path_)\n",
        "\n",
        "  @staticmethod     \n",
        "  def diretorios(file_variable, file_path, dataframe):\n",
        "\n",
        "    if not os.path.exists(file_variable):\n",
        "      os.makedirs(file_variable)\n",
        "      dataframe.to_csv(file_path, index=False)\n",
        "\n",
        "    else:\n",
        "      with open(file_path, 'a') as f_object:\n",
        "        writer_object = writer(f_object, delimiter=',')\n",
        "        \n",
        "        linhas = []\n",
        "        \n",
        "        for row in range(0, len(dataframe)):\n",
        "          linhas.append([row for row in dataframe.iloc[row]])\n",
        "          writer_object.writerow(linhas[row])\n",
        "\n",
        "        f_object.close()\n",
        "    \n",
        "  @staticmethod\n",
        "  def corrigir_timezone(tipo):\n",
        "    \n",
        "    utcmoment_naive = datetime.utcnow()\n",
        "    utcmoment = utcmoment_naive.replace(tzinfo=pytz.utc)\n",
        "    tz = 'America/Sao_Paulo'\n",
        "    extracao = utcmoment.astimezone(pytz.timezone(tz)) - timedelta(hours=0, minutes=60)\n",
        "    data_de_extracao = extracao.date().strftime(\"%d%m%Y\")\n",
        "\n",
        "    if tipo == 'data':\n",
        "      return data_de_extracao\n",
        "    \n",
        "    elif tipo == 'hora':\n",
        "      return extracao"
      ],
      "metadata": {
        "id": "VUzJV9SfHLw5"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PARTICIPANTS INFORMATIONS"
      ],
      "metadata": {
        "id": "-tWq-LFkMPmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class informacoes_de_participantes(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  @staticmethod\n",
        "  def checar_tipo_participante(usuario):\n",
        "      if usuario in participantes_camarote:\n",
        "        return 'Camarote'\n",
        "      elif usuario in participantes_pipoca:\n",
        "        return 'Pipoca'\n",
        "      elif usuario in perfis_oficiais_bbb:\n",
        "        return 'PerfilOficial'\n",
        "      else:\n",
        "        return 'Publico'\n",
        "  \n",
        "  #def status_do_participante -> eliminado? anjo? lider? acompanhar ao longo do tempo e comparar com o \"desempenho\" no twitter"
      ],
      "metadata": {
        "id": "Lu2x5fyPALbL"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEXT OPERATIONS"
      ],
      "metadata": {
        "id": "jdXXGgr9MKZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# metodo para adicionar termos mais relevantes para o modelo *publico e por participante, exemplo, memes\n",
        "#termos mais frequentes para determinado participante\n",
        "class termos_de_pesquisa(object):\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def termos(self):\n",
        "    \n",
        "    dicionario_de_termos_de_pesquisa = {'BigBrotherBrasil': {'BBB', 'BBB22', '#REDEBBB', '#BBB', '#BBB22'},\n",
        "         'Participantes': {'Arthur': ['Arthur', 'Artur', 'Aguiar', 'Arthur Aguiar', '\\u2747\\ufe0f', '#TeamAguiar', '#TeamArthurAguiar'],\n",
        "                           'Naiara Azevedo': ['Naiara', 'Azevedo', '1f4b8', '#TeamNaiara', 'Nai', 'cantora'],\n",
        "                           'Pedro Scooby': ['Pedro', 'Scooby', '1f30a', '#TeamScooby', '#TimeScooby', '1f499'],\n",
        "                           'Brunna GonÃ§alves': ['Brunna', 'GonÃ§alves', '1f984', '#BBBRUNNA', 'BBBrunna'],\n",
        "                           'Paulo AndrÃ©': ['Paulo', 'AndrÃ©', '1f3c1', '#TeamPauloAndrÃ©', 'TeamPauloAndre'],\n",
        "                           'Maria': ['Maria', '1f40d\t', '#TEAMMARIA', 'TIME MARIA', 'MARICONAS'],\n",
        "                           'Jade Picon': ['Jade', 'Picon', '1f32a\\ufe0f', 'furacÃ£o', '#TeamJade', 'PicÃ£o', 'KdJade', 'PicÃµes', 'Furacao'],\n",
        "                           'Douglas Silva': ['Douglas', 'Silva', '1f3b2', 'dado', 'dadinho', '#TeamDouglasSilva', '1f44a\\U0001f3ff'],\n",
        "                           'Linn da Quebrada': ['Linn', 'Quebrada', '1f9dc\\u200d\\u2640\\ufe0f', '1f9dc\\U0001f3ff\\u200d\\u2640\\ufe0f',\n",
        "                                                '1f9dc\\U0001f3fb\\u200d\\u2640\\ufe0f', '1f9dc\\U0001f3fe\\u200d\\u2640\\ufe0f', '1f9dc\\U0001f3fc\\u200d\\u2640\\ufe0f',\n",
        "                                                '1f9dc\\U0001f3fd\\u200d\\u2640\\ufe0f', '1f9dc', '1f9dc\\U0001f3ff', '1f9dc\\U0001f3fb', '1f9dc\\U0001f3fe',\n",
        "                                                '1f9dc\\U0001f3fc', '1f9dc\\U0001f3fd', '#TeamLinn', '#linndonas', '#LinnDonas'],\n",
        "                           'Tiago Abravanel': ['Tiago', 'Abravanel', '1f43b', '#TeamAbrava', '#TeamAbravanel', '1f9f8'],\n",
        "                           'LaÃ­s Caldas': ['LaÃ­s', 'Caldas', '1f462', '#TeamLais', '#TeamLaÃ­s', 'Time Lais', '1f49a'],\n",
        "                           'Luciano Estevan': ['Luciano', 'Estevan', '1f981', '#TeamLucianoEstevan', 'Lu', '1f346'],\n",
        "                           'Jessilane': ['Jessilane', '1f9ec', 'Jessi', '#TeamJessi', '#TimeJessi', '1f49c', 'Charmanders'],\n",
        "                           'Eliezer': ['Eliezer', '1F437', 'Eli', '#TeamEli', '1F953', '1F416', '1F43D'],\n",
        "                           'EslovÃªnia Marques': ['EslovÃªnia', 'EslovÃ¡quia', 'Marques', 'EslÃ´', '1f1f8\\U0001f1ee',\n",
        "                                                 'Time Eslo', 'Team Eslo', '#TimeEslÃ´', '#TeamEslÃ´', 'Eslovenia'],\n",
        "                           'BÃ¡bara Heck': ['BÃ¡rbara', 'Heck', '1f980', '#TeamBÃ¡', 'TeamBÃ¡', 'TimeBÃ¡',\n",
        "                                           'TimeBa', 'TeamBa', 'TeamBah', 'BBBah', 'BÃ¡', 'Ba', '1f9a6'],\n",
        "                           'Rodrigo Mussi': ['Rodrigo Mussi', 'Mussi', 'Rodrigo', 'TEAM MUSSI', '#TeamMussi',\n",
        "                                             '#TimeMussi', '1f977\\U0001f3ff', '1f977', '1f977\\U0001f3fb',\n",
        "                                             '1f977\\U0001f3fe', '1f977\\U0001f3fc', '1f977\\U0001f3fd', 'ninja', 'ninjas', 'TEAM NINJA'],\n",
        "                           'NatÃ¡lia Deodato': ['NatÃ¡lia', 'Deodato', '#TeamNaty', '#TimeNaty', 'Naty', 'vitilindos',\n",
        "                                               '1f483', '1f483\\U0001f3ff', '1f483\\U0001f3fb',\n",
        "                                               '1f483\\U0001f3fe', '1f483\\U0001f3fc', '1f483\\U0001f3fd'],\n",
        "                           'Vinicius': ['Vinicius', 'TeamVyni', '#TimeVyni', '#TeamVyni', '1f4a1'],\n",
        "                           'Lucas Bissoli': ['Lucas', 'Bissoli', '#TimeBissoli', '#TeamBissoli', \n",
        "                                             '1f3c4\\u200d\\u2642\\ufe0f', '1f3c4\\U0001f3ff\\u200d\\u2642\\ufe0f', \n",
        "                                             '1f3c4\\U0001f3fb\\u200d\\u2642\\ufe0f', '1f3c4\\U0001f3fe\\u200d\\u2642\\ufe0f',\n",
        "                                             '1f3c4\\U0001f3fc\\u200d\\u2642\\ufe0f', '1f3c4\\U0001f3fd\\u200d\\u2642\\ufe0f',\n",
        "                                             '1f3c4', '1f3c4\\U0001f3ff', '1f3c4\\U0001f3fb', '1f3c4\\U0001f3fe',\n",
        "                                             '1f3c4\\U0001f3fc', '1f3c4\\U0001f3fd', '1f3c4\\u200d\\u2640\\ufe0f',\n",
        "                                             '1f3c4\\U0001f3ff\\u200d\\u2640\\ufe0f', '1f3c4\\U0001f3fb\\u200d\\u2640\\ufe0f',\n",
        "                                             '1f3c4\\U0001f3fe\\u200d\\u2640\\ufe0f', '1f3c4\\U0001f3fc\\u200d\\u2640\\ufe0f',\n",
        "                                             '1f3c4\\U0001f3fd\\u200d\\u2640\\ufe0f']}}\n",
        "    return dicionario_de_termos_de_pesquisa\n",
        "    #termos_a_analisar = []\n",
        "\n",
        "    #avaliador_de_termos()\n",
        "\n",
        "    #return termos_de_pesquisa_BBB\n",
        "\n",
        "    #analisador_de_termos(termos_de_pesquisa_participantes)\n",
        "\n",
        "  def analisador_de_termos(self, termos_atuais, termos_novos):\n",
        "\n",
        "    self.__termos_atuais = termos_atuais\n",
        "    self.__termos_novos = termos_novos\n",
        "\n",
        "    #for termo in termos_atuais:\n",
        "      # if novo termo ajudar a explicar:\n",
        "      # append\n",
        "      # if len(termos) > limite:\n",
        "      # termo_a_remover = ()\n",
        "      # for termo in termos atuais atualizado:\n",
        "      # termo_a_remover['explicacao'] < min_a_explicar:\n",
        "      # termos_atuais.pop(termo_a_remover)\n",
        "\n",
        "  def tendencia(self, termo_novo):\n",
        "\n",
        "    self.termo_novo = termo_novo\n",
        "    pass\n",
        "\n",
        "  def correlacao_participantes(self):\n",
        "    pass"
      ],
      "metadata": {
        "id": "SbQrOBweyleK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dicionario_participantes = {'@Aguiarthur': 'Arthur Aguiar', \n",
        "                            '@Naiarazevedo': 'Naiara Azevedo',\n",
        "                            '@PedroScooby': 'Pedro Scooby',\n",
        "                            '@brunnagoncalves': 'Bruna GonÃ§alves',\n",
        "                            '@iampauloandre': 'Paulo AndrÃ©',\n",
        "                            '@eumaria': 'Maria',\n",
        "                            '@jadepicon': 'Jade Picon',\n",
        "                            '@Silva_DG': 'Douglas Silva',\n",
        "                            '@linndaquebrada': 'Linn da Quebrada',\n",
        "                            '@TiagoAbravanel': 'Tiago Abravanel',\n",
        "                            '@Dra_laiscaldass': 'LaÃ­s Caldas',\n",
        "                            '@LucianoEstevan': 'Luciano Estevan',\n",
        "                            '@a_jessilane': 'Jessilane',\n",
        "                            '@eusouoeli': 'Eliezer',\n",
        "                            '@eslomarques': 'EslovÃªnia Maques',\n",
        "                            '@bbaheck': 'BÃ¡bara Heck',\n",
        "                            '@oficialmussi': 'Rodrigo Mussi',\n",
        "                            '@oficial_deodato': 'NatÃ¡lia Deodato',\n",
        "                            '@vyniof': 'Vinicius',\n",
        "                            '@LucasBissoli_': 'Lucas Bissoli'}\n",
        "\n",
        "participantes_camarote = ['Aguiarthur', 'Naiarazevedo', 'PedroScooby', 'brunnagoncalves', \n",
        "                          'iampauloandre', 'eumaria', 'jadepicon', 'Silva_DG', 'linndaquebrada', 'TiagoAbravanel']\n",
        "\n",
        "participantes_pipoca = ['Dra_laiscaldass', 'LucianoEstevan', 'a_jessilane', 'Eli', 'eslomarques', \n",
        "                        'bbaheck', 'oficialmussi', 'oficial_deodato', 'vyniof', 'LucasBissoli_']\n",
        "\n",
        "participantes_camarote_twitter = ['@Aguiarthur', '@Naiarazevedo', '@PedroScooby', '@brunnagoncalves', \n",
        "                          '@iampauloandre', '@eumaria', '@jadepicon', '@Silva_DG', '@linndaquebrada', '@TiagoAbravanel']\n",
        "\n",
        "participantes_pipoca_twitter = ['@Dra_laiscaldass', '@LucianoEstevan', '@a_jessilane', '@eusouoeli', '@eslomarques', \n",
        "                        '@bbaheck', '@oficialmussi', '@oficial_deodato', '@vyniof', '@LucasBissoli_']\n",
        "\n",
        "perfis_oficiais_bbb = ['bbb', 'globoplay', 'tadeuschmidt']\n",
        "\n",
        "participantes_total = []\n",
        "participantes_total.extend(participantes_pipoca)\n",
        "participantes_total.extend(participantes_camarote)"
      ],
      "metadata": {
        "id": "WngU40ePkg1c"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTES"
      ],
      "metadata": {
        "id": "zUaqg0LXjs8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api = auth_twitter().parametros_de_acesso('/content/drive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/auth.txt')"
      ],
      "metadata": {
        "id": "af5Ep4TuZbTp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PESQUISAS POR NOME DE USUARIO(S)"
      ],
      "metadata": {
        "id": "9m2OSKZl5MeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pesquisar = pesquisar_tweets()\n",
        "organizar = organizar_coletas_de_tweets()"
      ],
      "metadata": {
        "id": "w_YWScvPN0pu"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets, tipo_de_pesquisa, tipo_de_participante = pesquisar.por_usuario('@PedroScooby')\n",
        "dicionario, tipo_de_pesquisa = organizar.criar_dicionario(tweets, tipo_de_pesquisa)\n",
        "dataframe, tipo_de_participante, tipo_de_pesquisa = organizar.dataframe_tweets(dicionario, tipo_de_pesquisa, multipla=False)\n",
        "salvar_arquivo(tipo_pesquisa=tipo_de_pesquisa, dataframe=dataframe, tipo_participante=tipo_de_participante)"
      ],
      "metadata": {
        "id": "rDl4iAtVCw-Y"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets, tipo_de_pesquisa, tipo_de_participante = pesquisar.por_usuario('@bbb')\n",
        "dicionario, tipo_de_pesquisa = organizar.criar_dicionario(tweets, tipo_de_pesquisa)\n",
        "dataframe, tipo_de_participante, tipo_de_pesquisa = organizar.dataframe_tweets(dicionario, tipo_de_pesquisa, multipla=False)\n",
        "organizar.salvar_arquivo(tipo_pesquisa=tipo_de_pesquisa, dataframe=dataframe, tipo_participante=tipo_de_participante)"
      ],
      "metadata": {
        "id": "n7oXnKqfIzYR"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets, tipo_de_pesquisa, tipo_de_participante = pesquisar.por_usuarios(participantes_total)\n",
        "dicionario, tipo_de_pesquisa = organizar.criar_dicionario(tweets, tipo_de_pesquisa)\n",
        "dataframe, tipo_de_participante, tipo_de_pesquisa = organizar.dataframe_tweets(dicionario, tipo_de_pesquisa, multipla=True)\n",
        "organizar.salvar_arquivo(tipo_pesquisa=tipo_de_pesquisa, dataframe=dataframe, tipo_participante=tipo_de_participante)"
      ],
      "metadata": {
        "id": "HyDlkJwE5O2I"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HISTÃ“RICO DOS PARTICIPANTES"
      ],
      "metadata": {
        "id": "YnL3PgPJMV_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets, tipo_de_pesquisa, tipo_de_participante = pesquisar.historico_participante(participantes_total)\n",
        "dataframe, tipo_de_pesquisa, tipo_de_participante = organizar.dataframe_dos_participantes(tweets)\n",
        "organizar.salvar_arquivo(dataframe=dataframe, tipo_pesquisa=tipo_de_pesquisa, tipo_participante=tipo_de_participante)"
      ],
      "metadata": {
        "id": "ilwiR0-WvgMZ"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PESQUISA POR TERMO"
      ],
      "metadata": {
        "id": "B7ANX5byMa0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "termos = termos_de_pesquisa().termos()\n",
        "dicionario_de_termos = termos['BigBrotherBrasil']\n",
        "query = (' '.join([str(item) for item in dicionario_de_termos]).replace(' ', ' OR ')) #metodo estÃ¡tico para criar a query a partir de um dicionario dado\n",
        "tweets, tipo_de_pesquisa = pesquisar.por_termo(query=query, result_type='popular')\n",
        "dicionario, tipo_de_pesquisa = organizar.criar_dicionario(tweets, tipo_de_pesquisa)\n",
        "dataframe, tipo_participante, tipo_de_pesquisa = organizar.dataframe_tweets(dicionario, tipo_de_pesquisa)\n",
        "organizar.salvar_arquivo(tipo_participante=tipo_participante, tipo_pesquisa=tipo_de_pesquisa, dataframe=dataframe)"
      ],
      "metadata": {
        "id": "Du_UrPpInk5X"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PESQUISA PERFIL OFICIAL"
      ],
      "metadata": {
        "id": "pn0nzU5z4RvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets, tipo_de_pesquisa, tipo_participante = pesquisar.por_usuario(participante='@bbb', result_type='popular', count=200)\n",
        "dicionario, tipo_de_pesquisa = organizar.criar_dicionario(tweets, tipo_de_pesquisa)\n",
        "dataframe, tipo_de_participante, tipo_de_pesquisa = organizar.dataframe_tweets(dicionario, tipo_de_pesquisa)\n",
        "organizar.salvar_arquivo(tipo_participante=tipo_de_participante, tipo_pesquisa=tipo_de_pesquisa, dataframe=dataframe)"
      ],
      "metadata": {
        "id": "xtgOxeFsOgfr"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MELHORIAS"
      ],
      "metadata": {
        "id": "O7EIhyB3FjM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funcionalidades\n",
        "\n",
        "# property para nÃ£o precisar \"herdar\" as informaÃ§Ãµes de tipo de pesquisa e tipo de consulta (GETTER)\n",
        "# coletas automÃ¡ticas/schedule!\n",
        "# criar uma base de textos de todos os tweets/csvs\n",
        "# rf para definir se tweet Ã© spam de tweet ou campanha de ganhar seguidores para excluir da base\n",
        "# usar texto do perfil oficial do bbb para \"lider\" anjo provas etc\n",
        "\n",
        "# Analises\n",
        "\n",
        "# em mÃ©dia, quantos seguidores ganha apÃ³s um tweet positivo? e quantos perde apÃ³s um tweet negativo?\n",
        "# modelo de classificaÃ§Ã£o baseado na popularidade do participante\n",
        "# prob de ser eliminado no paredÃ£o de acordo com o atual nÃ­vel de popularidade"
      ],
      "metadata": {
        "id": "aTkkDYfUpFWI"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP"
      ],
      "metadata": {
        "id": "emUJ6NgeWJbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class processamento_de_texto(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.texto_processado = []\n",
        "    self.texto_sendo_analisado = []\n",
        "    self.texto_sem_pontuacao = []\n",
        "    self.texto_sem_stopwords = []\n",
        "    self.texto_tokenizado = []\n",
        "\n",
        "  def remover_https_e_espacamento(self, base_de_textos):\n",
        "    \n",
        "    for tweet in base_de_textos:\n",
        "        self.__sem_https = re.sub(r'https.+', '', str(tweet))\n",
        "        self.__sem_caractere_de_espacamento = re.sub(r'\\n+', ' ', self.__sem_https)\n",
        "        self.texto_sendo_analisado.append(self.__sem_caractere_de_espacamento)\n",
        "    \n",
        "    return self.texto_sendo_analisado\n",
        "\n",
        "  def remover_pontuacao(self, base_de_textos):\n",
        "    \n",
        "    for _ in base_de_textos:\n",
        "      self.__s = re.sub(r'\\W', ' ', _)\n",
        "      self.__x = re.sub(' +', ' ', self.__s)\n",
        "      self.texto_sem_pontuacao.append(self.__x)\n",
        "      \n",
        "    return self.texto_sem_pontuacao\n",
        "\n",
        "  def remover_stopwords(self, base_de_textos):\n",
        "    \n",
        "    for _ in base_de_textos:\n",
        "      self.__x = [palavra for palavra in _.split() if palavra.lower() not in nltk.corpus.stopwords.words('portuguese')]\n",
        "      self.texto_sem_stopwords.append(self.__x)\n",
        "\n",
        "    return self.texto_sem_stopwords\n",
        "\n",
        "  def tokenizar_texto(self, base_de_textos):\n",
        "\n",
        "    for i in range(0, len(base_de_textos)):\n",
        "      \n",
        "      for _ in base_de_textos[i]:\n",
        "        \n",
        "        self.__x = word_tokenize(_)\n",
        "        self.texto_tokenizado.append(self.__x)\n",
        "    \n",
        "    return self.texto_tokenizado\n",
        "\n",
        "  def preprocessar(self, tweets, column):\n",
        "\n",
        "    self.__base_de_tweets = [row for row in tweets.iloc[:,column]]\n",
        "\n",
        "    self.__sem_link_nem_espaco = self.remover_https_e_espacamento(self.__base_de_tweets)\n",
        "    self.__sem_pontuacao = self.remover_pontuacao(self.__sem_link_nem_espaco)  \n",
        "    self.__sem_stopwords = self.remover_stopwords(self.__sem_pontuacao)\n",
        "    self.__tokenizado = self.tokenizar_texto(self.__sem_stopwords)\n",
        "    self.texto_processado.append(self.__tokenizado)\n",
        "    self.texto_processado_ = self.list_reduction(self.texto_processado)\n",
        "    \n",
        "    return self.texto_processado_\n",
        "  \n",
        "  def list_reduction(self, texto_processado):\n",
        "\n",
        "    self.__texto_processado = texto_processado\n",
        "\n",
        "    return [item for sublist in self.__texto_processado[0] for item in sublist]"
      ],
      "metadata": {
        "id": "x2TvzwPJ3NJW"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(f'/content/drive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/DataFrames/PerfilOficial/popular/23012022/Per_pop_23012022.csv',\n",
        "                 usecols=[1])\n",
        "df.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "f5UjDznNQG1E",
        "outputId": "7782537f-ca0c-4e85-b039-250172774fa8"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6024628c-336d-417c-9793-444b0635d6b4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>E vamos de mÃºsica no #BBB22 ðŸŽ¤ðŸŽ¶\\n\\n#RedeBBB htt...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6024628c-336d-417c-9793-444b0635d6b4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6024628c-336d-417c-9793-444b0635d6b4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6024628c-336d-417c-9793-444b0635d6b4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                text\n",
              "0  E vamos de mÃºsica no #BBB22 ðŸŽ¤ðŸŽ¶\\n\\n#RedeBBB htt..."
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(f'/content/gdrive/MyDrive/01.Instagram profissional/NLP/Sentiment Analysis/BBB22/Twitter/BaseSpacyPT/pt_core_news_sm-2.2.5/pt_core_news_sm/pt_core_news_sm-2.2.5')"
      ],
      "metadata": {
        "id": "aGOsjLh-dbm9"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = processamento_de_texto().preprocessar(tweets=df, column=0)"
      ],
      "metadata": {
        "id": "ld5cOF4UrJTc"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcaN51iAsF2I",
        "outputId": "2f026ecc-9ad9-43d4-e64d-1a94ff95c73c"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(a[0])"
      ],
      "metadata": {
        "id": "RUk9UXtFsCBy"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hWKpr6PrnZA0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}